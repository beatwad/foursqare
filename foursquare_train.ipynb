{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to solve the problem as a multi-class classification by finding candidate points based on geographic location.<br>\n",
    "Similarity as a string, such as edit distance and LCS (Longest Common Subsequence), was used for the features of the candidate points.<br>\n",
    "<br>\n",
    "Inference is made on test data only, but the code for training is left commented out.<br>\n",
    "<br>\n",
    "In addition, making the matches bidirectional as a post-processing step improved the score by about 1%.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pouTaX_llCrb",
    "outputId": "c050ef25-9133-4c86-a9fa-f0b76527faac"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:39:59.168464Z",
     "iopub.status.busy": "2022-05-06T13:39:59.168067Z",
     "iopub.status.idle": "2022-05-06T13:39:59.776524Z",
     "shell.execute_reply": "2022-05-06T13:39:59.775359Z",
     "shell.execute_reply.started": "2022-05-06T13:39:59.1684Z"
    },
    "id": "H5QntWoelAkH",
    "outputId": "31efe7df-24ff-40e8-8517-0c7174968413"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from unidecode import unidecode\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "\n",
    "from requests import get\n",
    "\n",
    "CFG = Namespace(\n",
    "    seed = 46,\n",
    "    train = True,\n",
    "    target = \"point_of_interest\",\n",
    "    n_neighbors = 10,\n",
    "    n_splits = 3\n",
    ")\n",
    "\n",
    "random.seed(CFG.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:39:59.784268Z",
     "iopub.status.busy": "2022-05-06T13:39:59.784008Z",
     "iopub.status.idle": "2022-05-06T13:40:06.591066Z",
     "shell.execute_reply": "2022-05-06T13:40:06.590044Z",
     "shell.execute_reply.started": "2022-05-06T13:39:59.784238Z"
    },
    "id": "wz7JepVilAkN",
    "outputId": "0652de28-9bd3-4ab7-c97c-55e6e11935e6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>country</th>\n",
       "      <th>url</th>\n",
       "      <th>phone</th>\n",
       "      <th>categories</th>\n",
       "      <th>point_of_interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_000001272c6c5d</td>\n",
       "      <td>Café Stad Oudenaarde</td>\n",
       "      <td>50.859975</td>\n",
       "      <td>3.634196</td>\n",
       "      <td>Abdijstraat</td>\n",
       "      <td>Nederename</td>\n",
       "      <td>Oost-Vlaanderen</td>\n",
       "      <td>9700</td>\n",
       "      <td>BE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Bars</td>\n",
       "      <td>P_677e840bb6fc7e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_000002eae2a589</td>\n",
       "      <td>Carioca Manero</td>\n",
       "      <td>-22.907225</td>\n",
       "      <td>-43.178244</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Brazilian Restaurants</td>\n",
       "      <td>P_d82910d8382a83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_000007f24ebc95</td>\n",
       "      <td>ร้านตัดผมการาเกด</td>\n",
       "      <td>13.780813</td>\n",
       "      <td>100.484900</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TH</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Salons / Barbershops</td>\n",
       "      <td>P_b1066599e78477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_000008a8ba4f48</td>\n",
       "      <td>Turkcell</td>\n",
       "      <td>37.844510</td>\n",
       "      <td>27.844202</td>\n",
       "      <td>Adnan Menderes Bulvarı</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mobile Phone Shops</td>\n",
       "      <td>P_b2ed86905a4cd3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_00001d92066153</td>\n",
       "      <td>Restaurante Casa Cofiño</td>\n",
       "      <td>43.338196</td>\n",
       "      <td>-4.326821</td>\n",
       "      <td>None</td>\n",
       "      <td>Caviedes</td>\n",
       "      <td>Cantabria</td>\n",
       "      <td>None</td>\n",
       "      <td>ES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Spanish Restaurants</td>\n",
       "      <td>P_809a884d4407fb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                     name   latitude   longitude  \\\n",
       "0  E_000001272c6c5d     Café Stad Oudenaarde  50.859975    3.634196   \n",
       "1  E_000002eae2a589           Carioca Manero -22.907225  -43.178244   \n",
       "2  E_000007f24ebc95         ร้านตัดผมการาเกด  13.780813  100.484900   \n",
       "3  E_000008a8ba4f48                 Turkcell  37.844510   27.844202   \n",
       "4  E_00001d92066153  Restaurante Casa Cofiño  43.338196   -4.326821   \n",
       "\n",
       "                  address        city            state   zip country   url  \\\n",
       "0             Abdijstraat  Nederename  Oost-Vlaanderen  9700      BE  None   \n",
       "1                    None        None             None  None      BR  None   \n",
       "2                    None        None             None  None      TH  None   \n",
       "3  Adnan Menderes Bulvarı        None             None  None      TR  None   \n",
       "4                    None    Caviedes        Cantabria  None      ES  None   \n",
       "\n",
       "  phone             categories point_of_interest  \n",
       "0  None                   Bars  P_677e840bb6fc7e  \n",
       "1  None  Brazilian Restaurants  P_d82910d8382a83  \n",
       "2  None   Salons / Barbershops  P_b1066599e78477  \n",
       "3  None     Mobile Phone Shops  P_b2ed86905a4cd3  \n",
       "4  None    Spanish Restaurants  P_809a884d4407fb  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet(\"foursquare-location-matching-parquet/train.parquet\")\n",
    "test = pd.read_csv(\"foursquare-location-matching/test.csv\")\n",
    "test[CFG.target] = \"TEST\"\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "## Sort categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sort_categories(cat):\n",
    "    if cat is None:\n",
    "        return None\n",
    "    return ', '.join(sorted(cat.split(', ')))\n",
    "\n",
    "train['categories'] = train['categories'].apply(sort_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode all names, addresses, cities and states in unicode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    return unidecode(val)\n",
    "\n",
    "train['name'] = train['name'].apply(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name_col):\n",
    "    return name_col.str.lower()\\\n",
    "                    .str.replace(\",\", \"\")\\\n",
    "                    .str.replace(\".\", \"\")\\\n",
    "                    .str.replace(\"'\", \"\")\\\n",
    "                    .str.replace(\"the \", \"\")\\\n",
    "                    .str.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add main category to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0380390ab534b42937ff0a9f342f43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1138812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_freq = dict()\n",
    "stop_words = ['/', '&', 'or', 'High', 'Miscellaneous', 'Fast', 'Other', 'Asian', 'Chinese', 'Event', \n",
    "              'Great', 'Noodle', 'Burger', 'Seafood', 'Breakfast', 'Ice', 'Diners', 'Cream', 'Indonesian', \n",
    "              'Thai', \"Women's\", 'Fried', 'Snack', 'Tea', 'Mexican', 'Nail', 'Sushi', 'Middle', 'Korean', \n",
    "              'Gift', 'Drink', 'Pet', 'Turkish', \"Men's\", 'Indian', 'Malay', 'Cocktail', 'Donut', 'Box', \n",
    "              'Condos)', 'Residential', 'Convenience', 'Gas', 'General', 'Bus', 'Pizza', 'Spaces', 'Mobile',\n",
    "              'Phone', 'Academic', 'Japanese', 'Business', 'Shoe', 'Italian', 'American', 'Home', 'Auto', \n",
    "              'Furniture', 'Cosmetics', 'Sandwich', 'Dessert', 'Car', 'Arts', 'Financial', 'Legal', 'BBQ',\n",
    "              'Hardware', 'Video', 'Music', 'Art', 'Student', 'Jewelry', 'Historic', 'Travel', 'Washes',\n",
    "              'Beer', 'Arcades', 'Bike', 'Lookouts', 'Scenic', 'Rental', 'Accessories', 'Repairs', 'Discount', \n",
    "              'Optical', 'Bodegas', 'Big', 'Assisted', 'Living', 'Athletics', 'Agencies', 'Locations', 'Trails', \n",
    "              'Bed', 'Breakfasts', 'Wine', 'Real', 'Elementary', 'Theme', 'Golf', 'Rest',  'Photography', \n",
    "              'Nightlife', 'Courses', 'Convention', 'Eastern', 'Concert', 'Conference', 'Startups', 'Tech', \n",
    "              'Meeting', 'French', 'Supplies', 'Events', 'Sake', 'Dog', 'Ramen', 'City', 'Juice', 'Science',\n",
    "              'Liquor', 'Lawyers', 'Insurance', 'Flower', 'Toy', 'Rentals', 'Paper', 'Flea', 'Bases', 'Baseball', \n",
    "              'Karaoke', 'Kids', 'Design', 'Farmers', 'Repair', 'Technology', 'Wards', 'Water', 'Supply', \n",
    "              'Filipino', 'Piers', 'Salad', 'Mattress', 'Print', 'Wings', 'Engineering', 'Non-Profits', \n",
    "              'Gastropubs', 'Bistros', 'Hot', 'Vietnamese', 'Hookah', 'Candy', 'Coffee', 'Electronics',\n",
    "              'Department', 'Clothing', 'Trucks', 'Chicken', 'Movie', 'Health', 'Soccer', 'Crafts', \n",
    "              'Game', 'Community', 'Food', 'College', 'Sporting', 'Beauty', 'Ferries', 'Soup', 'Veterinarians', \n",
    "              'Basketball', 'Light', 'Rail', 'Taco', 'Classrooms', 'Shopping', 'Developments', 'Train', 'Performing',\n",
    "              'Administrative', 'Lingerie', 'Dive', 'Storage', 'Office', 'Landscaping', 'Residence', 'Sports',\n",
    "              'Goods', 'Dealerships', 'Grocery', 'Workshops', 'History'\n",
    "             ]\n",
    "\n",
    "\n",
    "def get_categories(category):\n",
    "    if category == 'Auto':\n",
    "        return 'Automotive'\n",
    "    if category == 'Hotel' or c == 'Motels' or c == 'Hostels':\n",
    "        return 'Hotels'\n",
    "    if category == 'Courthouses':\n",
    "        return 'Court'\n",
    "    if category == 'College':\n",
    "        return 'Colleges'\n",
    "    if category == 'Cafés':\n",
    "        return 'Cafes'\n",
    "    if category == \"Doctor's\" or c == \"Dentist's\" or c == \"Doctors\":\n",
    "        return 'Medical'\n",
    "    if category == '(Apartments':\n",
    "        return 'Apartments'\n",
    "    return category\n",
    "    \n",
    "\n",
    "for category in tqdm(train['categories']):\n",
    "    if category is not None:\n",
    "        category_list = re.split(', | ', category)\n",
    "        for c in category_list:\n",
    "            if c in stop_words or c[-2:] == 'an':\n",
    "                continue\n",
    "            c = get_categories(c)\n",
    "            x = cat_freq.get(c, 0) + 1\n",
    "            cat_freq[c] = x\n",
    "\n",
    "            \n",
    "cat_freq = pd.DataFrame(cat_freq.items(), \n",
    "                        columns=['category', 'frequence']).sort_values('frequence', \n",
    "                                                                        ascending=False).reset_index(drop=True)\n",
    "cat_freq = cat_freq.iloc[:108]\n",
    "\n",
    "cat_freq_dict = dict(zip(cat_freq['category'], cat_freq['frequence']))\n",
    "\n",
    "def get_main_category(category):\n",
    "    if category is not None:\n",
    "        category_list = re.split(', | ', category)\n",
    "        most_freq_cat = np.nan\n",
    "        freq = 0\n",
    "        \n",
    "        for c in category_list:\n",
    "            if c in stop_words or c[-2:] == 'an':\n",
    "                continue\n",
    "            c = get_categories(c)\n",
    "            f = cat_freq_dict.get(c, 0)\n",
    "            if f > freq:\n",
    "                freq = f\n",
    "                most_freq_cat = c\n",
    "        \n",
    "        return most_freq_cat\n",
    "            \n",
    "    return np.nan\n",
    "\n",
    "train['main_categories'] = train['categories'].apply(get_main_category)\n",
    "test['main_categories'] = test['categories'].apply(get_main_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing data with data from outer sources\n",
    "\n",
    "- to fill the state we will use objects with code A (country, state, region)\n",
    "\n",
    "- to fill the city we will use objects with code P (city, village)\n",
    "\n",
    "- to fill the street we will use objects with codes R (road, railroad) and S (spot, building, farm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states = pd.read_csv('states.csv', index_col='Unnamed: 0')\n",
    "cities = pd.read_csv('additional_data/cities.csv', encoding = \"ISO-8859-1\")\n",
    "cities = cities[['asciiname', 'latitude', 'longitude', 'country code']]\n",
    "cities.rename({'asciiname': 'city', 'country code': 'country'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing data by finding closest neighbors from outer sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        0\n",
       "name                      1\n",
       "latitude                  0\n",
       "longitude                 0\n",
       "address              396621\n",
       "city                 299189\n",
       "state                420586\n",
       "zip                  595426\n",
       "country                  11\n",
       "url                  871088\n",
       "phone                795957\n",
       "categories            98307\n",
       "point_of_interest         0\n",
       "main_categories      189673\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "train['country'] = train['country'].fillna('NA')\n",
    "test['country'] = test['country'].fillna('NA')\n",
    "\n",
    "geoname_dict = {'city': cities}\n",
    "geoname_dists = {'city': 5000}\n",
    "\n",
    "def fill_the_missing_data(df, df_dict, df_dists):\n",
    "    dfs = []\n",
    "    columns = list(df_dict.keys())\n",
    "    for c in tqdm(columns):\n",
    "        for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "\n",
    "            geoname_df = df_dict[c]\n",
    "            \n",
    "            country_df = country_df[country_df[c].isnull()]\n",
    "            geoname_df = geoname_df[geoname_df['country'] == country]\n",
    "                \n",
    "            if len(country_df) == 0 or len(geoname_df) == 0:\n",
    "                continue\n",
    "            \n",
    "            knn = KNeighborsRegressor(n_neighbors=min(len(geoname_df), 2), metric='haversine', n_jobs=-1)\n",
    "            knn.fit(geoname_df[['latitude','longitude']], geoname_df.index)\n",
    "            dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "            \n",
    "            if nears.shape[1] < 2:\n",
    "                continue\n",
    "            \n",
    "            nears[:,1] = nears[:,0]\n",
    "            nears[:,0] = country_df.index\n",
    "            dists = dists[:,0]*6371000\n",
    "            \n",
    "            nears = nears[dists<=df_dists[c]]\n",
    "            \n",
    "#             display(nears)\n",
    "#             display(dists)\n",
    "            \n",
    "            for n in nears:\n",
    "                t_idx = n[0]\n",
    "                c_idx = n[1]\n",
    "                df.loc[t_idx, c] = geoname_df.iloc[c_idx]['city']\n",
    "#                 display(df.loc[t_idx])\n",
    "#                 display(geoname_df.iloc[c_idx])\n",
    "                       \n",
    "    return df\n",
    "    \n",
    "# train = fill_the_missing_data(train, geoname_dict, geoname_dists)\n",
    "# test = fill_the_missing_data(test, geoname_dict, geoname_dists)\n",
    "\n",
    "# del cities\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        0\n",
       "name                      1\n",
       "latitude                  0\n",
       "longitude                 0\n",
       "address              396621\n",
       "city                 299189\n",
       "state                420586\n",
       "zip                  595426\n",
       "country                   0\n",
       "url                  871088\n",
       "phone                795957\n",
       "categories            98307\n",
       "point_of_interest         0\n",
       "main_categories      189673\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO9c6tIe3j4B"
   },
   "source": [
    "## Divide Train Data into about 600K×2\n",
    "\n",
    "This is because test set size is about 600K, so we want our model to train on KNN embeddings built on datasets of similar size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:40:20.532144Z",
     "iopub.status.busy": "2022-05-06T13:40:20.531456Z",
     "iopub.status.idle": "2022-05-06T13:40:27.344966Z",
     "shell.execute_reply": "2022-05-06T13:40:27.343597Z",
     "shell.execute_reply.started": "2022-05-06T13:40:20.532101Z"
    },
    "id": "U6PcXKsn3pcK",
    "outputId": "948a4da0-23b5-4716-8a00-1afa3eb5a20b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    569406\n",
       "0.0    569406\n",
       "Name: set, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = GroupKFold(n_splits=2)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train, train[CFG.target], train[CFG.target])):\n",
    "    train.loc[val_idx, \"set\"] = i\n",
    "train[\"set\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yJIRkRD3jr-"
   },
   "source": [
    "## Search Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ab7a770f2b4f9fbc77821257f3245d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97800acfde274e878b500efd3982a360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def create_target(row):\n",
    "    if row[CFG.target] == row['near_target_0']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def add_neighbor_features(df, train_mode=True):\n",
    "    dfs = []\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "           'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), CFG.n_neighbors), \n",
    "                                  metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "\n",
    "        targets = country_df[CFG.target].values\n",
    "        \n",
    "        for i in range(1, min(len(country_df), CFG.n_neighbors)): # 200\n",
    "            for j in range(1, min(len(country_df), CFG.n_neighbors)): # 200\n",
    "                temp_df = country_df.copy()\n",
    "                # if j > 10 and j != i: continue\n",
    "                if j < i:\n",
    "                    country_df[f\"d_near_{j}\"] = dists[:, j]\n",
    "                    country_df[f\"near_target_{j}\"] = targets[nears[:, j]]\n",
    "                    for c in columns:\n",
    "                        country_df[f\"near_{c}_{j}\"] = country_df[c].values[nears[:, j]]\n",
    "                elif j > i:\n",
    "                    country_df[f\"d_near_{j-1}\"] = dists[:, j]\n",
    "                    country_df[f\"near_target_{j-1}\"] = targets[nears[:, j]]\n",
    "                    for c in columns:\n",
    "                        country_df[f\"near_{c}_{j-1}\"] = country_df[c].values[nears[:, j]]\n",
    "                else:\n",
    "                    country_df[f\"d_near_0\"] = dists[:, j]\n",
    "                    country_df[f\"near_target_0\"] = targets[nears[:, j]]\n",
    "                    for c in columns:\n",
    "                        country_df[f\"near_{c}_0\"] = country_df[c].values[nears[:, j]]    \n",
    "\n",
    "            for j in range(min(len(country_df), CFG.n_neighbors), CFG.n_neighbors):\n",
    "                country_df[f\"d_near_{j}\"] = np.nan\n",
    "                country_df[f\"near_target_{j}\"] = np.nan\n",
    "                for c in columns:\n",
    "                    country_df[f\"near_{c}_{j}\"] = np.nan\n",
    "\n",
    "            dfs.append(temp_df)\n",
    "        \n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    \n",
    "    if train_mode:\n",
    "        df['target'] = df.apply(create_target, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = pd.concat([\n",
    "    add_neighbor_features(train[train[\"set\"]==0]), \n",
    "    add_neighbor_features(train[train[\"set\"]==1])\n",
    "])\n",
    "test = add_neighbor_features(test, train_mode=False)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# select indexes of all positive targets\n",
    "# and select indexes of all ids that don't have postive targets at all\n",
    "pos_ids = train.loc[train['target'] == 1, 'id'].unique()\n",
    "pos_idxs = train[train['target'] == 1].index\n",
    "neg_idxs = train.loc[~train['id'].isin(pos_ids), 'id'].drop_duplicates(keep='first').index\n",
    "\n",
    "# additionally select indexes of ids that have negative target\n",
    "# but may have positive target \n",
    "neg_idxs1 = train[train['target'] == 0].index\n",
    "neg_idxs1 = neg_idxs1.difference(neg_idxs)\n",
    "neg_idxs1 = np.random.choice(neg_idxs1, size=len(pos_idxs)-len(neg_idxs))\n",
    "\n",
    "# and add them to negative indexes, so the total number of positive and negative indexes are equal\n",
    "neg_idxs = neg_idxs.union(neg_idxs1)\n",
    "\n",
    "# select these positive and negative indexes from the dataset\n",
    "train = train.loc[pos_idxs.union(neg_idxs)]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Maximum Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:46:23.2289Z",
     "iopub.status.busy": "2022-05-06T13:46:23.228443Z",
     "iopub.status.idle": "2022-05-06T13:46:45.008036Z",
     "shell.execute_reply": "2022-05-06T13:46:45.00682Z",
     "shell.execute_reply.started": "2022-05-06T13:46:23.228867Z"
    }
   },
   "outputs": [],
   "source": [
    "# # https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "# def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "#     return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "# def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "#     return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "# def get_score(input_df: pd.DataFrame):\n",
    "#     scores = []\n",
    "#     for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "#         targets = poi2ids[id2poi[id_str]]\n",
    "#         preds = set(matches.split())\n",
    "#         score = len((targets & preds)) / len((targets | preds))\n",
    "#         scores.append(score)\n",
    "#     scores = np.array(scores)\n",
    "#     return scores.mean()\n",
    "\n",
    "# id2poi = get_id2poi(train)\n",
    "# poi2ids = get_poi2ids(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:46:45.009888Z",
     "iopub.status.busy": "2022-05-06T13:46:45.009581Z",
     "iopub.status.idle": "2022-05-06T13:47:25.219321Z",
     "shell.execute_reply": "2022-05-06T13:47:25.218359Z",
     "shell.execute_reply.started": "2022-05-06T13:46:45.009852Z"
    }
   },
   "outputs": [],
   "source": [
    "# scores = []\n",
    "\n",
    "# train[\"matches\"] = \"\"\n",
    "# for i in tqdm(range(CFG.n_neighbors)):\n",
    "#     idx = train[CFG.target]==train[f\"near_target_{i}\"]\n",
    "#     train.loc[idx, \"matches\"] += \" \" + train.loc[idx, f\"near_id_{i}\"]\n",
    "#     scores.append(get_score(train))\n",
    "# train[\"mathces\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:47:25.220809Z",
     "iopub.status.busy": "2022-05-06T13:47:25.220548Z",
     "iopub.status.idle": "2022-05-06T13:47:25.466253Z",
     "shell.execute_reply": "2022-05-06T13:47:25.465532Z",
     "shell.execute_reply.started": "2022-05-06T13:47:25.220778Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(8, 3), facecolor=\"white\")\n",
    "# plt.plot(range(CFG.n_neighbors), scores, marker=\"o\")\n",
    "# plt.grid()\n",
    "# plt.xlabel(\"# of candidates\")\n",
    "# plt.ylabel(\"Maximum Score\")\n",
    "# plt.ylim([0.6, 1.0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:47:25.467894Z",
     "iopub.status.busy": "2022-05-06T13:47:25.467232Z",
     "iopub.status.idle": "2022-05-06T13:47:25.472531Z",
     "shell.execute_reply": "2022-05-06T13:47:25.47125Z",
     "shell.execute_reply.started": "2022-05-06T13:47:25.467851Z"
    }
   },
   "outputs": [],
   "source": [
    "# del train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k59Vk9d5Pmx"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:47:25.489436Z",
     "iopub.status.busy": "2022-05-06T13:47:25.486815Z",
     "iopub.status.idle": "2022-05-06T13:47:26.359925Z",
     "shell.execute_reply": "2022-05-06T13:47:26.358946Z",
     "shell.execute_reply.started": "2022-05-06T13:47:25.489387Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T13:47:26.361912Z",
     "iopub.status.busy": "2022-05-06T13:47:26.361649Z",
     "iopub.status.idle": "2022-05-06T13:47:26.369408Z",
     "shell.execute_reply": "2022-05-06T13:47:26.3687Z",
     "shell.execute_reply.started": "2022-05-06T13:47:26.36188Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset again (to avoid OOM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train, train[CFG.target])):\n",
    "    train.loc[val_idx, \"set\"] = i\n",
    "train[\"set\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import difflib\n",
    "\n",
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "           'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "    for i in tqdm(range(CFG.n_neighbors)):\n",
    "        for c in columns:\n",
    "            geshs = []\n",
    "            levens = []\n",
    "            jaros = []\n",
    "            lcss = []\n",
    "            for str1, str2 in df[[c, f\"near_{c}_{i}\"]].values.astype(str):\n",
    "                if str1 is not None and str2 is not None:\n",
    "                    geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                    levens.append(Levenshtein.distance(str1, str2))\n",
    "                    jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                    lcss.append(LCS(str(str1), str(str2)))\n",
    "                else:\n",
    "                    geshs.append(-1)\n",
    "                    levens.append(-1)\n",
    "                    jaros.append(-1)\n",
    "            df[f\"near_{c}_{i}_gesh\"] = geshs\n",
    "            df[f\"near_{c}_{i}_leven\"] = levens\n",
    "            df[f\"near_{c}_{i}_jaro\"] = jaros\n",
    "            df[f\"near_{c}_{i}_lcs\"] = lcss\n",
    "            \n",
    "            if not c in ['country', \"phone\", \"zip\"]:\n",
    "                df[f\"near_{c}_{i}_len\"] = df[f\"near_{c}_{i}\"].astype(str).map(len)\n",
    "                df[f\"near_{c}_{i}_nleven\"] = df[f\"near_{c}_{i}_leven\"] / df[[f\"near_{c}_{i}_len\", f\"near_{c}_0_len\"]].max(axis=1)\n",
    "                df[f\"near_{c}_{i}_nlcsi\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_{i}_len\"]\n",
    "                df[f\"near_{c}_{i}_nlcs0\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_0_len\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()-1\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = pd.concat([\n",
    "    add_distance_features(train[train[\"set\"]==0]), \n",
    "    add_distance_features(train[train[\"set\"]==1]),\n",
    "    add_distance_features(train[train[\"set\"]==2]), \n",
    "    add_distance_features(train[train[\"set\"]==3])\n",
    "])\n",
    "# train = add_distance_features(train)\n",
    "# test = add_distance_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3HV5kfs6saZ"
   },
   "source": [
    "## Delete unused columns (just to avoid OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "columns = ['name', 'address', 'city', 'state',\n",
    "       'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "for i in tqdm(range(CFG.n_neighbors)):\n",
    "    features.append(f\"d_near_{i}\")\n",
    "    for c in columns:        \n",
    "        features += [f\"near_{c}_{i}_gesh\", f\"near_{c}_{i}_jaro\", f\"near_{c}_{i}_lcs\"]\n",
    "        if c in ['country', \"phone\", \"zip\"]:\n",
    "            features += [f\"near_{c}_{i}_leven\"]\n",
    "        else:\n",
    "            features += [f\"near_{c}_{i}_len\", f\"near_{c}_{i}_nleven\", f\"near_{c}_{i}_nlcsi\", f\"near_{c}_{i}_nlcs0\"]\n",
    "\n",
    "for f in features:\n",
    "    if f not in test.columns:\n",
    "        test[f] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[features + [CFG.target, \"target\", \"id\"] + [f\"near_id_{i}\" for i in range(CFG.n_neighbors)]]\n",
    "# test = test[features + [\"id\"] + [f\"near_id_{i}\" for i in range(CFG.n_neighbors)]]\n",
    "\n",
    "train[features] = train[features].astype(np.float16)\n",
    "# test[features] = test[features].astype(np.float16)\n",
    "\n",
    "train[\"target\"] = train[\"target\"].fillna(0)\n",
    "train[\"near_id_0\"] = train[\"near_id_0\"].fillna('')\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "# test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for _ in range(5):\n",
    "    gc.collect()\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qknhwIvndmJ_"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-06T13:47:27.726106Z",
     "iopub.status.idle": "2022-05-06T13:47:27.726625Z",
     "shell.execute_reply": "2022-05-06T13:47:27.726435Z",
     "shell.execute_reply.started": "2022-05-06T13:47:27.726416Z"
    },
    "id": "iLKg1PSWlAkU"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.misc import derivative\n",
    "\n",
    "\n",
    "def fit_lgbm(X, y, params=None, es_rounds=20, seed=42, N_SPLITS=5, \n",
    "             n_class=None, model_dir=None, folds=None):\n",
    "    cat_features = X.select_dtypes(include='object').columns\n",
    "    \n",
    "    models = []\n",
    "    oof = np.zeros(len(y), dtype=np.float64)\n",
    "    \n",
    "    for i in tqdm(range(CFG.n_splits)):\n",
    "        print(f\"== fold {i} ==\")\n",
    "        trn_idx = folds != i\n",
    "        val_idx = folds == i\n",
    "        \n",
    "        train_dataset = lgb.Dataset(X.iloc[trn_idx], y.iloc[trn_idx], categorical_feature=cat_features)\n",
    "        valid_dataset = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx], categorical_feature=cat_features)\n",
    "\n",
    "        \n",
    "        focal_loss = lambda x,y: focal_loss_lgb(x, y, alpha=1., gamma=1.)\n",
    "        focal_loss_eval = lambda x,y: focal_loss_lgb_eval_error(x, y, alpha=1., gamma=1.)\n",
    "        \n",
    "        if model_dir is None:\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_set = train_dataset, \n",
    "                valid_sets = [train_dataset, valid_dataset], \n",
    "                callbacks = [lgb.log_evaluation(100), \n",
    "                             lgb.early_stopping(stopping_rounds=es_rounds)],\n",
    "            )\n",
    "        else:\n",
    "            with open(f'{model_dir}/lgbm_fold{i}.pkl', 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "        pred = model.predict(X.iloc[val_idx])\n",
    "        oof[val_idx] = pred\n",
    "        models.append(model)\n",
    "        \n",
    "        file = f'lgbm_fold{i}.pkl'\n",
    "        pickle.dump(model, open(file, 'wb'))\n",
    "        print()\n",
    "\n",
    "    cv = (np.round(oof) == y).mean()\n",
    "    print(f\"CV-accuracy: {cv}\")\n",
    "\n",
    "    return oof, models\n",
    "\n",
    "def inference_lgbm(models, feat_df):\n",
    "    pred = np.array([model.predict(feat_df) for model in models])\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-06T13:47:27.727487Z",
     "iopub.status.idle": "2022-05-06T13:47:27.728353Z",
     "shell.execute_reply": "2022-05-06T13:47:27.728137Z",
     "shell.execute_reply.started": "2022-05-06T13:47:27.72811Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "for i, (trn_idx, val_idx) in tqdm(enumerate(kf.split(train, train[\"target\"], train[\"target\"]))):\n",
    "    train.loc[val_idx, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "### Split folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-06T13:47:27.734661Z",
     "iopub.status.idle": "2022-05-06T13:47:27.735005Z",
     "shell.execute_reply": "2022-05-06T13:47:27.734859Z",
     "shell.execute_reply.started": "2022-05-06T13:47:27.734842Z"
    },
    "id": "y22l2kyLlAkV",
    "outputId": "c284471a-0dd7-4d4b-aa98-82378d2f4b32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "\n",
    "params = {\n",
    "    'seed': CFG.seed,\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.2,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'max_bin': 200,\n",
    "    'max_depth': 7,   \n",
    "    'num_leaves': 35, \n",
    "    'min_data_in_leaf': 25,\n",
    "    'n_estimators': 5000, \n",
    "    'colsample_bytree': 0.9,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "if CFG.train:\n",
    "    oof, models = fit_lgbm(train[features], train[\"target\"].astype(int), \n",
    "                           params=params, n_class=int(train[\"target\"].max() + 1), \n",
    "                           N_SPLITS=CFG.n_splits, folds=train[\"fold\"].values)\n",
    "#                            model_dir='foursquare-exp009')\n",
    "else:\n",
    "    models = [joblib.load(f'foursquare-exp009/lgbm_fold{i}.pkl') for i in range(CFG.n_splits)]\n",
    "pred = inference_lgbm(models, test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-06T13:47:27.735816Z",
     "iopub.status.idle": "2022-05-06T13:47:27.736112Z",
     "shell.execute_reply": "2022-05-06T13:47:27.735968Z",
     "shell.execute_reply.started": "2022-05-06T13:47:27.735953Z"
    },
    "id": "yHFNkcnglAkW",
    "outputId": "b886a788-4f08-468b-8050-17e92da005ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in tqdm(df[\"matches\"]):\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df \n",
    "\n",
    "near_id = train[\"near_id_0\"].values\n",
    "matches = []\n",
    "\n",
    "for id, ps, ids in tqdm(zip(train[\"id\"], oof, near_id)):\n",
    "    idx = np.round(ps)\n",
    "    if idx == 1:\n",
    "        matches.append(id + \" \" + ids)\n",
    "    else:\n",
    "        matches.append(id)\n",
    "        \n",
    "train['matches'] = matches\n",
    "        \n",
    "\n",
    "train = postprocess(train)\n",
    "# test = postprocess(test)\n",
    "\n",
    "id2poi = get_id2poi(train)\n",
    "poi2ids = get_poi2ids(train)\n",
    "print(f\"CV: {get_score(train):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline \n",
    "# acc: 0.9041545048699873\n",
    "# CV: 0.824261\n",
    "\n",
    "# Your base model:\n",
    "# acc: 0.85371\n",
    "# CV: 0.805996\n",
    "\n",
    "# Change task to binary, save near neighbours\"\n",
    "# acc: 0.932\n",
    "# CV: 0.8331\n",
    "\n",
    "# Add main categories:\n",
    "# acc 0.93494\n",
    "# CV 0.834842\n",
    "\n",
    "# Sort categories:\n",
    "# acc 0.934852\n",
    "# CV 0.834894\n",
    "\n",
    "# Unidecode name column:\n",
    "# acc \n",
    "# CV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-06T13:47:27.741423Z",
     "iopub.status.idle": "2022-05-06T13:47:27.741922Z",
     "shell.execute_reply": "2022-05-06T13:47:27.741675Z",
     "shell.execute_reply.started": "2022-05-06T13:47:27.741649Z"
    },
    "id": "o7YqLSXPlAkX",
    "outputId": "11bfa4d9-e19a-4934-f3de-8dacf74f4a18"
   },
   "outputs": [],
   "source": [
    "def plot_importances(models):\n",
    "    importance_df = pd.DataFrame(models[0].feature_importance(), \n",
    "                                 index=features, \n",
    "                                 columns=['importance'])\\\n",
    "                        .sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    plt.subplots(figsize=(len(features) // 4, 5))\n",
    "    plt.bar(importance_df.index, importance_df.importance)\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_importances(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-06T13:47:27.745582Z",
     "iopub.status.idle": "2022-05-06T13:47:27.746181Z",
     "shell.execute_reply": "2022-05-06T13:47:27.745922Z",
     "shell.execute_reply.started": "2022-05-06T13:47:27.745892Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssub = pd.read_csv(\"../input/foursquare-location-matching/sample_submission.csv\")\n",
    "ssub = ssub.drop(columns=\"matches\")\n",
    "ssub = ssub.merge(test[[\"id\", \"matches\"]], on=\"id\")\n",
    "ssub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "ssub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further ideas\n",
    "\n",
    "- normalize unicode strings: https://www.kaggle.com/competitions/foursquare-location-matching/discussion/320938\n",
    "- try clean the name after transforming to unicode string (you already have the function for this)\n",
    "- fill the gaps in address/state data\n",
    "- after that you can try to submit\n",
    "\n",
    "\n",
    "\n",
    "- add ntlk.edit_distance to your features\n",
    "- change KNN to the variant, that was proposed in this notebook: https://www.kaggle.com/code/ragnar123/flm-xlmroberta-inference-baseline\n",
    "- add manhattan distance and euqlidian distance\n",
    "- increase number of nearest neighbours to a very high value (like 50-100-200), so you will be able to find more matches; don't increase number of neighbours in the table to avoid OOM\n",
    "\n",
    "\n",
    "- how to handle missing data https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python\n",
    "- mean/median/std encode features\n",
    "- use feature generation and selection from this notebook https://www.kaggle.com/code/aerdem4/foursquare-gpu-accelerated-lofo-feature-importance\n",
    "- use Cat2Vec to calculate categories similarity https://www.kaggle.com/code/aerdem4/foursquare-cat2vec/notebook\n",
    "\n",
    "\n",
    "\n",
    "- try XLMRoberta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
