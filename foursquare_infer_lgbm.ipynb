{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.399414Z",
     "iopub.status.busy": "2022-05-20T23:06:53.399105Z",
     "iopub.status.idle": "2022-05-20T23:06:53.403901Z",
     "shell.execute_reply": "2022-05-20T23:06:53.403266Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.399358Z"
    },
    "id": "pouTaX_llCrb",
    "outputId": "c050ef25-9133-4c86-a9fa-f0b76527faac"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.408122Z",
     "iopub.status.busy": "2022-05-20T23:06:53.407759Z",
     "iopub.status.idle": "2022-05-20T23:06:53.948805Z",
     "shell.execute_reply": "2022-05-20T23:06:53.947555Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.408094Z"
    },
    "id": "H5QntWoelAkH",
    "outputId": "31efe7df-24ff-40e8-8517-0c7174968413"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "# import cudf\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "from requests import get\n",
    "\n",
    "CFG = Namespace(\n",
    "    kaggle = False,\n",
    "    seed = 46,\n",
    "    train = False,\n",
    "    validate = False,\n",
    "    inference = True,\n",
    "    target = \"point_of_interest\",\n",
    "    n_neighbors = 50,\n",
    "    n_neighbors_in_cols = 5,\n",
    "    n_splits = 3\n",
    ")\n",
    "\n",
    "random.seed(CFG.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.950215Z",
     "iopub.status.busy": "2022-05-20T23:06:53.949976Z",
     "iopub.status.idle": "2022-05-20T23:06:59.622657Z",
     "shell.execute_reply": "2022-05-20T23:06:59.621792Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.950187Z"
    },
    "id": "wz7JepVilAkN",
    "outputId": "0652de28-9bd3-4ab7-c97c-55e6e11935e6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>country</th>\n",
       "      <th>url</th>\n",
       "      <th>phone</th>\n",
       "      <th>categories</th>\n",
       "      <th>point_of_interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_000001272c6c5d</td>\n",
       "      <td>Café Stad Oudenaarde</td>\n",
       "      <td>50.859975</td>\n",
       "      <td>3.634196</td>\n",
       "      <td>Abdijstraat</td>\n",
       "      <td>Nederename</td>\n",
       "      <td>Oost-Vlaanderen</td>\n",
       "      <td>9700</td>\n",
       "      <td>BE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bars</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_000002eae2a589</td>\n",
       "      <td>Carioca Manero</td>\n",
       "      <td>-22.907225</td>\n",
       "      <td>-43.178244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazilian Restaurants</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_000007f24ebc95</td>\n",
       "      <td>ร้านตัดผมการาเกด</td>\n",
       "      <td>13.780813</td>\n",
       "      <td>100.484900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salons / Barbershops</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_000008a8ba4f48</td>\n",
       "      <td>Turkcell</td>\n",
       "      <td>37.844510</td>\n",
       "      <td>27.844202</td>\n",
       "      <td>Adnan Menderes Bulvarı</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mobile Phone Shops</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_00001d92066153</td>\n",
       "      <td>Restaurante Casa Cofiño</td>\n",
       "      <td>43.338196</td>\n",
       "      <td>-4.326821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caviedes</td>\n",
       "      <td>Cantabria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spanish Restaurants</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                     name   latitude   longitude  \\\n",
       "0  E_000001272c6c5d     Café Stad Oudenaarde  50.859975    3.634196   \n",
       "1  E_000002eae2a589           Carioca Manero -22.907225  -43.178244   \n",
       "2  E_000007f24ebc95         ร้านตัดผมการาเกด  13.780813  100.484900   \n",
       "3  E_000008a8ba4f48                 Turkcell  37.844510   27.844202   \n",
       "4  E_00001d92066153  Restaurante Casa Cofiño  43.338196   -4.326821   \n",
       "\n",
       "                  address        city            state   zip country  url  \\\n",
       "0             Abdijstraat  Nederename  Oost-Vlaanderen  9700      BE  NaN   \n",
       "1                     NaN         NaN              NaN   NaN      BR  NaN   \n",
       "2                     NaN         NaN              NaN   NaN      TH  NaN   \n",
       "3  Adnan Menderes Bulvarı         NaN              NaN   NaN      TR  NaN   \n",
       "4                     NaN    Caviedes        Cantabria   NaN      ES  NaN   \n",
       "\n",
       "  phone             categories point_of_interest  \n",
       "0   NaN                   Bars              TEST  \n",
       "1   NaN  Brazilian Restaurants              TEST  \n",
       "2   NaN   Salons / Barbershops              TEST  \n",
       "3   NaN     Mobile Phone Shops              TEST  \n",
       "4   NaN    Spanish Restaurants              TEST  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CFG.train:\n",
    "    if CFG.kaggle:\n",
    "        train = pd.read_parquet(\"../input/foursquare-location-matching/train.csv\")\n",
    "    else:\n",
    "        train = pd.read_parquet(\"foursquare-location-matching/train.csv\")\n",
    "\n",
    "if CFG.kaggle:\n",
    "#     test = pd.read_csv(\"../input/foursquare-location-matching/train.csv\").iloc[:650000]\n",
    "    test = pd.read_csv(\"../input/foursquare-location-matching/test.csv\")\n",
    "else:\n",
    "#     test = pd.read_csv(\"foursquare-location-matching/test.csv\")\n",
    "    test = pd.read_csv(\"foursquare-location-matching/train.csv\").iloc[:650000]\n",
    "    \n",
    "test[CFG.target] = \"TEST\"\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "## Sort categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:59.625612Z",
     "iopub.status.busy": "2022-05-20T23:06:59.625293Z",
     "iopub.status.idle": "2022-05-20T23:06:59.969411Z",
     "shell.execute_reply": "2022-05-20T23:06:59.968569Z",
     "shell.execute_reply.started": "2022-05-20T23:06:59.625570Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sort_categories(cat):\n",
    "    if cat != cat:\n",
    "        return np.nan\n",
    "    return ', '.join(sorted(cat.split(', ')))\n",
    "\n",
    "test['categories'] = test['categories'].apply(sort_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode name  in unicode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:59.973840Z",
     "iopub.status.busy": "2022-05-20T23:06:59.970646Z",
     "iopub.status.idle": "2022-05-20T23:07:01.998556Z",
     "shell.execute_reply": "2022-05-20T23:07:01.997919Z",
     "shell.execute_reply.started": "2022-05-20T23:06:59.973801Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode(col):\n",
    "    if col != col:\n",
    "        return np.nan\n",
    "    return unidecode(col)\n",
    "\n",
    "test['name'] = test['name'].apply(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:02.000825Z",
     "iopub.status.busy": "2022-05-20T23:07:01.999925Z",
     "iopub.status.idle": "2022-05-20T23:07:02.511541Z",
     "shell.execute_reply": "2022-05-20T23:07:02.510787Z",
     "shell.execute_reply.started": "2022-05-20T23:07:02.000783Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_name(col):\n",
    "    if col != col:\n",
    "        return np.nan\n",
    "    return col.lower()\\\n",
    "              .replace(\",\", \"\")\\\n",
    "              .replace(\".\", \"\")\\\n",
    "              .replace(\"'\", \"\")\\\n",
    "              .replace(\"the \", \"\")\n",
    "\n",
    "test['name'] = test['name'].apply(clean_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make same categories for the same names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:02.512763Z",
     "iopub.status.busy": "2022-05-20T23:07:02.512539Z",
     "iopub.status.idle": "2022-05-20T23:07:02.589150Z",
     "shell.execute_reply": "2022-05-20T23:07:02.588346Z",
     "shell.execute_reply.started": "2022-05-20T23:07:02.512738Z"
    }
   },
   "outputs": [],
   "source": [
    "# brings shops with similar names to one name\n",
    "if CFG.train:\n",
    "    train.loc[train['name']=='mc donalds', 'name'] = 'mcdonalds'\n",
    "test.loc[test['name']=='mc donalds', 'name'] = 'mcdonalds'\n",
    "# train.loc[train['name']=='7-eleven (echewn `iielfewn)', 'name'] = '7-eleven'\n",
    "# train.loc[train['name']=='starbucks coffee', 'name'] = 'starbucks'\n",
    "\n",
    "# vc_name = train['name'].value_counts()\n",
    "# vc_name = vc_name[vc_name >= 30]\n",
    "\n",
    "# for name in tqdm(vc_name.index):\n",
    "#     pop_cat = train.loc[train['name']==name, 'categories'].value_counts().index[0]\n",
    "#     if pop_cat != pop_cat:\n",
    "#         pop_cat = train.loc[train['name']==name, 'categories'].value_counts().index[1]\n",
    "#     train.loc[train['name']==name, 'categories'] = pop_cat\n",
    "    \n",
    "# del vc_name\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add main category to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:02.593477Z",
     "iopub.status.busy": "2022-05-20T23:07:02.590690Z",
     "iopub.status.idle": "2022-05-20T23:07:06.852054Z",
     "shell.execute_reply": "2022-05-20T23:07:06.851144Z",
     "shell.execute_reply.started": "2022-05-20T23:07:02.593440Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cat_freq = dict()\n",
    "stop_words = ['/', '&', 'or', 'High', 'Miscellaneous', 'Fast', 'Other', 'Asian', 'Chinese', 'Event', \n",
    "              'Great', 'Noodle', 'Burger', 'Seafood', 'Breakfast', 'Ice', 'Diners', 'Cream', 'Indonesian', \n",
    "              'Thai', \"Women's\", 'Fried', 'Snack', 'Tea', 'Mexican', 'Nail', 'Sushi', 'Middle', 'Korean', \n",
    "              'Gift', 'Drink', 'Pet', 'Turkish', \"Men's\", 'Indian', 'Malay', 'Cocktail', 'Donut', 'Box', \n",
    "              'Condos)', 'Residential', 'Convenience', 'Gas', 'General', 'Bus', 'Pizza', 'Spaces', 'Mobile',\n",
    "              'Phone', 'Academic', 'Japanese', 'Business', 'Shoe', 'Italian', 'American', 'Home', 'Auto', \n",
    "              'Furniture', 'Cosmetics', 'Sandwich', 'Dessert', 'Car', 'Arts', 'Financial', 'Legal', 'BBQ',\n",
    "              'Hardware', 'Video', 'Music', 'Art', 'Student', 'Jewelry', 'Historic', 'Travel', 'Washes',\n",
    "              'Beer', 'Arcades', 'Bike', 'Lookouts', 'Scenic', 'Rental', 'Accessories', 'Repairs', 'Discount', \n",
    "              'Optical', 'Bodegas', 'Big', 'Assisted', 'Living', 'Athletics', 'Agencies', 'Locations', 'Trails', \n",
    "              'Bed', 'Breakfasts', 'Wine', 'Real', 'Elementary', 'Theme', 'Golf', 'Rest',  'Photography', \n",
    "              'Nightlife', 'Courses', 'Convention', 'Eastern', 'Concert', 'Conference', 'Startups', 'Tech', \n",
    "              'Meeting', 'French', 'Supplies', 'Events', 'Sake', 'Dog', 'Ramen', 'City', 'Juice', 'Science',\n",
    "              'Liquor', 'Lawyers', 'Insurance', 'Flower', 'Toy', 'Rentals', 'Paper', 'Flea', 'Bases', 'Baseball', \n",
    "              'Karaoke', 'Kids', 'Design', 'Farmers', 'Repair', 'Technology', 'Wards', 'Water', 'Supply', \n",
    "              'Filipino', 'Piers', 'Salad', 'Mattress', 'Print', 'Wings', 'Engineering', 'Non-Profits', \n",
    "              'Gastropubs', 'Bistros', 'Hot', 'Vietnamese', 'Hookah', 'Candy', 'Coffee', 'Electronics',\n",
    "              'Department', 'Clothing', 'Trucks', 'Chicken', 'Movie', 'Health', 'Soccer', 'Crafts', \n",
    "              'Game', 'Community', 'Food', 'College', 'Sporting', 'Beauty', 'Ferries', 'Soup', 'Veterinarians', \n",
    "              'Basketball', 'Light', 'Rail', 'Taco', 'Classrooms', 'Shopping', 'Developments', 'Train', 'Performing',\n",
    "              'Administrative', 'Lingerie', 'Dive', 'Storage', 'Office', 'Landscaping', 'Residence', 'Sports',\n",
    "              'Goods', 'Dealerships', 'Grocery', 'Workshops', 'History'\n",
    "             ]\n",
    "\n",
    "\n",
    "def get_categories(category):\n",
    "    if category == 'Auto':\n",
    "        return 'Automotive'\n",
    "    if category == 'Hotel' or category == 'Motels' or category == 'Hostels':\n",
    "        return 'Hotels'\n",
    "    if category == 'Courthouses':\n",
    "        return 'Court'\n",
    "    if category == 'College':\n",
    "        return 'Colleges'\n",
    "    if category == 'Cafés':\n",
    "        return 'Cafes'\n",
    "    if category == \"Doctor's\" or category == \"Dentist's\" or category == \"Doctors\":\n",
    "        return 'Medical'\n",
    "    if category == '(Apartments':\n",
    "        return 'Apartments'\n",
    "    return category\n",
    "\n",
    "if CFG.kaggle:\n",
    "    cat_freq = pd.read_csv('../input/foursquare-main-categories/cat_freq.csv', index_col='Unnamed: 0')\n",
    "else:\n",
    "    cat_freq = pd.read_csv('foursquare_main_categories/cat_freq.csv', index_col='Unnamed: 0')\n",
    "\n",
    "cat_freq_dict = dict(zip(cat_freq['category'], cat_freq['frequence']))\n",
    "\n",
    "def get_main_category(category):\n",
    "    if category == category:\n",
    "        category_list = re.split(', | ', category)\n",
    "        most_freq_cat = np.nan\n",
    "        freq = 0\n",
    "        \n",
    "        for c in category_list:\n",
    "            if c in stop_words or c[-2:] == 'an':\n",
    "                continue\n",
    "            c = get_categories(c)\n",
    "            f = cat_freq_dict.get(c, 0)\n",
    "            if f > freq:\n",
    "                freq = f\n",
    "                most_freq_cat = c\n",
    "        \n",
    "        return most_freq_cat\n",
    "            \n",
    "    return np.nan\n",
    "\n",
    "if CFG.train:\n",
    "    train['main_categories'] = train['categories'].apply(get_main_category)\n",
    "    \n",
    "test['main_categories'] = test['categories'].apply(get_main_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing data with data from outer sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:06.853494Z",
     "iopub.status.busy": "2022-05-20T23:07:06.853225Z",
     "iopub.status.idle": "2022-05-20T23:07:06.857561Z",
     "shell.execute_reply": "2022-05-20T23:07:06.856630Z",
     "shell.execute_reply.started": "2022-05-20T23:07:06.853463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# states = pd.read_csv('states.csv', index_col='Unnamed: 0')\n",
    "# cities = pd.read_csv('additional_data/cities.csv', encoding = \"ISO-8859-1\")\n",
    "# cities = cities[['asciiname', 'latitude', 'longitude', 'country code']]\n",
    "\n",
    "# starbucks = pd.read_csv('additional_data/starbucks.csv', index_col='Unnamed: 0')\n",
    "# starbucks = starbucks[['countryCode', 'latitude', 'longitude', 'streetAddressLine2', 'city']]\n",
    "# starbucks.rename({'countryCode': 'country', 'streetAddressLine2': 'address'}, axis=1, inplace=True)\n",
    "# starbucks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing data by finding closest neighbors from outer sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:06.859067Z",
     "iopub.status.busy": "2022-05-20T23:07:06.858838Z",
     "iopub.status.idle": "2022-05-20T23:07:06.874674Z",
     "shell.execute_reply": "2022-05-20T23:07:06.874014Z",
     "shell.execute_reply.started": "2022-05-20T23:07:06.859040Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train['country'] = train['country'].fillna('NA')\n",
    "# test['country'] = test['country'].fillna('NA')\n",
    "\n",
    "# geoname_dict = {'address': starbucks, 'city': starbucks}\n",
    "# geoname_dists = {'address': 100, 'city': 5000}\n",
    "\n",
    "# def fill_the_missing_data(df, df_dict, df_dists):\n",
    "#     dfs = []\n",
    "#     columns = list(df_dict.keys())\n",
    "#     for c in tqdm(columns):\n",
    "#         for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "\n",
    "#             geoname_df = df_dict[c]\n",
    "            \n",
    "#             country_df = country_df[(country_df[c].isnull())]\n",
    "#             geoname_df = geoname_df[geoname_df['country'] == country]\n",
    "                \n",
    "#             if len(country_df) == 0 or len(geoname_df) == 0:\n",
    "#                 continue\n",
    "            \n",
    "#             knn = KNeighborsRegressor(n_neighbors=min(len(geoname_df), 2), metric='haversine', n_jobs=-1)\n",
    "#             knn.fit(geoname_df[['latitude','longitude']], geoname_df.index)\n",
    "#             dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "            \n",
    "#             if nears.shape[1] < 2:\n",
    "#                 continue\n",
    "            \n",
    "#             nears[:,1] = nears[:,0]\n",
    "#             nears[:,0] = country_df.index\n",
    "#             dists = dists[:,0]*6371000\n",
    "            \n",
    "#             nears = nears[dists<=df_dists[c]]\n",
    "            \n",
    "# #             display(nears)\n",
    "# #             display(dists)\n",
    "            \n",
    "#             for n in nears:\n",
    "#                 t_idx = n[0]\n",
    "#                 c_idx = n[1]\n",
    "#                 df.loc[t_idx, c] = geoname_df.iloc[c_idx]['city']\n",
    "# #                 display(df.loc[t_idx])\n",
    "# #                 display(geoname_df.iloc[c_idx])\n",
    "                       \n",
    "#     return df\n",
    "    \n",
    "# if CFG.train:\n",
    "#     train = fill_the_missing_data(train, geoname_dict, geoname_dists)\n",
    "# test = fill_the_missing_data(test, geoname_dict, geoname_dists)\n",
    "\n",
    "# del starbucks\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring all object columns to low register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:06.876285Z",
     "iopub.status.busy": "2022-05-20T23:07:06.875857Z",
     "iopub.status.idle": "2022-05-20T23:07:11.294594Z",
     "shell.execute_reply": "2022-05-20T23:07:11.293738Z",
     "shell.execute_reply.started": "2022-05-20T23:07:06.876241Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_lower(df):\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].fillna('')\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "    return df\n",
    "    \n",
    "test = to_lower(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yJIRkRD3jr-"
   },
   "source": [
    "## Search Candidates Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.296604Z",
     "iopub.status.busy": "2022-05-20T23:07:11.296073Z",
     "iopub.status.idle": "2022-05-20T23:07:11.304539Z",
     "shell.execute_reply": "2022-05-20T23:07:11.303490Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.296561Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def create_target(row):\n",
    "#     if row[CFG.target] == row['near_target_0']:\n",
    "#         return 1\n",
    "#     return 0\n",
    "\n",
    "# def add_neighbor_features(df, train_mode=True):\n",
    "#     dfs = None\n",
    "#     columns = ['id', 'name', 'address', 'city', 'state',\n",
    "#            'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "#     for c in columns:\n",
    "#         if c != \"id\":\n",
    "#             df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "#     for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "#         dfs_list = list()\n",
    "#         country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "#         knn = KNeighborsRegressor(n_neighbors=min(len(country_df), CFG.n_neighbors), \n",
    "#                                   metric='haversine', n_jobs=-1)\n",
    "#         knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "#         dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "\n",
    "#         targets = country_df[CFG.target].values\n",
    "        \n",
    "#         for i in range(min(len(country_df)-1, 1), min(len(country_df), CFG.n_neighbors)): # 200\n",
    "#             for j in range(min(len(country_df)-1, 1), min(len(country_df), CFG.n_neighbors)): # 200\n",
    "#                 if j > CFG.n_neighbors_in_cols and j != i: \n",
    "#                     continue\n",
    "#                 elif j < i:\n",
    "#                     country_df[f\"d_near_{j}\"] = dists[:, j]\n",
    "#                     country_df[f\"near_target_{j}\"] = targets[nears[:, j]]\n",
    "#                     for c in columns:\n",
    "#                         country_df[f\"near_{c}_{j}\"] = country_df[c].values[nears[:, j]]\n",
    "#                 elif j > i:\n",
    "#                     country_df[f\"d_near_{j-1}\"] = dists[:, j]\n",
    "#                     country_df[f\"near_target_{j-1}\"] = targets[nears[:, j]]\n",
    "#                     for c in columns:\n",
    "#                         country_df[f\"near_{c}_{j-1}\"] = country_df[c].values[nears[:, j]]\n",
    "#                 else:\n",
    "#                     country_df[f\"d_near_0\"] = dists[:, j]\n",
    "#                     country_df[f\"near_target_0\"] = targets[nears[:, j]]\n",
    "#                     for c in columns:\n",
    "#                         country_df[f\"near_{c}_0\"] = country_df[c].values[nears[:, j]]    \n",
    "\n",
    "#             for j in range(min(len(country_df), CFG.n_neighbors), CFG.n_neighbors):\n",
    "#                 country_df[f\"d_near_{j}\"] = np.nan\n",
    "#                 country_df[f\"near_target_{j}\"] = np.nan\n",
    "#                 for c in columns:\n",
    "#                     country_df[f\"near_{c}_{j}\"] = np.nan\n",
    "\n",
    "#             if train_mode:\n",
    "#                 df['target'] = df.apply(country_df, axis=1)\n",
    "            \n",
    "#             dfs_list.append(country_df.copy())\n",
    "                            \n",
    "#         res = pd.concat(dfs_list)\n",
    "                            \n",
    "#         if dfs is not None:\n",
    "#             dfs = pd.concat([dfs, res])\n",
    "#         else:\n",
    "#             dfs = res.copy()\n",
    "                            \n",
    "#         del res\n",
    "#         gc.collect()\n",
    "\n",
    "#     return dfs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.308464Z",
     "iopub.status.busy": "2022-05-20T23:07:11.308203Z",
     "iopub.status.idle": "2022-05-20T23:07:11.421300Z",
     "shell.execute_reply": "2022-05-20T23:07:11.420367Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.308433Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_target(row):\n",
    "    if row[CFG.target] == row['near_target']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def add_neighbour_features(df, train_mode=True):\n",
    "    dfs = None\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "           'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        dfs_list = list()\n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), CFG.n_neighbors), \n",
    "                                  metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "\n",
    "        targets = country_df[CFG.target].values\n",
    "        \n",
    "        if len(country_df) == 1:\n",
    "            country_df[f\"d_near\"] = np.nan\n",
    "            country_df[f\"near_target\"] = np.nan\n",
    "            for c in columns:\n",
    "                country_df[f\"near_{c}\"] = np.nan\n",
    "            if train_mode:\n",
    "                country_df['target'] = np.nan\n",
    "                \n",
    "            dfs_list.append(country_df.copy())\n",
    "            \n",
    "        else:\n",
    "            for i in range(1, min(len(country_df), CFG.n_neighbors)): # 200\n",
    "                country_df[f\"d_near\"] = dists[:, i]\n",
    "                country_df[f\"near_target\"] = targets[nears[:, i]]\n",
    "                for c in columns:\n",
    "                    country_df[f\"near_{c}\"] = country_df[c].values[nears[:, i]]\n",
    "\n",
    "                if train_mode:\n",
    "                    country_df['target'] = country_df.apply(create_target, axis=1)\n",
    "                \n",
    "                dfs_list.append(country_df.copy())\n",
    "            \n",
    "        res = pd.concat(dfs_list)\n",
    "        \n",
    "        del dists, nears, targets, dfs_list\n",
    "        gc.collect()\n",
    "        \n",
    "        if dfs is not None:\n",
    "            dfs = pd.concat([dfs, res])\n",
    "        else:\n",
    "            dfs = res.copy()\n",
    "            \n",
    "        del res\n",
    "        gc.collect()\n",
    "    \n",
    "    return dfs.reset_index(drop=True)\n",
    "\n",
    "def add_neighbour_features_low_mem(country_df):\n",
    "    dfs_list = list()\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "           'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "    \n",
    "    knn = KNeighborsRegressor(n_neighbors=min(len(country_df), CFG.n_neighbors), \n",
    "                              metric='haversine', n_jobs=-1)\n",
    "    knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "    dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "\n",
    "    targets = country_df[CFG.target].values\n",
    "\n",
    "    if len(country_df) == 1:\n",
    "        country_df[f\"d_near\"] = np.nan\n",
    "        country_df[f\"near_target\"] = np.nan\n",
    "        for c in columns:\n",
    "            country_df[f\"near_{c}\"] = ''\n",
    "\n",
    "        dfs_list.append(country_df.copy())\n",
    "\n",
    "    else:\n",
    "        for i in range(1, min(len(country_df), CFG.n_neighbors)): # 200\n",
    "            country_df[f\"d_near\"] = dists[:, i].astype(float)\n",
    "            country_df[f\"near_target\"] = targets[nears[:, i]]\n",
    "            for c in columns:\n",
    "                country_df[f\"near_{c}\"] = country_df[c].values[nears[:, i]]\n",
    "\n",
    "            dfs_list.append(country_df.copy())\n",
    "            \n",
    "    del knn, dists, nears, targets\n",
    "    gc.collect()\n",
    "    \n",
    "    return pd.concat(dfs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the kernel (to avoid OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.422818Z",
     "iopub.status.busy": "2022-05-20T23:07:11.422393Z",
     "iopub.status.idle": "2022-05-20T23:07:11.436199Z",
     "shell.execute_reply": "2022-05-20T23:07:11.435248Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.422773Z"
    }
   },
   "outputs": [],
   "source": [
    "# %reset --aggressive -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k59Vk9d5Pmx"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.438632Z",
     "iopub.status.busy": "2022-05-20T23:07:11.438405Z",
     "iopub.status.idle": "2022-05-20T23:07:11.951488Z",
     "shell.execute_reply": "2022-05-20T23:07:11.950653Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.438591Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.952759Z",
     "iopub.status.busy": "2022-05-20T23:07:11.952538Z",
     "iopub.status.idle": "2022-05-20T23:07:11.960409Z",
     "shell.execute_reply": "2022-05-20T23:07:11.959531Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.952733Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.961703Z",
     "iopub.status.busy": "2022-05-20T23:07:11.961485Z",
     "iopub.status.idle": "2022-05-20T23:07:11.973654Z",
     "shell.execute_reply": "2022-05-20T23:07:11.973111Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.961669Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Levenshtein\n",
    "# import difflib\n",
    "\n",
    "# def _add_distance_features(args):\n",
    "#     _, df = args\n",
    "\n",
    "#     columns = ['name', 'address', 'city', 'state',\n",
    "#            'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "#     for i in tqdm(range(CFG.n_neighbors)):\n",
    "#         for c in columns:\n",
    "#             geshs = []\n",
    "#             levens = []\n",
    "#             jaros = []\n",
    "#             lcss = []\n",
    "#             for str1, str2 in df[[c, f\"near_{c}_{i}\"]].values.astype(str):\n",
    "#                 if str1==str1 and str2==str2:\n",
    "#                     geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "#                     levens.append(Levenshtein.distance(str1, str2))\n",
    "#                     jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "#                     lcss.append(LCS(str(str1), str(str2)))\n",
    "#                 else:\n",
    "#                     geshs.append(-1)\n",
    "#                     levens.append(-1)\n",
    "#                     jaros.append(-1)\n",
    "#             df[f\"near_{c}_{i}_gesh\"] = geshs\n",
    "#             df[f\"near_{c}_{i}_leven\"] = levens\n",
    "#             df[f\"near_{c}_{i}_jaro\"] = jaros\n",
    "#             df[f\"near_{c}_{i}_lcs\"] = lcss\n",
    "            \n",
    "#             if not c in ['country', \"phone\", \"zip\"]:\n",
    "#                 df[f\"near_{c}_{i}_len\"] = df[f\"near_{c}_{i}\"].astype(str).map(len)\n",
    "#                 df[f\"near_{c}_{i}_nleven\"] = df[f\"near_{c}_{i}_leven\"] / df[[f\"near_{c}_{i}_len\", f\"near_{c}_0_len\"]].max(axis=1)\n",
    "#                 df[f\"near_{c}_{i}_nlcsi\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_{i}_len\"]\n",
    "#                 df[f\"near_{c}_{i}_nlcs0\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_0_len\"]\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def add_distance_features(df):\n",
    "#     processes = multiprocessing.cpu_count()-1\n",
    "#     with multiprocessing.Pool(processes=processes) as pool:\n",
    "#         dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "#         dfs = tqdm(dfs)\n",
    "#         dfs = list(dfs)\n",
    "#     df = pd.concat(dfs)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# if CFG.train:\n",
    "#     train = pd.concat([\n",
    "#         add_distance_features(train[train[\"set\"]==0]), \n",
    "#         add_distance_features(train[train[\"set\"]==1]),\n",
    "#         add_distance_features(train[train[\"set\"]==2]), \n",
    "#         add_distance_features(train[train[\"set\"]==3]),\n",
    "#         add_distance_features(train[train[\"set\"]==4]), \n",
    "#         add_distance_features(train[train[\"set\"]==5]),\n",
    "#         add_distance_features(train[train[\"set\"]==6]), \n",
    "#         add_distance_features(train[train[\"set\"]==7])\n",
    "#     ])\n",
    "\n",
    "# if n_test_splits == 5:\n",
    "#     test = pd.concat([\n",
    "#         add_distance_features(test[test[\"set\"]==0]), \n",
    "#         add_distance_features(test[test[\"set\"]==1]),\n",
    "#         add_distance_features(test[test[\"set\"]==2]), \n",
    "#         add_distance_features(test[test[\"set\"]==3]),\n",
    "#         add_distance_features(test[test[\"set\"]==4]), \n",
    "#         add_distance_features(test[test[\"set\"]==5])\n",
    "#     ])\n",
    "# else:\n",
    "#     test = pd.concat([\n",
    "#             add_distance_features(test[test[\"set\"]==0]), \n",
    "#             add_distance_features(test[test[\"set\"]==1]),\n",
    "#             add_distance_features(test[test[\"set\"]==2]), \n",
    "#             add_distance_features(test[test[\"set\"]==3]),\n",
    "#             add_distance_features(test[test[\"set\"]==4]), \n",
    "#             add_distance_features(test[test[\"set\"]==5]),\n",
    "#             add_distance_features(test[test[\"set\"]==6]), \n",
    "#             add_distance_features(test[test[\"set\"]==7])\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.975439Z",
     "iopub.status.busy": "2022-05-20T23:07:11.975007Z",
     "iopub.status.idle": "2022-05-20T23:07:12.328336Z",
     "shell.execute_reply": "2022-05-20T23:07:12.327575Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.975401Z"
    }
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import difflib\n",
    "\n",
    "def seq_match_distance(str1, str2):\n",
    "    if str1 == '' or str1 == '':\n",
    "        return -1\n",
    "    return difflib.SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "def lev_distance(str1, str2):\n",
    "    if str1 == '' or str1 == '':\n",
    "        return -1\n",
    "    return Levenshtein.distance(str1, str2)\n",
    "\n",
    "def jw_distance(str1, str2):\n",
    "    if str1 == '' or str1 == '':\n",
    "        return -1\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n",
    "\n",
    "def lcs_distance(str1, str2):\n",
    "    if str1 == '' or str1 == '':\n",
    "        return -1\n",
    "    return LCS(str1, str2)\n",
    "\n",
    "def get_distances(str1, str2):\n",
    "    if str1 == '' or str1 == '':\n",
    "        return -1, -1, -1, -1\n",
    "    return difflib.SequenceMatcher(None, str1, str2).ratio(), \\\n",
    "           Levenshtein.distance(str1, str2), \\\n",
    "           Levenshtein.jaro_winkler(str1, str2), \\\n",
    "           LCS(str1, str2)\n",
    "\n",
    "def add_distance_features(df):\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "    for c in columns:\n",
    "        df[f\"near_{c}_gesh\"]=[*map(seq_match_distance, df[c], df[f\"near_{c}\"])]\n",
    "        df[f\"near_{c}_leven\"]=[*map(lev_distance, df[c], df[f\"near_{c}\"])]\n",
    "        df[f\"near_{c}_jaro\"]=[*map(jw_distance, df[c], df[f\"near_{c}\"])]\n",
    "        df[f\"near_{c}_lcs\"]=[*map(lcs_distance, df[c], df[f\"near_{c}\"])]\n",
    "#         df[f\"near_{c}_gesh\"], df[f\"near_{c}_leven\"], \\\n",
    "#         df[f\"near_{c}_jaro\"], df[f\"near_{c}_lcs\"] = zip(*map(get_distances, df[c], df[f\"near_{c}\"]))\n",
    "\n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"near_{c}_len\"] = df[f\"near_{c}\"].astype(str).map(len)\n",
    "            df[f\"near_{c}_nleven\"] = df[f\"near_{c}_leven\"] / df[f\"near_{c}_len\"]\n",
    "            df[f\"near_{c}_nlcsi\"] = df[f\"near_{c}_lcs\"] / df[f\"near_{c}_len\"]\n",
    "            df[f\"near_{c}_nlcs0\"] = df[f\"near_{c}_lcs\"] / df[f\"near_{c}_len\"]\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce memory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.329694Z",
     "iopub.status.busy": "2022-05-20T23:07:12.329467Z",
     "iopub.status.idle": "2022-05-20T23:07:12.335613Z",
     "shell.execute_reply": "2022-05-20T23:07:12.334830Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.329666Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem(df, train_mode=False):\n",
    "    for f in features:\n",
    "        if f not in df.columns:\n",
    "            df[f] = np.nan\n",
    "\n",
    "    if train_mode:\n",
    "        df = df[features + [CFG.target, \"target\", \"id\"] + [\"near_id\"]]\n",
    "        df[\"target\"] = df[\"target\"].fillna(0)\n",
    "    else:    \n",
    "        df = df[features + [\"id\"] + [\"near_id\"]]\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    df[\"near_id\"] = df[\"near_id\"].fillna('')\n",
    "\n",
    "    gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.337055Z",
     "iopub.status.busy": "2022-05-20T23:07:12.336850Z",
     "iopub.status.idle": "2022-05-20T23:07:12.981799Z",
     "shell.execute_reply": "2022-05-20T23:07:12.981125Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.337029Z"
    },
    "id": "y22l2kyLlAkV",
    "outputId": "c284471a-0dd7-4d4b-aa98-82378d2f4b32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CFG.kaggle:\n",
    "    models = [joblib.load(f'../input/fsq-lgbm-models-3fold/lgbm_fold{i}.pkl') for i in range(CFG.n_splits)]\n",
    "else:\n",
    "    models = [joblib.load(f'fsq_lgbm_models_3fold/lgbm_fold{i}.pkl') for i in range(CFG.n_splits)]\n",
    "\n",
    "def inference_lgbm(models, feat_df):\n",
    "    pred = np.array([model.predict(feat_df) for model in models])\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuctions for postprocessing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.983819Z",
     "iopub.status.busy": "2022-05-20T23:07:12.983027Z",
     "iopub.status.idle": "2022-05-20T23:07:12.994514Z",
     "shell.execute_reply": "2022-05-20T23:07:12.993726Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.983695Z"
    },
    "id": "yHFNkcnglAkW",
    "outputId": "b886a788-4f08-468b-8050-17e92da005ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(input_df)\n",
    "    poi2ids = get_poi2ids(input_df)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in df[\"matches\"]:\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df \n",
    "\n",
    "def get_matches(df, preds):\n",
    "    near_id = df[\"near_id\"].values\n",
    "    matches = []\n",
    "\n",
    "    for df_id, pred, near_idx in zip(df[\"id\"], preds, near_id):\n",
    "        idx = np.round(pred)\n",
    "        if idx == 1:\n",
    "            matches.append(df_id + \" \" + near_idx)\n",
    "        else:\n",
    "            matches.append(df_id)\n",
    "    \n",
    "    df['matches'] = matches\n",
    "    df = postprocess(df)\n",
    "    return df[['id', 'matches']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set features to predict on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.996114Z",
     "iopub.status.busy": "2022-05-20T23:07:12.995898Z",
     "iopub.status.idle": "2022-05-20T23:07:13.011631Z",
     "shell.execute_reply": "2022-05-20T23:07:13.010775Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.996088Z"
    }
   },
   "outputs": [],
   "source": [
    "res = list()\n",
    "features = list()\n",
    "\n",
    "columns = ['name', 'address', 'city', 'state',\n",
    "       'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "features.append(f\"d_near\")\n",
    "for c in columns:        \n",
    "    features += [f\"near_{c}_gesh\", f\"near_{c}_jaro\", f\"near_{c}_lcs\"]\n",
    "    if c in ['country', \"phone\", \"zip\"]:\n",
    "        features += [f\"near_{c}_leven\"]\n",
    "    else:\n",
    "        features += [f\"near_{c}_len\", f\"near_{c}_nleven\", f\"near_{c}_nlcsi\", f\"near_{c}_nlcs0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf countries\n",
    "!mkdir countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:13.013394Z",
     "iopub.status.busy": "2022-05-20T23:07:13.013160Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c341eb3f5557456898aee9c8e2ae1a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 s, sys: 643 ms, total: 4.64 s\n",
      "Wall time: 35min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_data(args):\n",
    "    country, df = args\n",
    "    columns = ['name', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "    df = add_neighbour_features_low_mem(df)\n",
    "    df = add_distance_features(df)\n",
    "    df = reduce_mem(df)\n",
    "#     df.to_csv(f'countries/{country}.csv')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "num_countries = test['country'].nunique()\n",
    "    \n",
    "# try to group by countries, sort them according to number of ids\n",
    "# then select thedata by country and process them\n",
    "processes = multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=processes) as pool:\n",
    "    dfs = pool.imap_unordered(process_data, test.groupby('country', sort=False))\n",
    "    dfs = tqdm(dfs, total=num_countries)\n",
    "    dfs = list(dfs)\n",
    "    \n",
    "del test\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predict matches and postprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5e8ed9214a4c1bbe0d6053655ffa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'tqdm' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tqdm' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in tqdm(range(len(dfs))):\n",
    "    preds = inference_lgbm(models, dfs[i][features])\n",
    "    dfs[i] = get_matches(dfs[i], preds)\n",
    "    \n",
    "dfs = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/foursquare-location-matching/sample_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_532313/516280849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/foursquare-location-matching/sample_submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mssub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"matches\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mssub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mssub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ubiquant/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ubiquant/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ubiquant/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ubiquant/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ubiquant/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ubiquant/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/foursquare-location-matching/sample_submission.csv'"
     ]
    }
   ],
   "source": [
    "ssub = pd.read_csv(\"../input/foursquare-location-matching/sample_submission.csv\")\n",
    "ssub = ssub.drop(columns=\"matches\")\n",
    "\n",
    "ssub = ssub.merge(dfs, on=\"id\")\n",
    "ssub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "ssub.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further ideas\n",
    "\n",
    "- submit\n",
    "\n",
    "\n",
    "\n",
    "- do we need url/zip/phone?\n",
    "- add ntlk.edit_distance to your features\n",
    "- change KNN to the variant, that was proposed in this notebook: https://www.kaggle.com/code/ragnar123/flm-xlmroberta-inference-baseline\n",
    "- add manhattan distance and euqlidian distance\n",
    "- increase number of nearest neighbours to a very high value (like 50-100-200), so you will be able to find more matches; don't increase number of neighbours in the table to avoid OOM\n",
    "- add KNN by country\n",
    "\n",
    "\n",
    "- how to handle missing data https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python\n",
    "- mean/median/std encode features\n",
    "- use feature generation and selection from this notebook https://www.kaggle.com/code/aerdem4/foursquare-gpu-accelerated-lofo-feature-importance\n",
    "- use Cat2Vec to calculate categories similarity https://www.kaggle.com/code/aerdem4/foursquare-cat2vec/notebook\n",
    "\n",
    "\n",
    "- Optuna!\n",
    "\n",
    "\n",
    "\n",
    "- try XLMRoberta\n",
    "\n",
    "\n",
    "- you can use dict to store key-poi_id pairs and store only keys to save the memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
