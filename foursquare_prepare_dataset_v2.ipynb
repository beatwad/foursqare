{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0ff9a2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:02.707700Z",
     "iopub.status.busy": "2022-06-26T21:10:02.707263Z",
     "iopub.status.idle": "2022-06-26T21:10:03.911556Z",
     "shell.execute_reply": "2022-06-26T21:10:03.910623Z"
    },
    "papermill": {
     "duration": 1.214549,
     "end_time": "2022-06-26T21:10:03.914447",
     "exception": false,
     "start_time": "2022-06-26T21:10:02.699898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "from glob import glob\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from argparse import Namespace\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "from haversine import haversine\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None) # Suppress annoying warnings\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b046c",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8587a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = Namespace(\n",
    "    seed = 42,\n",
    "    train = False,\n",
    "    debug = True,\n",
    "    inference = False,\n",
    "    target = \"point_of_interest\",\n",
    "    n_neighbors = 20,\n",
    "    n_splits = 10,\n",
    "    threshold = 0.5,\n",
    "    train_path = 'train_dataset',\n",
    "    model_dir = '../input/fsquarecode/saved/',\n",
    "    encode = False\n",
    ")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefc08a",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00f602d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:03.966239Z",
     "iopub.status.busy": "2022-06-26T21:10:03.965608Z",
     "iopub.status.idle": "2022-06-26T21:10:04.251551Z",
     "shell.execute_reply": "2022-06-26T21:10:04.250870Z"
    },
    "papermill": {
     "duration": 0.293507,
     "end_time": "2022-06-26T21:10:04.253670",
     "exception": false,
     "start_time": "2022-06-26T21:10:03.960163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading, preprocessing\n",
    "if CFG.train:\n",
    "    df = pd.read_csv(\"foursquare_location_matching/train.csv\")\n",
    "elif CFG.debug:\n",
    "    df = pd.read_csv(\"foursquare_location_matching/train.csv\", nrows=3000)\n",
    "else:\n",
    "    \n",
    "    df = pd.read_csv(\"../input/foursquare-location-matching/test.csv\")\n",
    "\n",
    "if len(df) < 20:\n",
    "    df = pd.read_csv('../input/foursquare-location-matching/train.csv', nrows=3000)\n",
    "    df = df.drop('point_of_interest', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b490f",
   "metadata": {},
   "source": [
    "## Add main category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9b2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['/', '&', 'or', 'High', 'Miscellaneous', 'Fast', 'Other', 'Asian', 'Chinese', 'Event', \n",
    "              'Great', 'Noodle', 'Burger', 'Seafood', 'Breakfast', 'Ice', 'Diners', 'Cream', 'Indonesian', \n",
    "              'Thai', \"Women's\", 'Fried', 'Snack', 'Tea', 'Mexican', 'Nail', 'Sushi', 'Middle', 'Korean', \n",
    "              'Gift', 'Drink', 'Pet', 'Turkish', \"Men's\", 'Indian', 'Malay', 'Cocktail', 'Donut', 'Box', \n",
    "              'Condos)', 'Residential', 'Convenience', 'Gas', 'General', 'Bus', 'Pizza', 'Spaces', 'Mobile',\n",
    "              'Phone', 'Academic', 'Japanese', 'Business', 'Shoe', 'Italian', 'American', 'Home', 'Auto', \n",
    "              'Furniture', 'Cosmetics', 'Sandwich', 'Dessert', 'Car', 'Arts', 'Financial', 'Legal', 'BBQ',\n",
    "              'Hardware', 'Video', 'Music', 'Art', 'Student', 'Jewelry', 'Historic', 'Travel', 'Washes',\n",
    "              'Beer', 'Arcades', 'Bike', 'Lookouts', 'Scenic', 'Rental', 'Accessories', 'Repairs', 'Discount', \n",
    "              'Optical', 'Bodegas', 'Big', 'Assisted', 'Living', 'Athletics', 'Agencies', 'Locations', 'Trails', \n",
    "              'Bed', 'Breakfasts', 'Wine', 'Real', 'Elementary', 'Theme', 'Golf', 'Rest',  'Photography', \n",
    "              'Nightlife', 'Courses', 'Convention', 'Eastern', 'Concert', 'Conference', 'Startups', 'Tech', \n",
    "              'Meeting', 'French', 'Supplies', 'Events', 'Sake', 'Dog', 'Ramen', 'City', 'Juice', 'Science',\n",
    "              'Liquor', 'Lawyers', 'Insurance', 'Flower', 'Toy', 'Rentals', 'Paper', 'Flea', 'Bases', 'Baseball', \n",
    "              'Karaoke', 'Kids', 'Design', 'Farmers', 'Repair', 'Technology', 'Wards', 'Water', 'Supply', \n",
    "              'Filipino', 'Piers', 'Salad', 'Mattress', 'Print', 'Wings', 'Engineering', 'Non-Profits', \n",
    "              'Gastropubs', 'Bistros', 'Hot', 'Vietnamese', 'Hookah', 'Candy', 'Coffee', 'Electronics',\n",
    "              'Department', 'Clothing', 'Trucks', 'Chicken', 'Movie', 'Health', 'Soccer', 'Crafts', \n",
    "              'Game', 'Community', 'Food', 'College', 'Sporting', 'Beauty', 'Ferries', 'Soup', 'Veterinarians', \n",
    "              'Basketball', 'Light', 'Rail', 'Taco', 'Classrooms', 'Shopping', 'Developments', 'Train', 'Performing',\n",
    "              'Administrative', 'Lingerie', 'Dive', 'Storage', 'Office', 'Landscaping', 'Residence', 'Sports',\n",
    "              'Goods', 'Dealerships', 'Grocery', 'Workshops', 'History'\n",
    "             ]\n",
    "\n",
    "\n",
    "def get_categories(category):\n",
    "    if category == 'Auto':\n",
    "        return 'Automotive'\n",
    "    if category == 'Hotel' or category == 'Motels' or category == 'Hostels':\n",
    "        return 'Hotels'\n",
    "    if category == 'Courthouses':\n",
    "        return 'Court'\n",
    "    if category == 'College':\n",
    "        return 'Colleges'\n",
    "    if category == 'CafÃ©s':\n",
    "        return 'Cafes'\n",
    "    if category == \"Doctor's\" or category == \"Dentist's\" or category == \"Doctors\":\n",
    "        return 'Medical'\n",
    "    if category == '(Apartments':\n",
    "        return 'Apartments'\n",
    "    return category\n",
    "\n",
    "cat_freq = pd.read_csv('foursquare_main_categories/cat_freq.csv', index_col='Unnamed: 0')\n",
    "cat_freq_dict = dict(zip(cat_freq['category'], cat_freq['frequence']))\n",
    "\n",
    "def get_main_category(category):\n",
    "    if category == category:\n",
    "        category_list = re.split(', | ', category)\n",
    "        most_freq_cat = np.nan\n",
    "        freq = 0\n",
    "        \n",
    "        for c in category_list:\n",
    "            if c in stop_words or c[-2:] == 'an':\n",
    "                continue\n",
    "            c = get_categories(c)\n",
    "            f = cat_freq_dict.get(c, 0)\n",
    "            if f > freq:\n",
    "                freq = f\n",
    "                most_freq_cat = c\n",
    "        \n",
    "        return most_freq_cat\n",
    "            \n",
    "    return np.nan\n",
    "\n",
    "df['main_category'] = df['categories'].apply(get_main_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c1f8b",
   "metadata": {},
   "source": [
    "## Add closest city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7919a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae56bcc5fcd2432fba94d20f73bf6a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# states = pd.read_csv('states.csv', index_col='Unnamed: 0')\n",
    "cities = pd.read_csv('additional_data/cities.csv', encoding = \"ISO-8859-1\")\n",
    "cities = cities[['asciiname', 'latitude', 'longitude', 'country code']]\n",
    "cities.rename({'asciiname': 'city', 'country code': 'country'}, axis=1, inplace=True)\n",
    "\n",
    "geoname_dict = {'city': cities}\n",
    "\n",
    "def fill_the_missing_data(args):#, df_dists):\n",
    "    country, country_df = args\n",
    "    dfs = []\n",
    "    columns = list(geoname_dict.keys())\n",
    "    for c in tqdm(columns):\n",
    "\n",
    "            geoname_df = geoname_dict[c]\n",
    "            geoname_df = geoname_df[geoname_df['country'] == country]\n",
    "                \n",
    "            if len(country_df) == 0 or len(geoname_df) == 0:\n",
    "                continue\n",
    "            \n",
    "            knn = KNeighborsRegressor(n_neighbors=min(len(geoname_df), 2), metric='haversine')\n",
    "            knn.fit(geoname_df[['latitude','longitude']], geoname_df.index)\n",
    "            dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "            \n",
    "            if nears.shape[1] < 2:\n",
    "                continue\n",
    "            \n",
    "            nears[:,1] = nears[:,0]\n",
    "            nears[:,0] = country_df.index\n",
    "            \n",
    "            for n in nears:\n",
    "                t_idx = n[0]\n",
    "                c_idx = n[1]\n",
    "                country_df.loc[t_idx, f\"closest_{c}\"] = geoname_df.iloc[c_idx][c]\n",
    "                       \n",
    "    return country_df\n",
    "    \n",
    "    \n",
    "df['country'] = df['country'].fillna('NA')\n",
    "num_countries = df['country'].nunique()\n",
    "    \n",
    "processes = multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=processes) as pool:\n",
    "    dfs = pool.imap_unordered(fill_the_missing_data, df.groupby('country', sort=False))\n",
    "    dfs = tqdm(dfs, total=num_countries)\n",
    "    dfs = list(dfs)\n",
    "    \n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "del cities\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6567bcc",
   "metadata": {},
   "source": [
    "## Create vectors from text columns with multilingual encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be38e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')# paraphrase-MiniLM-L3-v2')# all-MiniLM-L6-v2')\n",
    "# vectors = model.encode(data['full_address'].values, batch_size=128, show_progress_bar=True)\n",
    "\n",
    "# with open('additional_data/text_vectors.npy', 'wb') as f:\n",
    "#     np.save(f, vectors)\n",
    "\n",
    "# with open('additional_data/text_vectors.npy', 'rb') as f:\n",
    "#     vectors = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b25fb",
   "metadata": {},
   "source": [
    "## Preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12aa4112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:03.928050Z",
     "iopub.status.busy": "2022-06-26T21:10:03.927669Z",
     "iopub.status.idle": "2022-06-26T21:10:03.953784Z",
     "shell.execute_reply": "2022-06-26T21:10:03.953040Z"
    },
    "papermill": {
     "duration": 0.03397,
     "end_time": "2022-06-26T21:10:03.955569",
     "exception": false,
     "start_time": "2022-06-26T21:10:03.921599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pickle_save(obj, filename):\n",
    "    pickle.dump(obj, open(filename, 'wb'))\n",
    "\n",
    "def pickle_load(filename):\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def apply_notnull(df, column, target_column, function):\n",
    "    df.loc[df[column].notnull(), target_column] = \\\n",
    "        df.loc[df[column].notnull(), column].apply(function)\n",
    "    return df\n",
    "\n",
    "def pair_func(func, x1, x2):\n",
    "    if type(x1) == float and type(x2) == float:\n",
    "        return -1\n",
    "    elif type(x1) == float or type(x2) == float:\n",
    "        return -0.5\n",
    "    try:\n",
    "        return func(x1, x2)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def clean_string(df, column, target_column):\n",
    "    # Unidecode\n",
    "    df = apply_notnull(df, column, target_column, lambda x: unidecode(x))\n",
    "\n",
    "    # Replace AND, AT\n",
    "    df = apply_notnull(df, target_column, target_column, lambda x: x.translate(\n",
    "        str.maketrans({\"@\": \"at\", \"&\": \"and\"})))\n",
    "\n",
    "    # Strip punctuation\n",
    "    df = apply_notnull(df, target_column, target_column, lambda x: x.translate(\n",
    "        str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # To lowercase\n",
    "    df = apply_notnull(df, target_column, target_column, lambda x: x.lower())\n",
    "\n",
    "    # Remove leading spaces\n",
    "    df = apply_notnull(df, target_column, target_column, lambda x: x.strip())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_shingles(df, column, shingle_k):\n",
    "    for k in shingle_k:\n",
    "        sh = ShingleBased(k=k)\n",
    "        df = apply_notnull(df, column,\n",
    "                           f\"{column}_shingles_{k}\", lambda x: sh.get_profile(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "_SPACE_PATTERN = re.compile(\"\\\\s+\")\n",
    "\n",
    "\n",
    "class ShingleBased:\n",
    "\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def get_k(self):\n",
    "        return self.k\n",
    "\n",
    "    def get_profile(self, string):\n",
    "        shingles = dict()\n",
    "        no_space_str = _SPACE_PATTERN.sub(\" \", string)\n",
    "        for i in range(len(no_space_str) - self.k + 1):\n",
    "            shingle = no_space_str[i:i + self.k]\n",
    "            old = shingles.get(shingle)\n",
    "            if old:\n",
    "                shingles[str(shingle)] = int(old + 1)\n",
    "            else:\n",
    "                shingles[str(shingle)] = 1\n",
    "        return shingles\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = df.set_index(\"id\", drop=False)\n",
    "\n",
    "    # Name cleaning\n",
    "    df = clean_string(df, \"name\", \"name_cleaned\")\n",
    "\n",
    "    # Name shingles\n",
    "    df = get_shingles(df, \"name_cleaned\", (2, 3))\n",
    "\n",
    "    # Closest city shingles\n",
    "    df = get_shingles(df, \"closest_city\", (3,))\n",
    "\n",
    "    # Full address\n",
    "    df[\"full_address\"] = df[\"address\"].fillna(\"\") +\\\n",
    "        \" \" + df[\"city\"].fillna(\"\") +\\\n",
    "        \" \" + df[\"state\"].fillna(\"\")\n",
    "\n",
    "    df.loc[df[\"full_address\"] == \"  \", \"full_address\"] = np.NaN\n",
    "    df = clean_string(df, \"full_address\", \"full_address_cleaned\")\n",
    "    df = get_shingles(df, \"full_address_cleaned\", (3,))\n",
    "\n",
    "    # Numbers in name/address\n",
    "    df = apply_notnull(\n",
    "        df, \"name_cleaned\", \"numbers_in_name\", get_numbers_from_name)\n",
    "    df.loc[df[\"numbers_in_name\"] == \"\", \"numbers_in_name\"] = np.NaN\n",
    "\n",
    "    df = apply_notnull(\n",
    "        df, \"full_address_cleaned\", \"numbers_in_full_address\", get_numbers_from_name)\n",
    "    df.loc[df[\"numbers_in_full_address\"]\n",
    "           == \"\", \"numbers_in_full_address\"] = np.NaN\n",
    "\n",
    "    df = get_shingles(df, \"numbers_in_name\", (1, 2))\n",
    "    df = get_shingles(df, \"numbers_in_full_address\", (1, 2))\n",
    "    \n",
    "    # Catogories shingles\n",
    "    df = get_shingles(df, \"categories\", (3,))\n",
    "\n",
    "    # Categories to frozenset\n",
    "    df[\"categories\"] = df[\"categories\"].fillna(\"None\")\n",
    "    df[\"categories\"] = df[\"categories\"].apply(lambda x: x.split(\", \"))\n",
    "    df[\"categories\"] = df[\"categories\"].apply(frozenset)\n",
    "    \n",
    "    # Main catogory shingles\n",
    "    df = get_shingles(df, \"main_category\", (3,))\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if CFG.encode:\n",
    "        # No encoders provided, create and save\n",
    "        encoder_params = {\"dtype\": np.int32,\n",
    "                          \"handle_unknown\": \"use_encoded_value\",\n",
    "                          \"unknown_value\": -1}\n",
    "\n",
    "        ordinal_encoder = OrdinalEncoder(**encoder_params)\n",
    "\n",
    "        ordinal_encoder = ordinal_encoder.fit(df[[\"country\", \"categories\"]]) # main_category, closest_city ?\n",
    "        pickle_save(ordinal_encoder, \"saved/ordinal_encoder.pkl\")\n",
    "        encoder = ordinal_encoder\n",
    "    else:\n",
    "        encoder = pickle_load(\"additional_data/ordinal_encoder.pkl\")\n",
    "\n",
    "    df[[\"country_enc\", \"categories_enc\"]] = encoder.transform(df[[\"country\", \"categories\"]])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pickle_load(filename):\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def get_numbers_from_name(name):\n",
    "    return \"\".join(re.findall(\"[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\", name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4dd24a",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c5b674",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = preprocessing(df)\n",
    "df.index.rename(\"index\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e65cd",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48905dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_vectorize(df):\n",
    "    # set dict for corresponding ids and index\n",
    "    id2index_d = dict(zip(df['id'].values, df.index))\n",
    "\n",
    "    # make TF-IDF features\n",
    "    tfidf_d = {}\n",
    "    for col in [\"categories\", \"name_cleaned\", \"full_address_cleaned\"]:\n",
    "        if col == \"categories\":\n",
    "            tfidf = TfidfVectorizer(use_idf=False)\n",
    "        else:\n",
    "            tfidf = TfidfVectorizer(ngram_range=(3, 3), analyzer=\"char_wb\", use_idf=False, stop_words=['unknown'])\n",
    "        tv_fit = tfidf.fit_transform(df[col].astype(str).values)\n",
    "        tfidf_d[col] = tv_fit\n",
    "        \n",
    "    return id2index_d, tfidf_d, tv_fit\n",
    "\n",
    "id2index_d, tfidf_d, tv_fit = tf_idf_vectorize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3942ee",
   "metadata": {},
   "source": [
    "## Split data on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0e1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_split(df):\n",
    "    gkf = GroupKFold(n_splits=2)\n",
    "    splits = list(gkf.split(\n",
    "        df, groups=df[\"point_of_interest\"]))\n",
    "\n",
    "    return df.iloc[splits[0][1]], df.iloc[splits[1][1]]\n",
    "\n",
    "df['country'] = df['country'].fillna('NA')\n",
    "df['name_cleaned'] = df['name_cleaned'].fillna('')\n",
    "\n",
    "if CFG.train:\n",
    "    fold0_df, fold1_df = group_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e2ec2",
   "metadata": {},
   "source": [
    "## Candidate search utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95123710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:04.264732Z",
     "iopub.status.busy": "2022-06-26T21:10:04.264150Z",
     "iopub.status.idle": "2022-06-26T21:10:04.297750Z",
     "shell.execute_reply": "2022-06-26T21:10:04.296802Z"
    },
    "papermill": {
     "duration": 0.041404,
     "end_time": "2022-06-26T21:10:04.299697",
     "exception": false,
     "start_time": "2022-06-26T21:10:04.258293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overlap(profile0, profile1):\n",
    "    union = set()\n",
    "    for k in profile0.keys():\n",
    "        union.add(k)\n",
    "    for k in profile1.keys():\n",
    "        union.add(k)\n",
    "    inter = int(len(profile0.keys()) + len(profile1.keys()) - len(union))\n",
    "    return inter / min(len(profile0), len(profile1))\n",
    "\n",
    "def country_closest_k(train_df, country, candidate_k):\n",
    "    country_df = train_df[train_df[\"country\"] == country]\n",
    "\n",
    "    # Coordinates\n",
    "    country_np = np.deg2rad(country_df[[\"latitude\", \"longitude\"]].to_numpy())\n",
    "\n",
    "    # To 3d\n",
    "    country_np = np.vstack([(np.cos(country_np[:, 0]) * np.cos(country_np[:, 1])),\n",
    "                            (np.cos(country_np[:, 0]) *\n",
    "                             np.sin(country_np[:, 1])),\n",
    "                            (np.sin(country_np[:, 0]))]).T\n",
    "\n",
    "    neigh = NearestNeighbors(n_jobs=-1).fit(country_np)\n",
    "    try:\n",
    "        distances, neighbors_indices = neigh.kneighbors(\n",
    "            country_np, n_neighbors=candidate_k, return_distance=True)\n",
    "    except:\n",
    "        # Handle Expected n_neighbors <= n_samples error\n",
    "        # Add all but exclude itself\n",
    "        neighbors_indices = [\n",
    "            [i for i in range(len(country_df)) if i != j] for j in range(len(country_df))]\n",
    "        neighbors_indices = np.array(neighbors_indices, dtype=int)\n",
    "\n",
    "    # Convert indices to id\n",
    "    ids = country_df[\"id\"].to_numpy()\n",
    "    neighbors_ids = pd.Series(list(neighbors_indices), index=country_df.index).apply(\n",
    "        lambda candidate_indices: ids[candidate_indices])\n",
    "\n",
    "    return neighbors_ids\n",
    "\n",
    "\n",
    "def candidate_selection(train_df, candidate_k):\n",
    "    train_df[\"k_candidates\"] = pd.Series(dtype='object')\n",
    "    uq_countries = train_df[\"country\"].value_counts().index\n",
    "\n",
    "    for country in tqdm(uq_countries):\n",
    "        train_df.loc[train_df[\"country\"] == country, \"k_candidates\"] = \\\n",
    "            country_closest_k(train_df, country, candidate_k)\n",
    "\n",
    "    # Empty candidates\n",
    "    for row in train_df.loc[train_df[\"k_candidates\"].isnull(), \"k_candidates\"].index:\n",
    "        train_df.at[row, \"k_candidates\"] = []\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def forming_pairs_filtering(train_df, th):\n",
    "    pairs = []\n",
    "    dict_ = train_df[\"name_cleaned_shingles_3\"].to_dict()\n",
    "\n",
    "    for p1_idx in tqdm(train_df.index):\n",
    "        for p2_idx in train_df.loc[p1_idx, \"k_candidates\"]:\n",
    "            if p1_idx == p2_idx:  # Skip\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                sim = overlap(dict_[p1_idx], dict_[p2_idx])\n",
    "                if sim >= th:\n",
    "                    pairs.append([p1_idx, p2_idx])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return pd.DataFrame(pairs, columns=[\"p1\", \"p2\"])\n",
    "\n",
    "\n",
    "def recall_knn(df, n_neighbors, label):\n",
    "    print(80*'=')\n",
    "    print('Start KNN grouped by country')\n",
    "    train_df_country = []\n",
    "    for country, country_df in tqdm(df.groupby('country')):\n",
    "        country_df = country_df.reset_index(drop = True)\n",
    "\n",
    "        # distance KNN\n",
    "        neighbors = min(len(country_df), n_neighbors)\n",
    "        knn = KNeighborsRegressor(n_neighbors = neighbors,\n",
    "                                  metric = 'haversine',\n",
    "                                  n_jobs = -1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude', 'longitude']], \n",
    "                                      return_distance = True)\n",
    "\n",
    "        # name KNN\n",
    "        tfidf = TfidfVectorizer(ngram_range=(3, 3), analyzer=\"char_wb\", use_idf=False, stop_words=['unknown'])\n",
    "        x_name = country_df['name_cleaned'].values # CHANGED: name_cleaned\n",
    "        x_name = tfidf.fit_transform(x_name)\n",
    "        \n",
    "        knn_name = NearestNeighbors(n_neighbors = neighbors,\n",
    "                                    metric = 'cosine',\n",
    "                                    n_jobs = -1)\n",
    "        knn_name.fit(x_name)\n",
    "        dists_name, nears_name = knn_name.kneighbors(x_name)\n",
    "        \n",
    "        del tfidf, knn, knn_name, x_name\n",
    "        gc.collect()\n",
    "        \n",
    "        # join distance and name KNNs\n",
    "        for k in range(neighbors):            \n",
    "            cur_df = country_df[['id']]\n",
    "            cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n",
    "            cur_df['kdist_country'] = dists[:, k]\n",
    "            cur_df['kneighbors_country'] = k\n",
    "            \n",
    "            cur_df_name = country_df[['id']]\n",
    "            cur_df_name['match_id'] = country_df['id'].values[nears_name[:, k]]\n",
    "            cur_df_name['kdist_name_country'] = dists_name[:, k]\n",
    "            cur_df_name['kneighbors_name_country'] = k\n",
    "            cur_df = cur_df.merge(cur_df_name, on = ['id', 'match_id'], how = 'outer')\n",
    "            \n",
    "            train_df_country.append(cur_df)\n",
    "    \n",
    "    train_df_country = pd.concat(train_df_country)\n",
    "    train_df_country = train_df_country.drop_duplicates(subset=['id', 'match_id'])\n",
    "    \n",
    "    print('Start KNN for the whole dataset')\n",
    "    train_df = []\n",
    "    knn = NearestNeighbors(n_neighbors = n_neighbors,\n",
    "                           n_jobs = -1)\n",
    "    knn.fit(df[['latitude','longitude']], df.index)\n",
    "    dists, nears = knn.kneighbors(df[['latitude','longitude']])\n",
    "    \n",
    "    for k in range(n_neighbors):            \n",
    "        cur_df = df[['id']]\n",
    "        cur_df['match_id'] = df['id'].values[nears[:, k]]\n",
    "        cur_df['kdist'] = dists[:, k]\n",
    "        cur_df['kneighbors'] = k\n",
    "        train_df.append(cur_df)\n",
    "    \n",
    "    train_df = pd.concat(train_df)\n",
    "    train_df = train_df.merge(train_df_country, on = ['id', 'match_id'], how = 'outer')\n",
    "    \n",
    "    del train_df_country\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_df, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f87781",
   "metadata": {},
   "source": [
    "## Candidate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff8e105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:04.310529Z",
     "iopub.status.busy": "2022-06-26T21:10:04.309910Z",
     "iopub.status.idle": "2022-06-26T21:10:15.837000Z",
     "shell.execute_reply": "2022-06-26T21:10:15.835850Z"
    },
    "papermill": {
     "duration": 11.534607,
     "end_time": "2022-06-26T21:10:15.838876",
     "exception": false,
     "start_time": "2022-06-26T21:10:04.304269",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Start KNN grouped by country\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccd97f232a5492da8d6b977d4c79013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start KNN for the whole dataset\n"
     ]
    }
   ],
   "source": [
    "# Candidate selection, pairs forming\n",
    "#df = candidate_selection(df, 320)\n",
    "#pairs = forming_pairs_filtering(df, 0.2)\n",
    "\n",
    "if CFG.train:\n",
    "    fold0_df_pairs = recall_knn(fold0_df, 20, 'train')[0][[\"id\", \"match_id\"]]\n",
    "    fold0_df_pairs.rename(columns={\"id\": \"p1\", \"match_id\": \"p2\"}, inplace=True)\n",
    "\n",
    "    fold1_df_pairs = recall_knn(fold1_df, 20, 'train')[0][[\"id\", \"match_id\"]]\n",
    "    fold1_df_pairs.rename(columns={\"id\": \"p1\", \"match_id\": \"p2\"}, inplace=True)\n",
    "else:\n",
    "    pairs = recall_knn(df, 20, 'train')[0][[\"id\", \"match_id\"]]\n",
    "    pairs.rename(columns={\"id\": \"p1\", \"match_id\": \"p2\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9a5f3",
   "metadata": {},
   "source": [
    "## Create train target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd52cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\n",
      "Wall time: 4.77 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if CFG.train:\n",
    "    df = df.set_index('id')\n",
    "\n",
    "    ids = fold0_df_pairs['p1'].tolist()\n",
    "    match_ids = fold0_df_pairs['p2'].tolist()\n",
    "    poi = df.loc[ids]['point_of_interest'].values\n",
    "    match_poi = df.loc[match_ids]['point_of_interest'].values\n",
    "    fold0_df_pairs['match'] = np.array(poi == match_poi, dtype = np.int8)\n",
    "\n",
    "    ids = fold1_df_pairs['p1'].tolist()\n",
    "    match_ids = fold1_df_pairs['p2'].tolist()\n",
    "    poi = df.loc[ids]['point_of_interest'].values \n",
    "    match_poi = df.loc[match_ids]['point_of_interest'].values\n",
    "    fold1_df_pairs['match'] = np.array(poi == match_poi, dtype = np.int8)\n",
    "\n",
    "    del poi, match_poi, ids, match_ids\n",
    "    gc.collect()\n",
    "\n",
    "    print('Num of unique train id: %s' % fold0_df_pairs['p1'].nunique())\n",
    "    print('Num of train data: %s' % len(fold0_df_pairs))\n",
    "    print('Pos rate: %s' % fold0_df_pairs['match'].mean())\n",
    "    print('')\n",
    "    print('Num of unique valid id: %s' % fold1_df_pairs['p1'].nunique())\n",
    "    print('Num of valid data: %s' % len(fold1_df_pairs))\n",
    "    print('Pos rate: %s' % fold1_df_pairs['match'].mean())\n",
    "\n",
    "    df = df.reset_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ce074",
   "metadata": {},
   "source": [
    "## Group generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e7a61c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70160a444878484097e701c577cbe3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a003a4d9016e4dbe863dbc1038a9d2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0dc0ee4703476ea1678e341fc87488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e10141e4aa1498aa88c1bb12d9d64d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_poi2id(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('p2')['p1'].apply(set).to_dict()\n",
    "\n",
    "def create_poi_groups(poi2id):\n",
    "    # merge POIs by groups\n",
    "    for p1, values in tqdm(poi2id.items()):\n",
    "        for p2 in values:\n",
    "            if p2 != p1:\n",
    "                poi2id[p1] = poi2id[p1].union(poi2id[p2])\n",
    "                poi2id[p2] = set()\n",
    "    # remove empty groups\n",
    "    poi2id = {k: v for k, v in poi2id.items() if v}\n",
    "    return poi2id\n",
    "\n",
    "def merge_poi_groups(poi2id, poi_length, divider=20):\n",
    "    l = len(poi2id)\n",
    "    keys = list(poi2id.keys())\n",
    "    for i in tqdm(range(l)):\n",
    "        j = i + 1\n",
    "        while 0 < len(poi2id[keys[i]]) < int(poi_length/divider) and j < l:\n",
    "            poi2id[keys[i]] = poi2id[keys[i]].union(poi2id[keys[j]])\n",
    "            poi2id[keys[j]] = set()\n",
    "            j += 1\n",
    "    # remove empty groups\n",
    "    poi2id = {k: v for k, v in poi2id.items() if v}\n",
    "    return poi2id\n",
    "\n",
    "def clean_poi(poi2id):\n",
    "    # clean POIs\n",
    "    values_to_remove = set() \n",
    "    for p1, values in tqdm(poi2id.items()):\n",
    "        values = values.difference(values_to_remove)\n",
    "        values_to_remove = values_to_remove.union(values)\n",
    "        poi2id[p1] = values\n",
    "    # remove empty groups again\n",
    "    poi2id = {k: v for k, v in poi2id.items() if v}\n",
    "    return poi2id\n",
    "\n",
    "def group_generation(pairs, n_splits=10):\n",
    "    # get POI-ID dictionary\n",
    "    poi2id = get_poi2id(pairs)\n",
    "    poi_length = len(poi2id)\n",
    "\n",
    "    # merge poi by groups\n",
    "    poi2id = create_poi_groups(poi2id)\n",
    "\n",
    "    # clean poi\n",
    "    poi2id = clean_poi(poi2id)\n",
    "\n",
    "    # decrease number of group by divider number to increase performance of further processes\n",
    "    poi2id = merge_poi_groups(poi2id, poi_length, 20)\n",
    "\n",
    "    # set groups for pairs\n",
    "    pairs = pairs.set_index('p1')\n",
    "    for idx, values in tqdm(enumerate(poi2id.values()), total=len(poi2id)):\n",
    "        pairs.loc[list(values), 'set'] = idx\n",
    "    pairs = pairs.reset_index()\n",
    "\n",
    "    kf = GroupKFold(n_splits=n_splits)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(pairs, \n",
    "                                                    pairs['set'], \n",
    "                                                    pairs['set'])):\n",
    "        pairs.loc[val_idx, 'group'] = i\n",
    "\n",
    "    pairs['group'] = pairs['group'].astype('int8')\n",
    "    pairs = pairs.drop('set', axis=1)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "if CFG.train:\n",
    "    fold0_df_pairs = group_generation(fold0_df_pairs, CFG.n_splits)\n",
    "    fold1_df_pairs = group_generation(fold1_df_pairs, CFG.n_splits)\n",
    "else:\n",
    "    pairs = group_generation(pairs, CFG.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ffa6d",
   "metadata": {},
   "source": [
    "## Feature Engeneering utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff56a6d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:15.856365Z",
     "iopub.status.busy": "2022-06-26T21:10:15.855954Z",
     "iopub.status.idle": "2022-06-26T21:10:15.918905Z",
     "shell.execute_reply": "2022-06-26T21:10:15.918087Z"
    },
    "papermill": {
     "duration": 0.074288,
     "end_time": "2022-06-26T21:10:15.920802",
     "exception": false,
     "start_time": "2022-06-26T21:10:15.846514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 110)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m110\u001b[0m\n\u001b[0;31m    \"categories_enc\"].astype(np.int32).to_numpy()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(train_df, pairs):\n",
    "\n",
    "    # Candidates count\n",
    "    if \"count1\" not in pairs.columns:\n",
    "        pairs[\"count1\"] = pairs.groupby(\"p1\")[\"p1\"].transform(\"count\")\n",
    "        pairs[\"count1\"] = pairs[\"count1\"].astype(np.int32)\n",
    "\n",
    "    if \"count2\" not in pairs.columns:\n",
    "        pairs[\"count2\"] = pairs.groupby(\"p2\")[\"p2\"].transform(\"count\")\n",
    "        pairs[\"count2\"] = pairs[\"count2\"].astype(np.int32)\n",
    "\n",
    "    # Distance metrics\n",
    "    lat1 = train_df.loc[pairs[\"p1\"], \"latitude\"],\n",
    "    lon1 = train_df.loc[pairs[\"p1\"], \"longitude\"],\n",
    "    lat2 = train_df.loc[pairs[\"p2\"], \"latitude\"],\n",
    "    lon2 = train_df.loc[pairs[\"p2\"], \"longitude\"])\n",
    "    diff_lat = np.abs(lat2-lat1)\n",
    "    diff_lon = np.abs(lon2-lon1)\n",
    "    # Haversine\n",
    "    if \"haversine\" not in pairs.columns:\n",
    "        pairs[\"haversine\"] = haversine_vec(lat1, lon1, lat2, lon2)\n",
    "        pairs[\"haversine\"] = pairs[\"haversine\"].astype(np.float32)\n",
    "    # Manhattan\n",
    "    if \"manhattan\" not in pairs.columns:\n",
    "        pairs['manhattan'] = diff_lat + diff_lon\n",
    "        pairs[\"manhattan\"] = pairs[\"manhattan\"].astype(np.float32)\n",
    "    # Euclidian\n",
    "    if \"euclidean\" not in pairs.columns:   \n",
    "        pairs['euclidean'] = np.sqrt(np.square(diff_lat) + np.square(diff_lon))\n",
    "        pairs[\"euclidean\"] = pairs[\"euclidean\"].astype(np.float32)\n",
    "    \n",
    "    # Name similarity\n",
    "    for name in [\"jaccard\", \"overlap\", \"cosine\"]:\n",
    "        for k in tqdm([2, 3]):\n",
    "            feature_name = f\"name_cleaned_{name}_{k}\"\n",
    "            if feature_name not in pairs.columns:\n",
    "                similarity = get_shingle_similarity(name)\n",
    "                pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"name_cleaned_shingles_{k}\"],\n",
    "                                                 train_df.loc[pairs[\"p2\"], f\"name_cleaned_shingles_{k}\"])\n",
    "                pairs[feature_name] = pairs[feature_name].astype(np.float16)\n",
    "\n",
    "    # Full address similarity\n",
    "    for name in [\"jaccard\", \"overlap\"]:\n",
    "        for k in tqdm([3]):\n",
    "            feature_name = f\"full_address_{name}_{k}\"\n",
    "            if feature_name not in pairs.columns:\n",
    "                similarity = get_shingle_similarity(name)\n",
    "                pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"full_address_cleaned_shingles_{k}\"],\n",
    "                                                 train_df.loc[pairs[\"p2\"], f\"full_address_cleaned_shingles_{k}\"])\n",
    "                pairs[feature_name] = pairs[feature_name].astype(np.float16)\n",
    "\n",
    "    # Name-address similarity\n",
    "    for name in [\"overlap\"]:\n",
    "        for k in tqdm([3]):\n",
    "            feature_name = f\"name_address_{name}_{k}\"\n",
    "            if feature_name not in pairs.columns:\n",
    "                similarity = get_shingle_similarity(name)\n",
    "                pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"name_cleaned_shingles_{k}\"],\n",
    "                                                 train_df.loc[pairs[\"p2\"], f\"full_address_cleaned_shingles_{k}\"])\n",
    "\n",
    "                pairs[feature_name] += similarity(train_df.loc[pairs[\"p1\"], f\"full_address_cleaned_shingles_{k}\"],\n",
    "                                                  train_df.loc[pairs[\"p2\"], f\"name_cleaned_shingles_{k}\"])\n",
    "                pairs[feature_name] = pairs[feature_name] / 2\n",
    "\n",
    "                pairs[feature_name] = pairs[feature_name].astype(np.float16)\n",
    "\n",
    "    # Numbers in name similarity\n",
    "    for name in [\"overlap\"]:\n",
    "        feature_name = f\"numbers_in_name_{name}\"\n",
    "        if feature_name not in pairs.columns:\n",
    "            similarity = get_shingle_similarity(name)\n",
    "            pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_name_shingles_1\"], train_df.loc[pairs[\"p2\"], f\"numbers_in_name_shingles_1\"])\n",
    "            pairs[feature_name] += similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_name_shingles_2\"], train_df.loc[pairs[\"p2\"], f\"numbers_in_name_shingles_2\"])\n",
    "            pairs[feature_name] = pairs[feature_name] / 2\n",
    "\n",
    "            pairs[feature_name] = pairs[feature_name].astype(np.float16)\n",
    "\n",
    "    # Numbers in address similarity\n",
    "#     for name in [\"overlap\"]:\n",
    "#         feature_name = f\"numbers_in_address_{name}\"\n",
    "#         if feature_name not in pairs.columns:\n",
    "#             similarity = get_shingle_similarity(name)\n",
    "#             pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_full_address_shingles_1\"],\n",
    "#                                              train_df.loc[pairs[\"p2\"], f\"numbers_in_full_address_shingles_1\"])\n",
    "#             pairs[feature_name] += similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_full_address_shingles_2\"],\n",
    "#                                               train_df.loc[pairs[\"p2\"], f\"numbers_in_full_address_shingles_2\"])\n",
    "#             pairs[feature_name] = pairs[feature_name] / 2\n",
    "\n",
    "#             pairs[feature_name] = pairs[feature_name].astype(np.float16)\n",
    "\n",
    "#     # Numbers in name-address similarity\n",
    "#     for name in [\"overlap\"]:\n",
    "#         feature_name = f\"numbers_in_name_address_{name}\"\n",
    "#         if feature_name not in pairs.columns:\n",
    "#             similarity = get_shingle_similarity(name)\n",
    "#             pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_name_shingles_1\"],\n",
    "#                                              train_df.loc[pairs[\"p2\"], f\"numbers_in_full_address_shingles_1\"])\n",
    "#             pairs[feature_name] += similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_full_address_shingles_1\"], train_df.loc[pairs[\"p2\"], f\"numbers_in_name_shingles_1\"])\n",
    "#             pairs[feature_name] += similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_name_shingles_2\"],\n",
    "#                                               train_df.loc[pairs[\"p2\"], f\"numbers_in_full_address_shingles_2\"])\n",
    "#             pairs[feature_name] += similarity(train_df.loc[pairs[\"p1\"], f\"numbers_in_full_address_shingles_2\"],\n",
    "#                                               train_df.loc[pairs[\"p2\"], f\"numbers_in_name_shingles_2\"])\n",
    "#             pairs[feature_name] = pairs[feature_name] / 4\n",
    "\n",
    "#             pairs[feature_name] = pairs[feature_name].astype(np.float16)\n",
    "\n",
    "    # Category\n",
    "    if \"categories1\" not in pairs.columns:\n",
    "        pairs[\"categories1\"] = train_df.loc[pairs[\"p1\"],\n",
    "                                            \"categories_enc\"].astype(np.int32).to_numpy()\n",
    "        pairs[\"categories1\"] = pairs[\"categories1\"].astype(np.int32)\n",
    "\n",
    "    if \"categories2\" not in pairs.columns:\n",
    "        pairs[\"categories2\"] = train_df.loc[pairs[\"p2\"],\n",
    "                                            \"categories_enc\"].to_numpy()\n",
    "        pairs[\"categories2\"] = pairs[\"categories2\"].astype(np.int32)\n",
    "        \n",
    "    # Categories text similarity\n",
    "    for name in [\"overlap\", \"jaccard\"]:\n",
    "        feature_name = f\"categories_{name}\"\n",
    "        if feature_name not in pairs.columns:\n",
    "            similarity = get_shingle_similarity(name)\n",
    "            pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"categories_shingles_3\"],\n",
    "                                             train_df.loc[pairs[\"p2\"], f\"categories_shingles_3\"])\n",
    "            pairs[feature_name] = pairs[feature_name].astype(np.float16)\n",
    "            \n",
    "    # Main categories text similarity\n",
    "    for name in [\"jaccard\", \"overlap\", \"cosine\"]:\n",
    "        feature_name = f\"main_category_{name}\"\n",
    "        if feature_name not in pairs.columns:\n",
    "            similarity = get_shingle_similarity(name)\n",
    "            pairs[feature_name] = similarity(train_df.loc[pairs[\"p1\"], f\"main_category_shingles_3\"],\n",
    "                                             train_df.loc[pairs[\"p2\"], f\"main_category_shingles_3\"])\n",
    "            pairs[feature_name] = pairs[feature_name].astype(np.float16) \n",
    "\n",
    "    # Country (same for every pair)\n",
    "    if \"country\" not in pairs.columns:\n",
    "        pairs[\"country1\"] = train_df.loc[pairs[\"p1\"], \"country_enc\"].to_numpy()\n",
    "        pairs[\"country1\"] = pairs[\"country1\"].astype(np.int32)\n",
    "        \n",
    "        pairs[\"country2\"] = train_df.loc[pairs[\"p2\"], \"country_enc\"].to_numpy()\n",
    "        pairs[\"country2\"] = pairs[\"country2\"].astype(np.int32)\n",
    "\n",
    "    # TF-IDF features\n",
    "    for column in [\"name_cleaned\", \"full_address_cleaned\"]:\n",
    "        # for each id and match_id add corresponding TF-IDF vector\n",
    "        # than multiply them elementwise to get similarity\n",
    "        tv_fit = tfidf_d[col]\n",
    "        indexs = [id2index_d[i] for i in pairs['p1']]\n",
    "        match_indexs = [id2index_d[i] for i in pairs['p2']]                    \n",
    "        df[f\"tfidf_trigram_{column}\"] = tv_fit[indexs].multiply(tv_fit[match_indexs]).sum(axis = 1).A.ravel()\n",
    "    \n",
    "    \n",
    "    # Country-based TF-IDF features\n",
    "#     for country in tqdm(train_df[\"country_enc\"].unique()):\n",
    "#         country_df = train_df[train_df[\"country_enc\"] == country]\n",
    "#         country_df[\"country_index\"] = np.arange(len(country_df)).astype(int)\n",
    "\n",
    "#         country_pairs = pairs[pairs[\"country\"] == country]\n",
    "#         country_pairs[\"country_index1\"] = country_df.loc[country_pairs[\"p1\"],\n",
    "#                                                          \"country_index\"].to_numpy()\n",
    "#         country_pairs[\"country_index2\"] = country_df.loc[country_pairs[\"p2\"],\n",
    "#                                                          \"country_index\"].to_numpy()\n",
    "\n",
    "#         index1 = country_pairs[\"country_index1\"].to_numpy()\n",
    "#         index2 = country_pairs[\"country_index2\"].to_numpy()\n",
    "\n",
    "#         for column in [\"name_cleaned\", \"full_address_cleaned\"]:\n",
    "#             try:\n",
    "#                 vectorizer_words = TfidfVectorizer()\n",
    "#                 vectorizer_trigrams = TfidfVectorizer(\n",
    "#                     analyzer=\"char_wb\", ngram_range=(3, 3))\n",
    "#                 words_matrix = vectorizer_words.fit_transform(\n",
    "#                     country_df[column].fillna(\"\"))\n",
    "#                 trigrams_matrix = vectorizer_trigrams.fit_transform(\n",
    "#                     country_df[column].fillna(\"\"))\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "#             pairs.loc[pairs[\"country\"] == country, f\"tfidf_trigram_{column}\"] = \\\n",
    "#                 np.sum(trigrams_matrix[index1].multiply(\n",
    "#                     trigrams_matrix[index2]), axis=1)\n",
    "\n",
    "#             pairs.loc[pairs[\"country\"] == country, f\"tfidf_words_{column}\"] = \\\n",
    "#                 np.sum(words_matrix[index1].multiply(\n",
    "#                     words_matrix[index2]), axis=1)\n",
    "\n",
    "#             pairs[f\"tfidf_trigram_{column}\"] = \\\n",
    "#                 pairs[f\"tfidf_trigram_{column}\"].astype(np.float16)\n",
    "\n",
    "#             pairs[f\"tfidf_words_{column}\"] = \\\n",
    "#                 pairs[f\"tfidf_words_{column}\"].astype(np.float16)\n",
    "            \n",
    "    # Group-by features\n",
    "    # Haversine features\n",
    "    groupby_p1 = pairs.groupby('p1')[\"haversine\"]\n",
    "    groupby_p2 = pairs.groupby('p2')[\"haversine\"]\n",
    "    pairs[f\"p1_haversine_mean\"] = groupby_p1.transform(\n",
    "        np.mean).astype(np.float32)\n",
    "    pairs[f\"p2_haversine_mean\"] = groupby_p2.transform(\n",
    "        np.mean).astype(np.float32)\n",
    "    pairs[f\"p1_haversine_min\"] = groupby_p1.transform(\n",
    "        np.min).astype(np.float32)\n",
    "    pairs[f\"p2_haversine_min\"] = groupby_p2.transform(\n",
    "        np.min).astype(np.float32)\n",
    "    pairs[f\"p1_haversine_max\"] = groupby_p1.transform(\n",
    "        np.max).astype(np.float32)\n",
    "    pairs[f\"p2_haversine_max\"] = groupby_p2.transform(\n",
    "        np.max).astype(np.float32)\n",
    "\n",
    "    pairs[f\"p1_haversine_rank\"] = ((groupby_p1.transform(\n",
    "        \"rank\", method=\"min\") - 1) / pairs[\"count1\"]).astype(np.float16)\n",
    "    pairs[f\"p2_haversine_rank\"] = ((groupby_p2.transform(\n",
    "        \"rank\", method=\"min\") - 1) / pairs[\"count2\"]).astype(np.float16)\n",
    "    \n",
    "    # Manhattan features\n",
    "    groupby_p1 = pairs.groupby('p1')[\"manhattan\"]\n",
    "    groupby_p2 = pairs.groupby('p2')[\"manhattan\"]\n",
    "    pairs[f\"p1_manhattan_mean\"] = groupby_p1.transform(\n",
    "        np.mean).astype(np.float32)\n",
    "    pairs[f\"p2_manhattan_mean\"] = groupby_p2.transform(\n",
    "        np.mean).astype(np.float32)\n",
    "    pairs[f\"p1_manhattan_min\"] = groupby_p1.transform(\n",
    "        np.min).astype(np.float32)\n",
    "    pairs[f\"p2_manhattan_min\"] = groupby_p2.transform(\n",
    "        np.min).astype(np.float32)\n",
    "    pairs[f\"p1_manhattan_max\"] = groupby_p1.transform(\n",
    "        np.max).astype(np.float32)\n",
    "    pairs[f\"p2_manhattan_max\"] = groupby_p2.transform(\n",
    "        np.max).astype(np.float32)\n",
    "\n",
    "    pairs[f\"p1_manhattan_rank\"] = ((groupby_p1.transform(\n",
    "        \"rank\", method=\"min\") - 1) / pairs[\"count1\"]).astype(np.float16)\n",
    "    pairs[f\"p2_manhattan_rank\"] = ((groupby_p2.transform(\n",
    "        \"rank\", method=\"min\") - 1) / pairs[\"count2\"]).astype(np.float16)\n",
    "    \n",
    "    # Euclidian features\n",
    "    groupby_p1 = pairs.groupby('p1')[\"euclidian\"]\n",
    "    groupby_p2 = pairs.groupby('p2')[\"euclidian\"]\n",
    "    pairs[f\"p1_euclidian_mean\"] = groupby_p1.transform(\n",
    "        np.mean).astype(np.float32)\n",
    "    pairs[f\"p2_euclidian_mean\"] = groupby_p2.transform(\n",
    "        np.mean).astype(np.float32)\n",
    "    pairs[f\"p1_euclidian_min\"] = groupby_p1.transform(\n",
    "        np.min).astype(np.float32)\n",
    "    pairs[f\"p2_euclidian_min\"] = groupby_p2.transform(\n",
    "        np.min).astype(np.float32)\n",
    "    pairs[f\"p1_euclidian_max\"] = groupby_p1.transform(\n",
    "        np.max).astype(np.float32)\n",
    "    pairs[f\"p2_euclidian_max\"] = groupby_p2.transform(\n",
    "        np.max).astype(np.float32)\n",
    "\n",
    "    pairs[f\"p1_euclidian_rank\"] = ((groupby_p1.transform(\n",
    "        \"rank\", method=\"min\") - 1) / pairs[\"count1\"]).astype(np.float16)\n",
    "    pairs[f\"p2_euclidian_rank\"] = ((groupby_p2.transform(\n",
    "        \"rank\", method=\"min\") - 1) / pairs[\"count2\"]).astype(np.float16)\n",
    "\n",
    "    # Name features\n",
    "    for feature in [\"name_cleaned_overlap_3\"]:\n",
    "        groupby_p1 = pairs.groupby('p1')[feature]\n",
    "        groupby_p2 = pairs.groupby('p2')[feature]\n",
    "        pairs[f\"p1_{feature}_mean\"] = groupby_p1.transform(\n",
    "            np.mean).astype(np.float16)\n",
    "        pairs[f\"p2_{feature}_mean\"] = groupby_p2.transform(\n",
    "            np.mean).astype(np.float16)\n",
    "        pairs[f\"p1_{feature}_max\"] = groupby_p1.transform(\n",
    "            np.max).astype(np.float16)\n",
    "        pairs[f\"p2_{feature}_max\"] = groupby_p2.transform(\n",
    "            np.max).astype(np.float16)\n",
    "\n",
    "        pairs[f\"p1_{feature}_rank\"] = ((groupby_p1.transform(\n",
    "            \"rank\", method=\"min\") - 1) / pairs[\"count1\"]).astype(np.float16)\n",
    "        pairs[f\"p2_{feature}_rank\"] = ((groupby_p2.transform(\n",
    "            \"rank\", method=\"min\") - 1) / pairs[\"count2\"]).astype(np.float16)\n",
    "\n",
    "    # Address/numbers features: only mean\n",
    "    for feature in [\"full_address_overlap_3\", \"numbers_in_name_overlap\",\n",
    "                    \"numbers_in_address_overlap\", \"numbers_in_name_address_overlap\",\n",
    "                    \"categories_overlap\", \"categories_jaccard\"]:\n",
    "        groupby_p1 = pairs.groupby('p1')[feature]\n",
    "        groupby_p2 = pairs.groupby('p2')[feature]\n",
    "        pairs[f\"p1_{feature}_mean\"] = groupby_p1.transform(\n",
    "            np.mean).astype(np.float16)\n",
    "        pairs[f\"p2_{feature}_mean\"] = groupby_p2.transform(\n",
    "            np.mean).astype(np.float16)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def overlap(profile0, profile1):\n",
    "    union = set()\n",
    "    for k in profile0.keys():\n",
    "        union.add(k)\n",
    "    for k in profile1.keys():\n",
    "        union.add(k)\n",
    "    inter = int(len(profile0.keys()) + len(profile1.keys()) - len(union))\n",
    "    return inter / min(len(profile0), len(profile1))\n",
    "\n",
    "\n",
    "def jaccard(profile0, profile1):\n",
    "    union = set()\n",
    "    for ite in profile0.keys():\n",
    "        union.add(ite)\n",
    "    for ite in profile1.keys():\n",
    "        union.add(ite)\n",
    "    inter = int(len(profile0.keys()) + len(profile1.keys()) - len(union))\n",
    "    return 1.0 * inter / len(union)\n",
    "\n",
    "\n",
    "def cosine(profile0, profile1):\n",
    "    small = profile1\n",
    "    large = profile0\n",
    "    if len(profile0) < len(profile1):\n",
    "        small = profile0\n",
    "        large = profile1\n",
    "    agg = 0.0\n",
    "    for k, v in small.items():\n",
    "        i = large.get(k)\n",
    "        if not i:\n",
    "            continue\n",
    "        agg += 1.0 * v * i\n",
    "    dot_product = agg\n",
    "\n",
    "    agg = 0.0\n",
    "    for k, v in profile0.items():\n",
    "        agg += 1.0 * v * v\n",
    "    profile0_norm = math.sqrt(agg)\n",
    "\n",
    "    agg = 0.0\n",
    "    for k, v in profile1.items():\n",
    "        agg += 1.0 * v * v\n",
    "    profile1_norm = math.sqrt(agg)\n",
    "\n",
    "    return dot_product / (profile0_norm * profile1_norm)\n",
    "\n",
    "\n",
    "def get_shingle_similarity(name):\n",
    "    if name == \"cosine\":\n",
    "        func = cosine\n",
    "    elif name == \"jaccard\":\n",
    "        func = jaccard\n",
    "    elif name == \"overlap\":\n",
    "        func = overlap\n",
    "\n",
    "    func_ = np.vectorize(\n",
    "        lambda x1, x2: pair_func(func, x1, x2))\n",
    "    return func_\n",
    "\n",
    "\n",
    "def haversine_vec(lat1, lon1, lat2, lon2):\n",
    "    def h(la1, lo1, la2, lo2):\n",
    "        return haversine((la1, lo1), (la2, lo2), unit='m')\n",
    "    return np.vectorize(h)(lat1, lon1, lat2, lon2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd484e",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766267e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:15.937776Z",
     "iopub.status.busy": "2022-06-26T21:10:15.936842Z",
     "iopub.status.idle": "2022-06-26T21:10:15.949780Z",
     "shell.execute_reply": "2022-06-26T21:10:15.948726Z"
    },
    "papermill": {
     "duration": 0.023384,
     "end_time": "2022-06-26T21:10:15.951689",
     "exception": false,
     "start_time": "2022-06-26T21:10:15.928305",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset_by_chunks(df, pairs, n_splits, label='train'):\n",
    "    count = 0\n",
    "    pred_df = pd.DataFrame()\n",
    "    \n",
    "    for k in tqdm(range(n_splits)):\n",
    "        # split dataset by chunks\n",
    "        print(f'Current split: {k+1}')\n",
    "        cur_data = pairs[pairs['group'] == k]\n",
    "\n",
    "        # add features & model prediction\n",
    "        cur_data = feature_engineering(df, cur_data)\n",
    "\n",
    "        count += len(cur_data)\n",
    "        \n",
    "        # save dataset\n",
    "        cur_data = cur_data.drop('group', axis=1)\n",
    "        cur_data.to_pickle(f'{CFG.train_path}/{label}_data_{k+1}.pkl')    \n",
    "\n",
    "        del cur_data\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f'Total len is {count}')\n",
    "    return pred_df\n",
    "\n",
    "if CFG.train:\n",
    "    with multiprocessing.Pool(processes=2) as pool:\n",
    "        pool.starmap(generate_dataset_by_chunks, [(fold0_df, fold0_df_pairs, CFG.n_splits, 'train'), \n",
    "                                                  (fold1_df, fold1_df_pairs, CFG.n_splits, 'valid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e00ca",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2edf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_and_predict_by_chunks(df, pairs, models, n_splits):\n",
    "    count = 0\n",
    "    \n",
    "    pred_df = pd.DataFrame()\n",
    "    \n",
    "    for k in tqdm(range(n_splits)):\n",
    "        # split dataset by chunks\n",
    "        print(f'Current split: {k+1}')\n",
    "        cur_data = pairs[pairs['group'] == k]\n",
    "\n",
    "        # add features & model prediction\n",
    "        cur_data = feature_engineering(df, cur_data)\n",
    "\n",
    "        # predict\n",
    "        cur_data = cur_data.drop('group', axis=1)\n",
    "        cur_data['predict_proba'] = np.zeros(len(cur_data), dtype=np.float16)\n",
    "\n",
    "        for model in models:\n",
    "            cat_features = [\"country1\", \"country2\", \"categories1\", \"categories2\"]\n",
    "            num_features = [x for x in cur_data.columns\n",
    "                            if x not in ['p1', 'p2', 'match'] + cat_features + [\"predict_proba\"]]\n",
    "            \n",
    "            pred = np.array([model.predict_proba(cur_data[num_features + cat_features])[:, 1] for model in models])\n",
    "            pred = np.mean(pred, axis=0)\n",
    "\n",
    "        \n",
    "        pred_df = pd.concat([pred_df, cur_data[[\"p1\", \"p2\", \"predict_proba\"]]])\n",
    "\n",
    "        count += len(cur_data)\n",
    "        \n",
    "        del cur_data\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f'Total len is {count}')\n",
    "    return pred_df\n",
    "\n",
    "if CFG.inference:\n",
    "    pred_df = feature_and_predict_by_chunks(df, pairs, models, CFG.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40858a10",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbea115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T21:10:28.832265Z",
     "iopub.status.busy": "2022-06-26T21:10:28.831161Z",
     "iopub.status.idle": "2022-06-26T21:10:28.854047Z",
     "shell.execute_reply": "2022-06-26T21:10:28.853095Z"
    },
    "papermill": {
     "duration": 0.040469,
     "end_time": "2022-06-26T21:10:28.856550",
     "exception": false,
     "start_time": "2022-06-26T21:10:28.816081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Submission\n",
    "# prediction = pred_df[pred_df[\"predict_proba\"] > CFG.threshold][[\"p1\", \"p2\"]].groupby(\"p1\").agg(set)\n",
    "# df[\"prediction\"] = prediction\n",
    "\n",
    "# # Fill empty\n",
    "# for row in df.loc[df[\"prediction\"].isnull(), \"prediction\"].index:\n",
    "#     df.at[row, \"prediction\"] = set()\n",
    "\n",
    "# # Add itself\n",
    "# df.apply(lambda x: x[\"prediction\"].add(x[\"id\"]), axis=1)\n",
    "\n",
    "# # Forming sumbission.csv\n",
    "# df[\"prediction_sub\"] = df[\"prediction\"].apply(lambda x: \" \".join(x))\n",
    "# pd.concat([df['id'], df[\"prediction_sub\"]], axis=1, keys=['id', 'matches']).to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4206cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(df):\n",
    "    id2match = dict(zip(df['id'].values, df['matches'].str.split()))\n",
    "\n",
    "    for base, match in df[['id', 'matches']].values:\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        for m in match:\n",
    "            if base not in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df['matches'] = df['id'].map(id2match).map(' '.join)\n",
    "    return df \n",
    "\n",
    "## Submission    \n",
    "pred_df = pred_df[pred_df['predict_proba'] >= CFG.threshold][['p1', 'p2']]\n",
    "\n",
    "out_df = pd.DataFrame()\n",
    "df = df.reset_index()\n",
    "out_df['id'] = df['id'].unique().tolist()\n",
    "out_df['match_id'] = out_df['id']\n",
    "\n",
    "out_df = pd.concat([out_df, pred_df])\n",
    "out_df = out_df.groupby('id')['match_id'].apply(list).reset_index()\n",
    "out_df['matches'] = out_df['match_id'].apply(lambda x: ' '.join(set(x)))\n",
    "out_df = post_process(out_df)\n",
    "print(f'Unique id: {len(out_df)}')\n",
    "display(out_df.head())\n",
    "\n",
    "out_df[['id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a359c",
   "metadata": {},
   "source": [
    "# Further ideas\n",
    "\n",
    "- ordinal encode main_category and closest_city, set them as categorical features for LGBM\n",
    "- include multilingual encoder for the full_address\n",
    "\n",
    "- Optuna\n",
    "- stacking of the best Optuna solutions\n",
    "\n",
    "- add Catboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.446214,
   "end_time": "2022-06-26T21:10:29.626890",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-26T21:09:53.180676",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
