{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.399414Z",
     "iopub.status.busy": "2022-05-20T23:06:53.399105Z",
     "iopub.status.idle": "2022-05-20T23:06:53.403901Z",
     "shell.execute_reply": "2022-05-20T23:06:53.403266Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.399358Z"
    },
    "id": "pouTaX_llCrb",
    "outputId": "c050ef25-9133-4c86-a9fa-f0b76527faac"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.408122Z",
     "iopub.status.busy": "2022-05-20T23:06:53.407759Z",
     "iopub.status.idle": "2022-05-20T23:06:53.948805Z",
     "shell.execute_reply": "2022-05-20T23:06:53.947555Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.408094Z"
    },
    "id": "H5QntWoelAkH",
    "outputId": "31efe7df-24ff-40e8-8517-0c7174968413"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "# import cudf\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "import difflib\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "from requests import get\n",
    "\n",
    "CFG = Namespace(\n",
    "    kaggle = False,\n",
    "    seed = 46,\n",
    "    train = False,\n",
    "    validate = False,\n",
    "    inference = True,\n",
    "    target = \"point_of_interest\",\n",
    "    n_neighbors = 50,\n",
    "    n_neighbors_in_cols = 5,\n",
    "    n_splits = 3\n",
    ")\n",
    "\n",
    "random.seed(CFG.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.950215Z",
     "iopub.status.busy": "2022-05-20T23:06:53.949976Z",
     "iopub.status.idle": "2022-05-20T23:06:59.622657Z",
     "shell.execute_reply": "2022-05-20T23:06:59.621792Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.950187Z"
    },
    "id": "wz7JepVilAkN",
    "outputId": "0652de28-9bd3-4ab7-c97c-55e6e11935e6",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>country</th>\n",
       "      <th>url</th>\n",
       "      <th>phone</th>\n",
       "      <th>categories</th>\n",
       "      <th>point_of_interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_000001272c6c5d</td>\n",
       "      <td>Café Stad Oudenaarde</td>\n",
       "      <td>50.859975</td>\n",
       "      <td>3.634196</td>\n",
       "      <td>Abdijstraat</td>\n",
       "      <td>Nederename</td>\n",
       "      <td>Oost-Vlaanderen</td>\n",
       "      <td>9700</td>\n",
       "      <td>BE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bars</td>\n",
       "      <td>P_677e840bb6fc7e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_000002eae2a589</td>\n",
       "      <td>Carioca Manero</td>\n",
       "      <td>-22.907225</td>\n",
       "      <td>-43.178244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazilian Restaurants</td>\n",
       "      <td>P_d82910d8382a83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_000007f24ebc95</td>\n",
       "      <td>ร้านตัดผมการาเกด</td>\n",
       "      <td>13.780813</td>\n",
       "      <td>100.484900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salons / Barbershops</td>\n",
       "      <td>P_b1066599e78477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_000008a8ba4f48</td>\n",
       "      <td>Turkcell</td>\n",
       "      <td>37.844510</td>\n",
       "      <td>27.844202</td>\n",
       "      <td>Adnan Menderes Bulvarı</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mobile Phone Shops</td>\n",
       "      <td>P_b2ed86905a4cd3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_00001d92066153</td>\n",
       "      <td>Restaurante Casa Cofiño</td>\n",
       "      <td>43.338196</td>\n",
       "      <td>-4.326821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caviedes</td>\n",
       "      <td>Cantabria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spanish Restaurants</td>\n",
       "      <td>P_809a884d4407fb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                     name   latitude   longitude  \\\n",
       "0  E_000001272c6c5d     Café Stad Oudenaarde  50.859975    3.634196   \n",
       "1  E_000002eae2a589           Carioca Manero -22.907225  -43.178244   \n",
       "2  E_000007f24ebc95         ร้านตัดผมการาเกด  13.780813  100.484900   \n",
       "3  E_000008a8ba4f48                 Turkcell  37.844510   27.844202   \n",
       "4  E_00001d92066153  Restaurante Casa Cofiño  43.338196   -4.326821   \n",
       "\n",
       "                  address        city            state   zip country  url  \\\n",
       "0             Abdijstraat  Nederename  Oost-Vlaanderen  9700      BE  NaN   \n",
       "1                     NaN         NaN              NaN   NaN      BR  NaN   \n",
       "2                     NaN         NaN              NaN   NaN      TH  NaN   \n",
       "3  Adnan Menderes Bulvarı         NaN              NaN   NaN      TR  NaN   \n",
       "4                     NaN    Caviedes        Cantabria   NaN      ES  NaN   \n",
       "\n",
       "  phone             categories point_of_interest  \n",
       "0   NaN                   Bars  P_677e840bb6fc7e  \n",
       "1   NaN  Brazilian Restaurants  P_d82910d8382a83  \n",
       "2   NaN   Salons / Barbershops  P_b1066599e78477  \n",
       "3   NaN     Mobile Phone Shops  P_b2ed86905a4cd3  \n",
       "4   NaN    Spanish Restaurants  P_809a884d4407fb  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"foursquare_location_matching/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "## Sort categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:59.625612Z",
     "iopub.status.busy": "2022-05-20T23:06:59.625293Z",
     "iopub.status.idle": "2022-05-20T23:06:59.969411Z",
     "shell.execute_reply": "2022-05-20T23:06:59.968569Z",
     "shell.execute_reply.started": "2022-05-20T23:06:59.625570Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sort_categories(cat):\n",
    "    if cat != cat:\n",
    "        return np.nan\n",
    "    return ', '.join(sorted(cat.split(', ')))\n",
    "\n",
    "train['categories'] = train['categories'].apply(sort_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode name  in unicode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:59.973840Z",
     "iopub.status.busy": "2022-05-20T23:06:59.970646Z",
     "iopub.status.idle": "2022-05-20T23:07:01.998556Z",
     "shell.execute_reply": "2022-05-20T23:07:01.997919Z",
     "shell.execute_reply.started": "2022-05-20T23:06:59.973801Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode(col):\n",
    "    if col != col:\n",
    "        return np.nan\n",
    "    return unidecode(col)\n",
    "\n",
    "train['name'] = train['name'].apply(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:02.000825Z",
     "iopub.status.busy": "2022-05-20T23:07:01.999925Z",
     "iopub.status.idle": "2022-05-20T23:07:02.511541Z",
     "shell.execute_reply": "2022-05-20T23:07:02.510787Z",
     "shell.execute_reply.started": "2022-05-20T23:07:02.000783Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_name(col):\n",
    "    if col != col:\n",
    "        return np.nan\n",
    "    return col.lower()\\\n",
    "              .replace(\",\", \"\")\\\n",
    "              .replace(\".\", \"\")\\\n",
    "              .replace(\"'\", \"\")\\\n",
    "              .replace(\"the \", \"\")\n",
    "\n",
    "train['name'] = train['name'].apply(clean_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make same categories for the same names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:02.512763Z",
     "iopub.status.busy": "2022-05-20T23:07:02.512539Z",
     "iopub.status.idle": "2022-05-20T23:07:02.589150Z",
     "shell.execute_reply": "2022-05-20T23:07:02.588346Z",
     "shell.execute_reply.started": "2022-05-20T23:07:02.512738Z"
    }
   },
   "outputs": [],
   "source": [
    "# brings shops with similar names to one name\n",
    "train.loc[train['name']=='mc donalds', 'name'] = 'mcdonalds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add main category to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:02.593477Z",
     "iopub.status.busy": "2022-05-20T23:07:02.590690Z",
     "iopub.status.idle": "2022-05-20T23:07:06.852054Z",
     "shell.execute_reply": "2022-05-20T23:07:06.851144Z",
     "shell.execute_reply.started": "2022-05-20T23:07:02.593440Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cat_freq = dict()\n",
    "stop_words = ['/', '&', 'or', 'High', 'Miscellaneous', 'Fast', 'Other', 'Asian', 'Chinese', 'Event', \n",
    "              'Great', 'Noodle', 'Burger', 'Seafood', 'Breakfast', 'Ice', 'Diners', 'Cream', 'Indonesian', \n",
    "              'Thai', \"Women's\", 'Fried', 'Snack', 'Tea', 'Mexican', 'Nail', 'Sushi', 'Middle', 'Korean', \n",
    "              'Gift', 'Drink', 'Pet', 'Turkish', \"Men's\", 'Indian', 'Malay', 'Cocktail', 'Donut', 'Box', \n",
    "              'Condos)', 'Residential', 'Convenience', 'Gas', 'General', 'Bus', 'Pizza', 'Spaces', 'Mobile',\n",
    "              'Phone', 'Academic', 'Japanese', 'Business', 'Shoe', 'Italian', 'American', 'Home', 'Auto', \n",
    "              'Furniture', 'Cosmetics', 'Sandwich', 'Dessert', 'Car', 'Arts', 'Financial', 'Legal', 'BBQ',\n",
    "              'Hardware', 'Video', 'Music', 'Art', 'Student', 'Jewelry', 'Historic', 'Travel', 'Washes',\n",
    "              'Beer', 'Arcades', 'Bike', 'Lookouts', 'Scenic', 'Rental', 'Accessories', 'Repairs', 'Discount', \n",
    "              'Optical', 'Bodegas', 'Big', 'Assisted', 'Living', 'Athletics', 'Agencies', 'Locations', 'Trails', \n",
    "              'Bed', 'Breakfasts', 'Wine', 'Real', 'Elementary', 'Theme', 'Golf', 'Rest',  'Photography', \n",
    "              'Nightlife', 'Courses', 'Convention', 'Eastern', 'Concert', 'Conference', 'Startups', 'Tech', \n",
    "              'Meeting', 'French', 'Supplies', 'Events', 'Sake', 'Dog', 'Ramen', 'City', 'Juice', 'Science',\n",
    "              'Liquor', 'Lawyers', 'Insurance', 'Flower', 'Toy', 'Rentals', 'Paper', 'Flea', 'Bases', 'Baseball', \n",
    "              'Karaoke', 'Kids', 'Design', 'Farmers', 'Repair', 'Technology', 'Wards', 'Water', 'Supply', \n",
    "              'Filipino', 'Piers', 'Salad', 'Mattress', 'Print', 'Wings', 'Engineering', 'Non-Profits', \n",
    "              'Gastropubs', 'Bistros', 'Hot', 'Vietnamese', 'Hookah', 'Candy', 'Coffee', 'Electronics',\n",
    "              'Department', 'Clothing', 'Trucks', 'Chicken', 'Movie', 'Health', 'Soccer', 'Crafts', \n",
    "              'Game', 'Community', 'Food', 'College', 'Sporting', 'Beauty', 'Ferries', 'Soup', 'Veterinarians', \n",
    "              'Basketball', 'Light', 'Rail', 'Taco', 'Classrooms', 'Shopping', 'Developments', 'Train', 'Performing',\n",
    "              'Administrative', 'Lingerie', 'Dive', 'Storage', 'Office', 'Landscaping', 'Residence', 'Sports',\n",
    "              'Goods', 'Dealerships', 'Grocery', 'Workshops', 'History'\n",
    "             ]\n",
    "\n",
    "\n",
    "def get_categories(category):\n",
    "    if category == 'Auto':\n",
    "        return 'Automotive'\n",
    "    if category == 'Hotel' or category == 'Motels' or category == 'Hostels':\n",
    "        return 'Hotels'\n",
    "    if category == 'Courthouses':\n",
    "        return 'Court'\n",
    "    if category == 'College':\n",
    "        return 'Colleges'\n",
    "    if category == 'Cafés':\n",
    "        return 'Cafes'\n",
    "    if category == \"Doctor's\" or category == \"Dentist's\" or category == \"Doctors\":\n",
    "        return 'Medical'\n",
    "    if category == '(Apartments':\n",
    "        return 'Apartments'\n",
    "    return category\n",
    "\n",
    "if CFG.kaggle:\n",
    "    cat_freq = pd.read_csv('../input/foursquare-main-categories/cat_freq.csv', index_col='Unnamed: 0')\n",
    "else:\n",
    "    cat_freq = pd.read_csv('foursquare_main_categories/cat_freq.csv', index_col='Unnamed: 0')\n",
    "\n",
    "cat_freq_dict = dict(zip(cat_freq['category'], cat_freq['frequence']))\n",
    "\n",
    "def get_main_category(category):\n",
    "    if category == category:\n",
    "        category_list = re.split(', | ', category)\n",
    "        most_freq_cat = np.nan\n",
    "        freq = 0\n",
    "        \n",
    "        for c in category_list:\n",
    "            if c in stop_words or c[-2:] == 'an':\n",
    "                continue\n",
    "            c = get_categories(c)\n",
    "            f = cat_freq_dict.get(c, 0)\n",
    "            if f > freq:\n",
    "                freq = f\n",
    "                most_freq_cat = c\n",
    "        \n",
    "        return most_freq_cat\n",
    "            \n",
    "    return np.nan\n",
    "\n",
    "train['main_categories'] = train['categories'].apply(get_main_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing data with data from outer sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:06.853494Z",
     "iopub.status.busy": "2022-05-20T23:07:06.853225Z",
     "iopub.status.idle": "2022-05-20T23:07:06.857561Z",
     "shell.execute_reply": "2022-05-20T23:07:06.856630Z",
     "shell.execute_reply.started": "2022-05-20T23:07:06.853463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# states = pd.read_csv('states.csv', index_col='Unnamed: 0')\n",
    "cities = pd.read_csv('additional_data/cities.csv', encoding = \"ISO-8859-1\")\n",
    "cities = cities[['asciiname', 'latitude', 'longitude', 'country code']]\n",
    "cities.rename({'asciiname': 'city', 'country code': 'country'}, axis=1, inplace=True)\n",
    "\n",
    "# starbucks = pd.read_csv('additional_data/starbucks.csv', index_col='Unnamed: 0')\n",
    "# starbucks = starbucks[['countryCode', 'latitude', 'longitude', 'streetAddressLine2', 'city']]\n",
    "# starbucks.rename({'countryCode': 'country', 'streetAddressLine2': 'address'}, axis=1, inplace=True)\n",
    "# starbucks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing data by finding closest neighbors from outer sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:06.859067Z",
     "iopub.status.busy": "2022-05-20T23:07:06.858838Z",
     "iopub.status.idle": "2022-05-20T23:07:06.874674Z",
     "shell.execute_reply": "2022-05-20T23:07:06.874014Z",
     "shell.execute_reply.started": "2022-05-20T23:07:06.859040Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['country'] = train['country'].fillna('NA')\n",
    "# train['closest_city'] = ''\n",
    "\n",
    "geoname_dict = {'city': cities}\n",
    "\n",
    "def fill_the_missing_data(args):#, df_dists):\n",
    "    country, country_df = args\n",
    "    dfs = []\n",
    "    columns = list(geoname_dict.keys())\n",
    "    for c in tqdm(columns):\n",
    "\n",
    "            geoname_df = geoname_dict[c]\n",
    "            \n",
    "#             country_df = country_df[(country_df[c].isnull())]\n",
    "            geoname_df = geoname_df[geoname_df['country'] == country]\n",
    "                \n",
    "            if len(country_df) == 0 or len(geoname_df) == 0:\n",
    "                continue\n",
    "            \n",
    "            knn = KNeighborsRegressor(n_neighbors=min(len(geoname_df), 2), metric='haversine', n_jobs=-1)\n",
    "            knn.fit(geoname_df[['latitude','longitude']], geoname_df.index)\n",
    "            dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "            \n",
    "            if nears.shape[1] < 2:\n",
    "                continue\n",
    "            \n",
    "            nears[:,1] = nears[:,0]\n",
    "            nears[:,0] = country_df.index\n",
    "#             dists = dists[:,0]*6371000\n",
    "            \n",
    "#             nears = nears[dists<=df_dists[c]]\n",
    "            \n",
    "#             display(nears)\n",
    "#             display(dists)\n",
    "            \n",
    "            for n in nears:\n",
    "                t_idx = n[0]\n",
    "                c_idx = n[1]\n",
    "                country_df.loc[t_idx, f\"closest_{c}\"] = geoname_df.iloc[c_idx][c]\n",
    "#                 display(df.loc[t_idx])\n",
    "#                 display(geoname_df.iloc[c_idx])\n",
    "                       \n",
    "    return country_df\n",
    "    \n",
    "    \n",
    "num_countries = train['country'].nunique()\n",
    "    \n",
    "# processes = multiprocessing.cpu_count()\n",
    "# with multiprocessing.Pool(processes=processes) as pool:\n",
    "#     dfs = pool.imap_unordered(fill_the_missing_data, train.groupby('country', sort=False))\n",
    "#     dfs = tqdm(dfs, total=num_countries)\n",
    "#     dfs = list(dfs)\n",
    "    \n",
    "# train = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "# del cities\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring all object columns to low register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:06.876285Z",
     "iopub.status.busy": "2022-05-20T23:07:06.875857Z",
     "iopub.status.idle": "2022-05-20T23:07:11.294594Z",
     "shell.execute_reply": "2022-05-20T23:07:11.293738Z",
     "shell.execute_reply.started": "2022-05-20T23:07:06.876241Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_lower(df):\n",
    "    for c in df.columns:\n",
    "#         df[c] = df[c].fillna('')\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "    return df\n",
    "    \n",
    "train = to_lower(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yJIRkRD3jr-"
   },
   "source": [
    "## Search Candidates Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.296604Z",
     "iopub.status.busy": "2022-05-20T23:07:11.296073Z",
     "iopub.status.idle": "2022-05-20T23:07:11.304539Z",
     "shell.execute_reply": "2022-05-20T23:07:11.303490Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.296561Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def create_target(row):\n",
    "#     if row[CFG.target] == row['near_target_0']:\n",
    "#         return 1\n",
    "#     return 0\n",
    "\n",
    "# def add_neighbor_features(df, train_mode=True):\n",
    "#     dfs = None\n",
    "#     columns = ['id', 'name', 'address', 'city', 'state',\n",
    "#            'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "#     for c in columns:\n",
    "#         if c != \"id\":\n",
    "#             df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "#     for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "#         dfs_list = list()\n",
    "#         country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "#         knn = KNeighborsRegressor(n_neighbors=min(len(country_df), CFG.n_neighbors), \n",
    "#                                   metric='haversine', n_jobs=-1)\n",
    "#         knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "#         dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "\n",
    "#         targets = country_df[CFG.target].values\n",
    "        \n",
    "#         for i in range(min(len(country_df)-1, 1), min(len(country_df), CFG.n_neighbors)): # 200\n",
    "#             for j in range(min(len(country_df)-1, 1), min(len(country_df), CFG.n_neighbors)): # 200\n",
    "#                 if j > CFG.n_neighbors_in_cols and j != i: \n",
    "#                     continue\n",
    "#                 elif j < i:\n",
    "#                     country_df[f\"d_near_{j}\"] = dists[:, j]\n",
    "#                     country_df[f\"near_target_{j}\"] = targets[nears[:, j]]\n",
    "#                     for c in columns:\n",
    "#                         country_df[f\"near_{c}_{j}\"] = country_df[c].values[nears[:, j]]\n",
    "#                 elif j > i:\n",
    "#                     country_df[f\"d_near_{j-1}\"] = dists[:, j]\n",
    "#                     country_df[f\"near_target_{j-1}\"] = targets[nears[:, j]]\n",
    "#                     for c in columns:\n",
    "#                         country_df[f\"near_{c}_{j-1}\"] = country_df[c].values[nears[:, j]]\n",
    "#                 else:\n",
    "#                     country_df[f\"d_near_0\"] = dists[:, j]\n",
    "#                     country_df[f\"near_target_0\"] = targets[nears[:, j]]\n",
    "#                     for c in columns:\n",
    "#                         country_df[f\"near_{c}_0\"] = country_df[c].values[nears[:, j]]    \n",
    "\n",
    "#             for j in range(min(len(country_df), CFG.n_neighbors), CFG.n_neighbors):\n",
    "#                 country_df[f\"d_near_{j}\"] = np.nan\n",
    "#                 country_df[f\"near_target_{j}\"] = np.nan\n",
    "#                 for c in columns:\n",
    "#                     country_df[f\"near_{c}_{j}\"] = np.nan\n",
    "\n",
    "#             if train_mode:\n",
    "#                 df['target'] = df.apply(country_df, axis=1)\n",
    "            \n",
    "#             dfs_list.append(country_df.copy())\n",
    "                            \n",
    "#         res = pd.concat(dfs_list)\n",
    "                            \n",
    "#         if dfs is not None:\n",
    "#             dfs = pd.concat([dfs, res])\n",
    "#         else:\n",
    "#             dfs = res.copy()\n",
    "                            \n",
    "#         del res\n",
    "#         gc.collect()\n",
    "\n",
    "#     return dfs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.308464Z",
     "iopub.status.busy": "2022-05-20T23:07:11.308203Z",
     "iopub.status.idle": "2022-05-20T23:07:11.421300Z",
     "shell.execute_reply": "2022-05-20T23:07:11.420367Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.308433Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_target(target, near_target):\n",
    "    if target == near_target:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def add_neighbour_features(df, train_mode=True):\n",
    "    dfs = None\n",
    "    columns = ['id', 'name', 'address', 'city', 'state', # 'closest_city', \n",
    "           'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        dfs_list = list()\n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), CFG.n_neighbors), \n",
    "                                  metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "\n",
    "        targets = country_df[CFG.target].values\n",
    "        \n",
    "        if len(country_df) == 1:\n",
    "            country_df[f\"d_near\"] = np.nan\n",
    "            country_df[f\"near_target\"] = np.nan\n",
    "            for c in columns:\n",
    "                country_df[f\"near_{c}\"] = np.nan\n",
    "            if train_mode:\n",
    "                country_df['target'] = np.nan\n",
    "                \n",
    "            dfs_list.append(country_df.copy())\n",
    "            \n",
    "        else:\n",
    "            for i in range(1, min(len(country_df), CFG.n_neighbors)): # 200\n",
    "                country_df[f\"d_near\"] = dists[:, i]\n",
    "                country_df[f\"near_target\"] = targets[nears[:, i]]\n",
    "                for c in columns:\n",
    "                    country_df[f\"near_{c}\"] = country_df[c].values[nears[:, i]]\n",
    "\n",
    "                if train_mode:\n",
    "                    country_df['target'] = country_df.apply(create_target, axis=1)\n",
    "                \n",
    "                dfs_list.append(country_df.copy())\n",
    "            \n",
    "        res = pd.concat(dfs_list)\n",
    "        \n",
    "        del dists, nears, targets, dfs_list\n",
    "        gc.collect()\n",
    "        \n",
    "        if dfs is not None:\n",
    "            dfs = pd.concat([dfs, res])\n",
    "        else:\n",
    "            dfs = res.copy()\n",
    "            \n",
    "        del res\n",
    "        gc.collect()\n",
    "    \n",
    "    return dfs.reset_index(drop=True)\n",
    "\n",
    "def add_neighbour_features_low_mem(country_df):\n",
    "    dfs_list = list()\n",
    "    columns = ['id', 'name', 'address', 'city', 'state', # 'closest_city', \n",
    "           'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "    \n",
    "    knn = KNeighborsRegressor(n_neighbors=min(len(country_df), CFG.n_neighbors), \n",
    "                              metric='haversine', n_jobs=-1)\n",
    "    knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "    dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "\n",
    "    targets = country_df[CFG.target].values\n",
    "\n",
    "    if len(country_df) == 1:\n",
    "        country_df[f\"d_near\"] = np.nan\n",
    "        country_df[f\"near_target\"] = np.nan\n",
    "        for c in columns:\n",
    "            country_df[f\"near_{c}\"] = '' \n",
    "        country_df['target'] = [*map(create_target, country_df[CFG.target], country_df[\"near_target\"])]\n",
    "\n",
    "        dfs_list.append(country_df.copy())\n",
    "\n",
    "    else:\n",
    "        for i in range(1, min(len(country_df), CFG.n_neighbors)): # 200\n",
    "            country_df[f\"d_near\"] = dists[:, i].astype(float)\n",
    "            country_df[f\"near_target\"] = targets[nears[:, i]]\n",
    "            for c in columns:\n",
    "                country_df[f\"near_{c}\"] = country_df[c].values[nears[:, i]]\n",
    "            country_df['target'] = [*map(create_target, country_df[CFG.target], country_df[\"near_target\"])]\n",
    "            \n",
    "            dfs_list.append(country_df.copy())\n",
    "            \n",
    "    del knn, dists, nears, targets\n",
    "    gc.collect()\n",
    "    \n",
    "    return pd.concat(dfs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the kernel (to avoid OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.422818Z",
     "iopub.status.busy": "2022-05-20T23:07:11.422393Z",
     "iopub.status.idle": "2022-05-20T23:07:11.436199Z",
     "shell.execute_reply": "2022-05-20T23:07:11.435248Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.422773Z"
    }
   },
   "outputs": [],
   "source": [
    "# %reset --aggressive -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k59Vk9d5Pmx"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.438632Z",
     "iopub.status.busy": "2022-05-20T23:07:11.438405Z",
     "iopub.status.idle": "2022-05-20T23:07:11.951488Z",
     "shell.execute_reply": "2022-05-20T23:07:11.950653Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.438591Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.952759Z",
     "iopub.status.busy": "2022-05-20T23:07:11.952538Z",
     "iopub.status.idle": "2022-05-20T23:07:11.960409Z",
     "shell.execute_reply": "2022-05-20T23:07:11.959531Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.952733Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.961703Z",
     "iopub.status.busy": "2022-05-20T23:07:11.961485Z",
     "iopub.status.idle": "2022-05-20T23:07:11.973654Z",
     "shell.execute_reply": "2022-05-20T23:07:11.973111Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.961669Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Levenshtein\n",
    "# import difflib\n",
    "\n",
    "# def _add_distance_features(args):\n",
    "#     _, df = args\n",
    "\n",
    "#     columns = ['name', 'address', 'city', 'closest_city', 'state',\n",
    "#            'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "#     for i in tqdm(range(CFG.n_neighbors)):\n",
    "#         for c in columns:\n",
    "#             geshs = []\n",
    "#             levens = []\n",
    "#             jaros = []\n",
    "#             lcss = []\n",
    "#             for str1, str2 in df[[c, f\"near_{c}_{i}\"]].values.astype(str):\n",
    "#                 if str1==str1 and str2==str2:\n",
    "#                     geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "#                     levens.append(Levenshtein.distance(str1, str2))\n",
    "#                     jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "#                     lcss.append(LCS(str(str1), str(str2)))\n",
    "#                 else:\n",
    "#                     geshs.append(-1)\n",
    "#                     levens.append(-1)\n",
    "#                     jaros.append(-1)\n",
    "#             df[f\"near_{c}_{i}_gesh\"] = geshs\n",
    "#             df[f\"near_{c}_{i}_leven\"] = levens\n",
    "#             df[f\"near_{c}_{i}_jaro\"] = jaros\n",
    "#             df[f\"near_{c}_{i}_lcs\"] = lcss\n",
    "            \n",
    "#             if not c in ['country', \"phone\", \"zip\"]:\n",
    "#                 df[f\"near_{c}_{i}_len\"] = df[f\"near_{c}_{i}\"].astype(str).map(len)\n",
    "#                 df[f\"near_{c}_{i}_nleven\"] = df[f\"near_{c}_{i}_leven\"] / df[[f\"near_{c}_{i}_len\", f\"near_{c}_0_len\"]].max(axis=1)\n",
    "#                 df[f\"near_{c}_{i}_nlcsi\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_{i}_len\"]\n",
    "#                 df[f\"near_{c}_{i}_nlcs0\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_0_len\"]\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def add_distance_features(df):\n",
    "#     processes = multiprocessing.cpu_count()-1\n",
    "#     with multiprocessing.Pool(processes=processes) as pool:\n",
    "#         dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "#         dfs = tqdm(dfs)\n",
    "#         dfs = list(dfs)\n",
    "#     df = pd.concat(dfs)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# if CFG.train:\n",
    "#     train = pd.concat([\n",
    "#         add_distance_features(train[train[\"set\"]==0]), \n",
    "#         add_distance_features(train[train[\"set\"]==1]),\n",
    "#         add_distance_features(train[train[\"set\"]==2]), \n",
    "#         add_distance_features(train[train[\"set\"]==3]),\n",
    "#         add_distance_features(train[train[\"set\"]==4]), \n",
    "#         add_distance_features(train[train[\"set\"]==5]),\n",
    "#         add_distance_features(train[train[\"set\"]==6]), \n",
    "#         add_distance_features(train[train[\"set\"]==7])\n",
    "#     ])\n",
    "\n",
    "# if n_test_splits == 5:\n",
    "#     test = pd.concat([\n",
    "#         add_distance_features(test[test[\"set\"]==0]), \n",
    "#         add_distance_features(test[test[\"set\"]==1]),\n",
    "#         add_distance_features(test[test[\"set\"]==2]), \n",
    "#         add_distance_features(test[test[\"set\"]==3]),\n",
    "#         add_distance_features(test[test[\"set\"]==4]), \n",
    "#         add_distance_features(test[test[\"set\"]==5])\n",
    "#     ])\n",
    "# else:\n",
    "#     test = pd.concat([\n",
    "#             add_distance_features(test[test[\"set\"]==0]), \n",
    "#             add_distance_features(test[test[\"set\"]==1]),\n",
    "#             add_distance_features(test[test[\"set\"]==2]), \n",
    "#             add_distance_features(test[test[\"set\"]==3]),\n",
    "#             add_distance_features(test[test[\"set\"]==4]), \n",
    "#             add_distance_features(test[test[\"set\"]==5]),\n",
    "#             add_distance_features(test[test[\"set\"]==6]), \n",
    "#             add_distance_features(test[test[\"set\"]==7])\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:11.975439Z",
     "iopub.status.busy": "2022-05-20T23:07:11.975007Z",
     "iopub.status.idle": "2022-05-20T23:07:12.328336Z",
     "shell.execute_reply": "2022-05-20T23:07:12.327575Z",
     "shell.execute_reply.started": "2022-05-20T23:07:11.975401Z"
    }
   },
   "outputs": [],
   "source": [
    "def seq_match_distance(str1, str2):\n",
    "#     if str1 == '' or str1 == '':\n",
    "#         return -1\n",
    "    return difflib.SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "def lev_distance(str1, str2):\n",
    "#     if str1 == '' or str1 == '':\n",
    "#         return -1\n",
    "    return Levenshtein.distance(str1, str2)\n",
    "\n",
    "def jw_distance(str1, str2):\n",
    "#     if str1 == '' or str1 == '':\n",
    "#         return -1\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n",
    "\n",
    "def lcs_distance(str1, str2):\n",
    "#     if str1 == '' or str1 == '':\n",
    "#         return -1\n",
    "    return LCS(str1, str2)\n",
    "\n",
    "def get_distances(str1, str2):\n",
    "#     if str1 == '' or str1 == '':\n",
    "#         return -1, -1, -1, -1\n",
    "    return difflib.SequenceMatcher(None, str1, str2).ratio(), \\\n",
    "           Levenshtein.distance(str1, str2), \\\n",
    "           Levenshtein.jaro_winkler(str1, str2), \\\n",
    "           LCS(str1, str2)\n",
    "\n",
    "def add_distance_features(df):\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state', 'zip', # 'closest_city', \n",
    "               'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "\n",
    "    for c in columns:\n",
    "        df[f\"near_{c}_gesh\"]=[*map(seq_match_distance, df[c], df[f\"near_{c}\"])]\n",
    "        df[f\"near_{c}_leven\"]=[*map(lev_distance, df[c], df[f\"near_{c}\"])]\n",
    "        df[f\"near_{c}_jaro\"]=[*map(jw_distance, df[c], df[f\"near_{c}\"])]\n",
    "        df[f\"near_{c}_lcs\"]=[*map(lcs_distance, df[c], df[f\"near_{c}\"])]\n",
    "#         df[f\"near_{c}_gesh\"], df[f\"near_{c}_leven\"], \\\n",
    "#         df[f\"near_{c}_jaro\"], df[f\"near_{c}_lcs\"] = zip(*map(get_distances, df[c], df[f\"near_{c}\"]))\n",
    "\n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"near_{c}_len\"] = df[f\"near_{c}\"].astype(str).map(len)\n",
    "            df[f\"near_{c}_nleven\"] = df[f\"near_{c}_leven\"] / df[f\"near_{c}_len\"]\n",
    "            df[f\"near_{c}_nlcsi\"] = df[f\"near_{c}_lcs\"] / df[f\"near_{c}_len\"]\n",
    "            df[f\"near_{c}_nlcs0\"] = df[f\"near_{c}_lcs\"] / df[f\"near_{c}_len\"]\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce memory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.329694Z",
     "iopub.status.busy": "2022-05-20T23:07:12.329467Z",
     "iopub.status.idle": "2022-05-20T23:07:12.335613Z",
     "shell.execute_reply": "2022-05-20T23:07:12.334830Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.329666Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem(df, train_mode=False):\n",
    "    for f in features:\n",
    "        if f not in df.columns:\n",
    "            df[f] = np.nan\n",
    "\n",
    "    if train_mode:\n",
    "        df = df[features + [CFG.target, \"target\", \"id\"] + [\"near_id\"]]\n",
    "        df[\"target\"] = df[\"target\"].fillna(0)\n",
    "    else:    \n",
    "        df = df[features + [\"id\"] + [\"near_id\"]]\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    df[\"near_id\"] = df[\"near_id\"].fillna('')\n",
    "\n",
    "    gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set features to predict on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.996114Z",
     "iopub.status.busy": "2022-05-20T23:07:12.995898Z",
     "iopub.status.idle": "2022-05-20T23:07:13.011631Z",
     "shell.execute_reply": "2022-05-20T23:07:13.010775Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.996088Z"
    }
   },
   "outputs": [],
   "source": [
    "features = list()\n",
    "\n",
    "columns = ['name', 'address', 'city', 'state', # 'closest_city', \n",
    "           'zip', 'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "features.append(f\"d_near\")\n",
    "for c in columns:        \n",
    "    features += [f\"near_{c}_gesh\", f\"near_{c}_jaro\", f\"near_{c}_lcs\"]\n",
    "    if c in ['country', \"phone\", \"zip\"]:\n",
    "        features += [f\"near_{c}_leven\"]\n",
    "    else:\n",
    "        features += [f\"near_{c}_len\", f\"near_{c}_nleven\", f\"near_{c}_nlcsi\", f\"near_{c}_nlcs0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:13.013394Z",
     "iopub.status.busy": "2022-05-20T23:07:13.013160Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0352bc25d0924c6a8c0081d08f4d6da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def add_neighbours(args):\n",
    "    country, df = args\n",
    "    columns = ['name', 'address', 'city', 'state', 'zip',  # 'closest_city', \n",
    "               'country', 'url', 'phone', 'categories', 'main_categories']\n",
    "    df = add_neighbour_features_low_mem(df)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "processes = multiprocessing.cpu_count()-1\n",
    "with multiprocessing.Pool(processes=processes) as pool:\n",
    "    dfs = pool.imap_unordered(add_neighbours, train.groupby('country', sort=False))\n",
    "    dfs = tqdm(dfs, total=num_countries)\n",
    "    dfs = list(dfs)\n",
    "    \n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split list of dasets on smaller chunks to speed up process and prevent OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "threshold = 250000\n",
    "\n",
    "def split_df(df, idx):\n",
    "    n_splits = int(df.shape[0] / threshold) + 1\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for trn_idx, val_idx in kf.split(df):\n",
    "        dfs.append(df.loc[val_idx])\n",
    "    \n",
    "idx = 0\n",
    "while idx < len(dfs):\n",
    "    if dfs[idx].shape[0] > threshold:\n",
    "        dfs[idx] = dfs[idx].reset_index(drop=True)\n",
    "        split_df(dfs[idx], idx)\n",
    "        dfs.pop(idx)\n",
    "    else:\n",
    "        idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add distance features and reduce the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def add_distance_and_reduce(df):\n",
    "    df = add_distance_features(df)\n",
    "    df = reduce_mem(df, train_mode=True)\n",
    "    return df\n",
    "\n",
    "l = len(dfs)\n",
    "\n",
    "with multiprocessing.Pool(processes=processes) as pool:\n",
    "    dfs = pool.imap_unordered(add_distance_and_reduce, dfs)\n",
    "    dfs = tqdm(dfs, total=l)\n",
    "    dfs = list(dfs)\n",
    "    \n",
    "train = pd.concat(dfs).reset_index(drop=True)\n",
    "del dfs\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select indexes of all positive targets\n",
    "# and select indexes of all ids that don't have postive targets at all\n",
    "pos_ids = train.loc[train['target'] == 1, 'id'].unique()\n",
    "pos_idxs = train[train['target'] == 1].index\n",
    "neg_idxs = train.loc[~train['id'].isin(pos_ids), 'id'].drop_duplicates(keep='first').index\n",
    "\n",
    "# additionally select indexes of ids that have negative target\n",
    "# but may have positive target \n",
    "neg_idxs1 = train[train['target'] == 0].index\n",
    "neg_idxs1 = neg_idxs1.difference(neg_idxs)\n",
    "neg_idxs1 = np.random.choice(neg_idxs1, size=len(pos_idxs)-len(neg_idxs))\n",
    "\n",
    "# and add them to negative indexes, so the total number of positive and negative indexes are equal\n",
    "neg_idxs = neg_idxs.union(neg_idxs1)\n",
    "\n",
    "# select these positive and negative indexes from the dataset\n",
    "train = train.loc[pos_idxs.union(neg_idxs)]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.misc import derivative\n",
    "\n",
    "\n",
    "def fit_lgbm(X, y, params=None, es_rounds=50, seed=42, N_SPLITS=5, \n",
    "             n_class=None, model_dir=None, folds=None):\n",
    "    cat_features = X.select_dtypes(include='object').columns\n",
    "    \n",
    "    models = []\n",
    "    oof = np.zeros(len(y), dtype=np.float64)\n",
    "    \n",
    "    for i in tqdm(range(CFG.n_splits)):\n",
    "        print(f\"== fold {i} ==\")\n",
    "        trn_idx = folds != i\n",
    "        val_idx = folds == i\n",
    "        \n",
    "        train_dataset = lgb.Dataset(X.iloc[trn_idx], y.iloc[trn_idx], categorical_feature=cat_features)\n",
    "        valid_dataset = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx], categorical_feature=cat_features)\n",
    "\n",
    "        \n",
    "        focal_loss = lambda x,y: focal_loss_lgb(x, y, alpha=1., gamma=1.)\n",
    "        focal_loss_eval = lambda x,y: focal_loss_lgb_eval_error(x, y, alpha=1., gamma=1.)\n",
    "        \n",
    "        if model_dir is None:\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_set = train_dataset, \n",
    "                valid_sets = [train_dataset, valid_dataset], \n",
    "                callbacks = [lgb.log_evaluation(100), \n",
    "                             lgb.early_stopping(stopping_rounds=es_rounds)],\n",
    "            )\n",
    "        else:\n",
    "            with open(f'{model_dir}/lgbm_fold{i}.pkl', 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "        pred = model.predict(X.iloc[val_idx])\n",
    "        oof[val_idx] = pred\n",
    "        models.append(model)\n",
    "        \n",
    "        file = f'lgbm_fold{i}.pkl'\n",
    "        pickle.dump(model, open(file, 'wb'))\n",
    "        print()\n",
    "\n",
    "    cv = (np.round(oof) == y).mean()\n",
    "    print(f\"CV-accuracy: {cv}\")\n",
    "\n",
    "    return oof, models\n",
    "\n",
    "def inference_lgbm(models, feat_df):\n",
    "    pred = np.array([model.predict(feat_df) for model in models])\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "\n",
    "kf = StratifiedGroupKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "for i, (trn_idx, val_idx) in tqdm(enumerate(kf.split(train, train[\"target\"], train[\"id\"]))):\n",
    "    train.loc[val_idx, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "\n",
    "params = {\n",
    "    'seed': CFG.seed,\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.2,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'max_bin': 200,\n",
    "    'max_depth': 7,   \n",
    "    'num_leaves': 35, \n",
    "    'min_data_in_leaf': 25,\n",
    "    'n_estimators': 5000, \n",
    "    'colsample_bytree': 0.9,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "oof, models = fit_lgbm(train[features], train[\"target\"].astype(int), \n",
    "                       params=params, n_class=int(train[\"target\"].max() + 1), \n",
    "                       N_SPLITS=CFG.n_splits, folds=train[\"fold\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuctions for postprocessing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.983819Z",
     "iopub.status.busy": "2022-05-20T23:07:12.983027Z",
     "iopub.status.idle": "2022-05-20T23:07:12.994514Z",
     "shell.execute_reply": "2022-05-20T23:07:12.993726Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.983695Z"
    },
    "id": "yHFNkcnglAkW",
    "outputId": "b886a788-4f08-468b-8050-17e92da005ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(input_df)\n",
    "    poi2ids = get_poi2ids(input_df)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in df[\"matches\"].values:\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def get_matches(df, preds):\n",
    "    near_id = df[\"near_id\"].values\n",
    "    matches = []\n",
    "\n",
    "    for df_id, pred, near_idx in zip(df[\"id\"], preds, near_id):\n",
    "        idx = np.round(pred)\n",
    "        if idx == 1:\n",
    "            matches.append(df_id + \" \" + near_idx)\n",
    "        else:\n",
    "            matches.append(df_id)\n",
    "    \n",
    "    df['matches'] = matches\n",
    "    df = postprocess(df)\n",
    "    \n",
    "    return df[['id', 'matches', 'point_of_interest']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predict matches and postprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = get_matches(train, oof)\n",
    "train = train.drop_duplicates()\n",
    "print(f\"CV: {get_score(train):.6f}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline:\n",
    "# acc: 0.9013\n",
    "# CV: 0.861652\n",
    "\n",
    "# Add closest_city from city dataframe\n",
    "# acc: 0.90581\n",
    "# CV: 0.863641\n",
    "\n",
    "# Don't replace nans with ''\n",
    "# acc: 0.909\n",
    "# CV: 0.865036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further ideas\n",
    "\n",
    "- do we need url/zip/phone?\n",
    "- add ntlk.edit_distance to your features\n",
    "- change KNN to the variant, that was proposed in this notebook: https://www.kaggle.com/code/ragnar123/flm-xlmroberta-inference-baseline\n",
    "- add manhattan distance and euqlidian distance\n",
    "- increase number of nearest neighbours to a very high value (like 50-100-200), so you will be able to find more matches; don't increase number of neighbours in the table to avoid OOM\n",
    "- add KNN by country\n",
    "\n",
    "\n",
    "- how to handle missing data https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python\n",
    "- mean/median/std encode features\n",
    "- use feature generation and selection from this notebook https://www.kaggle.com/code/aerdem4/foursquare-gpu-accelerated-lofo-feature-importance\n",
    "- use Cat2Vec to calculate categories similarity https://www.kaggle.com/code/aerdem4/foursquare-cat2vec/notebook\n",
    "\n",
    "\n",
    "- Optuna!\n",
    "\n",
    "\n",
    "\n",
    "- try XLMRoberta\n",
    "\n",
    "\n",
    "- you can use dict to store key-poi_id pairs and store only keys to save the memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
