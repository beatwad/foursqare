{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.408122Z",
     "iopub.status.busy": "2022-05-20T23:06:53.407759Z",
     "iopub.status.idle": "2022-05-20T23:06:53.948805Z",
     "shell.execute_reply": "2022-05-20T23:06:53.947555Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.408094Z"
    },
    "id": "H5QntWoelAkH",
    "outputId": "31efe7df-24ff-40e8-8517-0c7174968413"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "import difflib\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = Namespace(\n",
    "    train = True,\n",
    "    full = False,\n",
    "    select_features = False,\n",
    "    find_thresh = False,\n",
    "    seed = 42,\n",
    "    debug = False,\n",
    "    validate = False,\n",
    "    target = \"label\",\n",
    "    n_neighbors = 20,\n",
    "    n_splits = 5,\n",
    "    threshold = 0.5,\n",
    "    train_path = 'train_dataset'\n",
    ")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "new_features = ['category_venn',\n",
    "                'euclidean',\n",
    "                'haversine',\n",
    "                'kdist_diff',\n",
    "                'kneighbors_mean',\n",
    "                'manhattan',\n",
    "                'text_gesh',\n",
    "                'text_jaro',\n",
    "                'text_lcs',\n",
    "                'text_len_diff',\n",
    "                'text_leven',\n",
    "                'text_nlcs',\n",
    "                'text_nlcsk',\n",
    "                'text_nleven'\n",
    "               ]  \n",
    "\n",
    "bad_features = ['city_gesh', 'text_leven', 'address_encoded_len_diff',\n",
    "               'name_encoded_nleven', 'name_lcs', 'city_leven', 'city_sim',\n",
    "               'closest_city_leven', 'main_categories_leven', 'city_lcs',\n",
    "               'url_leven', 'address_nlcsk', 'address_len_diff', 'url_lcs',\n",
    "               'closest_city_nlcs', 'url_nlcsk', 'kneighbors', 'city_encoded_lcs',\n",
    "               'city_jaro', 'address_encoded_lcs', 'main_categories_nlcsk',\n",
    "               'address_encoded_jaro', 'url_nleven', 'url_len_diff',\n",
    "               'closest_city_len_diff']\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.950215Z",
     "iopub.status.busy": "2022-05-20T23:06:53.949976Z",
     "iopub.status.idle": "2022-05-20T23:06:59.622657Z",
     "shell.execute_reply": "2022-05-20T23:06:59.621792Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.950187Z"
    },
    "id": "wz7JepVilAkN",
    "outputId": "0652de28-9bd3-4ab7-c97c-55e6e11935e6",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d986ef55ea4dfd98c0ddfcb20add1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CFG.full:\n",
    "    train_files = glob(os.path.join(CFG.train_path, \"train_*.parquet\"))\n",
    "    valid_files = glob(os.path.join(CFG.train_path, \"valid_*.parquet\"))\n",
    "    train_files = train_files + valid_files\n",
    "else:\n",
    "    train_files = glob(os.path.join(CFG.train_path, \"train_*.parquet\"))\n",
    "\n",
    "train = list()\n",
    "for filename in tqdm(train_files):\n",
    "    df = pd.read_parquet(filename)\n",
    "    train.append(df)\n",
    "\n",
    "train = pd.concat(train, axis=0, ignore_index=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c513caabd6d542b1bef70ee3c89764b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not CFG.full:\n",
    "    valid_files = glob(os.path.join(CFG.train_path, \"valid_*.parquet\"))\n",
    "\n",
    "    valid = list()\n",
    "    for filename in tqdm(valid_files):\n",
    "        df = pd.read_parquet(filename)\n",
    "        valid.append(df)\n",
    "\n",
    "    valid = pd.concat(valid, axis=0, ignore_index=True)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add haversine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'foursquare_location_matching'\n",
    "data = pd.read_csv(os.path.join(data_root, 'train.csv'))\n",
    "data = data[['id', 'latitude', 'longitude']]\n",
    "\n",
    "\n",
    "def vectorized_haversine(lats1, lats2, longs1, longs2):\n",
    "    radius = 6371\n",
    "    dlat=np.radians(lats2 - lats1)\n",
    "    dlon=np.radians(longs2 - longs1)\n",
    "    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats1)) \\\n",
    "        * np.cos(np.radians(lats2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = radius * c\n",
    "    return d\n",
    "\n",
    "def add_haversine(df):\n",
    "    df = df.merge(data, how='left', on='id')\n",
    "    df.rename({'latitude': 'lat1', 'longitude': 'lon1'}, axis=1, inplace=True)\n",
    "    df = df.merge(data, how='left', left_on='match_id', right_on='id')\n",
    "    df.rename({'latitude': 'lat2', 'longitude': 'lon2', 'id_x': 'id'}, axis=1, inplace=True)\n",
    "    df['haversine'] = vectorized_haversine(df['lat1'], df['lon1'], df['lat2'], df['lon2'])\n",
    "    df.drop(['lat1', 'lon1', 'lat2', 'lon2', 'id_y'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train = add_haversine(train)\n",
    "valid = add_haversine(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if bad_features:\n",
    "    train = train.drop(bad_features, axis=1)\n",
    "    valid = valid.drop(bad_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "features = list(train.select_dtypes(include=numerics).columns)\n",
    "features.remove(CFG.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm(X_train, y_train, X_val, y_val, params=None, es_rounds=50, model_dir=None):\n",
    "    train_dataset = lgb.Dataset(X_train, y_train)\n",
    "    valid_dataset = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set = train_dataset, \n",
    "        valid_sets = [train_dataset, valid_dataset],\n",
    "        callbacks = [lgb.log_evaluation(50), \n",
    "                     lgb.early_stopping(stopping_rounds=es_rounds)]\n",
    "        )\n",
    "\n",
    "    file = f'fsq_lgbm_models_train_test/lgbm.pkl'\n",
    "    pickle.dump(model, open(file, 'wb'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model, threshold, X_val, y_val):\n",
    "    pred = model.predict(X_val)\n",
    "    cv = ((pred > threshold) == y_val).mean()\n",
    "    print(f\"ROC AUC: {cv}\")\n",
    "    return pred, cv\n",
    "\n",
    "def inference_lgbm(model, feat_df):\n",
    "    pred = np.array(model.predict(feat_df))\n",
    "    return pred\n",
    "\n",
    "def inference_lgbm_fold(models, feat_df):\n",
    "    pred = np.array([model.predict(feat_df) for model in models])\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset for train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.995025\tvalid_1's auc: 0.994005\n",
      "[100]\ttraining's auc: 0.995768\tvalid_1's auc: 0.994497\n",
      "[150]\ttraining's auc: 0.996089\tvalid_1's auc: 0.994665\n",
      "[200]\ttraining's auc: 0.996386\tvalid_1's auc: 0.994763\n",
      "[250]\ttraining's auc: 0.996597\tvalid_1's auc: 0.994834\n",
      "[300]\ttraining's auc: 0.996794\tvalid_1's auc: 0.994896\n",
      "[350]\ttraining's auc: 0.996959\tvalid_1's auc: 0.994924\n",
      "[400]\ttraining's auc: 0.997089\tvalid_1's auc: 0.99496\n",
      "[450]\ttraining's auc: 0.997199\tvalid_1's auc: 0.994977\n",
      "[500]\ttraining's auc: 0.997307\tvalid_1's auc: 0.995019\n",
      "[550]\ttraining's auc: 0.997406\tvalid_1's auc: 0.995024\n",
      "[600]\ttraining's auc: 0.997495\tvalid_1's auc: 0.99504\n",
      "[650]\ttraining's auc: 0.99759\tvalid_1's auc: 0.995032\n",
      "Early stopping, best iteration is:\n",
      "[620]\ttraining's auc: 0.997529\tvalid_1's auc: 0.995043\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "\n",
    "params = {\n",
    "    'seed': CFG.seed,\n",
    "#     'device': 'gpu',\n",
    "#     'gpu_platform_id': 0,\n",
    "#     'gpu_device_id': 0,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.2,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "#     'max_bin': 200,\n",
    "    'max_depth': 7,   \n",
    "    'num_leaves': 35, \n",
    "#     'min_data_in_leaf': 40,\n",
    "    'n_estimators': 1500, \n",
    "    'colsample_bytree': 0.9,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "if CFG.select_features:\n",
    "    import lofo\n",
    "    from lofo import LOFOImportance, Dataset, plot_importance\n",
    "    # extract a sample of the data\n",
    "    train = train.sample(frac=0.1, random_state=CFG.seed)\n",
    "    valid = valid.sample(frac=0.1, random_state=CFG.seed)\n",
    "    train = pd.concat([train, valid], ignore_index=True)\n",
    "    del valid\n",
    "    gc.collect()\n",
    "    # define the validation scheme\n",
    "    cv = KFold(n_splits=2)\n",
    "    # define the binary target and the features\n",
    "    dataset = Dataset(df=train, target=CFG.target, features=new_features)\n",
    "    # define the validation scheme and scorer\n",
    "    lofo_imp = LOFOImportance(dataset, scoring=\"roc_auc\", cv=cv, model=lgb.LGBMClassifier(**params))\n",
    "    # get the mean and standard deviation of the importances in pandas format\n",
    "    importance_df = lofo_imp.get_importance()\n",
    "    importance_df.to_csv('importance_df.csv')\n",
    "    # plot the means and standard deviations of the importances\n",
    "    plot_importance(importance_df, figsize=(12, 20))\n",
    "elif CFG.full and CFG.train:\n",
    "    model = fit_lgbm(train[features], train[CFG.target], \n",
    "                     train[features], train[CFG.target], \n",
    "                     params=params, es_rounds=50)\n",
    "elif CFG.train:\n",
    "    model = fit_lgbm(train[features], train[CFG.target], \n",
    "                     valid[features], valid[CFG.target], \n",
    "                     params=params, es_rounds=50)\n",
    "else:\n",
    "    model_file = 'fsq_lgbm_models_train_test/lgbm1.pkl'\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [50]\ttraining's auc: 0.99466\tvalid_1's auc: 0.993636\n",
    "# [100]\ttraining's auc: 0.995336\tvalid_1's auc: 0.99416\n",
    "# [150]\ttraining's auc: 0.995729\tvalid_1's auc: 0.99439\n",
    "# [200]\ttraining's auc: 0.995985\tvalid_1's auc: 0.994485\n",
    "# [250]\ttraining's auc: 0.996176\tvalid_1's auc: 0.994543\n",
    "# [300]\ttraining's auc: 0.996344\tvalid_1's auc: 0.994573\n",
    "# [350]\ttraining's auc: 0.996497\tvalid_1's auc: 0.994609\n",
    "# [400]\ttraining's auc: 0.996644\tvalid_1's auc: 0.99465\n",
    "# [450]\ttraining's auc: 0.996772\tvalid_1's auc: 0.994685\n",
    "# [500]\ttraining's auc: 0.996888\tvalid_1's auc: 0.994706\n",
    "# [550]\ttraining's auc: 0.996997\tvalid_1's auc: 0.994726\n",
    "# [600]\ttraining's auc: 0.997098\tvalid_1's auc: 0.994742\n",
    "# [650]\ttraining's auc: 0.99719\tvalid_1's auc: 0.994745\n",
    "# [700]\ttraining's auc: 0.997271\tvalid_1's auc: 0.994765\n",
    "# [750]\ttraining's auc: 0.997345\tvalid_1's auc: 0.994767\n",
    "# Early stopping, best iteration is:\n",
    "# [710]\ttraining's auc: 0.997286\tvalid_1's auc: 0.99477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_features = importance_df.loc[(importance_df.importance_mean < 0) & \n",
    "                                 (importance_df.val_imp_0 < 0) & \n",
    "                                 (importance_df.val_imp_1 < 0), 'feature'].values\n",
    "bad_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuctions for postprocessing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.983819Z",
     "iopub.status.busy": "2022-05-20T23:07:12.983027Z",
     "iopub.status.idle": "2022-05-20T23:07:12.994514Z",
     "shell.execute_reply": "2022-05-20T23:07:12.993726Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.983695Z"
    },
    "id": "yHFNkcnglAkW",
    "outputId": "b886a788-4f08-468b-8050-17e92da005ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(input_df)\n",
    "    poi2ids = get_poi2ids(input_df)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in df[\"matches\"].values:\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def get_matches(df, preds):\n",
    "    match_id = df[\"match_id\"].values\n",
    "    matches = []\n",
    "\n",
    "    for df_id, pred, match_idx in tqdm(zip(df[\"id\"], preds, match_id), total=df.shape[0]):\n",
    "        idx = np.round(pred)\n",
    "        if idx == 1:\n",
    "            matches.append(df_id + \" \" + match_idx)\n",
    "        else:\n",
    "            matches.append(df_id)\n",
    "    \n",
    "    df['matches'] = matches\n",
    "    df = postprocess(df)\n",
    "    \n",
    "    return df[['id', 'matches', 'point_of_interest']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add POI column to validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.full:\n",
    "    data_root = 'foursquare_location_matching'\n",
    "    data = pd.read_csv(os.path.join(data_root, 'train.csv'))[['id', 'point_of_interest']]\n",
    "\n",
    "    valid = valid.merge(data, how='left', on='id')\n",
    "\n",
    "    del data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predict matches and postprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.987181950977739\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1e95d721e14941a654927890417ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13919513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU: 0.839483\n",
      "CPU times: user 10min 4s, sys: 2.9 s, total: 10min 7s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if CFG.find_thresh:\n",
    "    pred, cv = predict(model, best_thr, valid[features], valid[CFG.target])\n",
    "else:\n",
    "    pred, cv = predict(model, CFG.threshold, valid[features], valid[CFG.target])\n",
    "\n",
    "if not CFG.full:\n",
    "    res = get_matches(valid, pred)\n",
    "    res = res.drop_duplicates()\n",
    "    print(f\"IOU: {get_score(res):.6f}\")\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline:\n",
    "# ROC AUC: 0.986\n",
    "# IOU: 0.831\n",
    "\n",
    "# Add 'is_unbalance' parameter\n",
    "# ROC AUC: 0.967\n",
    "# IOU: 0.749\n",
    "\n",
    "# Sort categories\n",
    "# ROC AUC: 0.9863\n",
    "# IOU: 0.8325\n",
    "\n",
    "# Drop bad features\n",
    "# ROC AUC: 0.98656\n",
    "# IOU: 0.835467\n",
    "\n",
    "# Add main_category and closest_city\n",
    "# ROC AUC: 0.9868/0.987\n",
    "# IOU: 0.83869/0.838849\n",
    "\n",
    "# Clean name\n",
    "# ROC AUC: 0.98686/0.98697\n",
    "# IOU: 0.8392/0.8383\n",
    "\n",
    "# Add some many different distances and text features\n",
    "# ROC AUC: 0.9871/\n",
    "# IOU: 0.839483/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
