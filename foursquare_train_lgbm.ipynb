{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.408122Z",
     "iopub.status.busy": "2022-05-20T23:06:53.407759Z",
     "iopub.status.idle": "2022-05-20T23:06:53.948805Z",
     "shell.execute_reply": "2022-05-20T23:06:53.947555Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.408094Z"
    },
    "id": "H5QntWoelAkH",
    "outputId": "31efe7df-24ff-40e8-8517-0c7174968413"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "import difflib\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = Namespace(\n",
    "    train = True,\n",
    "    full = False,\n",
    "    debug = False,\n",
    "    select_features = False,\n",
    "    folds = 0,\n",
    "    seed = 42,\n",
    "    pos_frac = 0.2,\n",
    "    target = \"label\",\n",
    "    threshold = 0.5,\n",
    "    train_path = 'train_dataset',\n",
    "    model_dir = 'fsq_lgbm_models_train_test',\n",
    "    es_rounds = 50\n",
    ")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "bad_features = ['cat_cluster_same_cluster']\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:06:53.950215Z",
     "iopub.status.busy": "2022-05-20T23:06:53.949976Z",
     "iopub.status.idle": "2022-05-20T23:06:59.622657Z",
     "shell.execute_reply": "2022-05-20T23:06:59.621792Z",
     "shell.execute_reply.started": "2022-05-20T23:06:53.950187Z"
    },
    "id": "wz7JepVilAkN",
    "outputId": "0652de28-9bd3-4ab7-c97c-55e6e11935e6",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5196a6b2237d492594c232fa305d32df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def downcast_floats(df):\n",
    "    floats = ['float32', 'float64']\n",
    "    float_features = list(df.select_dtypes(include=floats).columns)\n",
    "    for f in float_features:\n",
    "        df[f] = df[f].astype('float16')\n",
    "    return df\n",
    "    \n",
    "if CFG.full or CFG.folds:\n",
    "    train_files = glob(os.path.join(CFG.train_path, \"train_*.parquet\"))\n",
    "    valid_files = glob(os.path.join(CFG.train_path, \"valid_*.parquet\"))\n",
    "    train_files = train_files + valid_files\n",
    "else:\n",
    "    train_files = glob(os.path.join(CFG.train_path, \"train_*.parquet\"))\n",
    "\n",
    "train = list()\n",
    "for filename in tqdm(train_files):\n",
    "    df = pd.read_parquet(filename)\n",
    "    if CFG.debug:\n",
    "        df = df.sample(n = 10000, random_state = CFG.seed)\n",
    "        df = df.reset_index(drop = True)\n",
    "    df = downcast_floats(df)\n",
    "    train.append(df)\n",
    "\n",
    "train = pd.concat(train, axis=0, ignore_index=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a00cdc4694c4a0cac9a48424f99ac07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not CFG.full and not CFG.folds:\n",
    "    valid_files = glob(os.path.join(CFG.train_path, \"valid_*.parquet\"))\n",
    "\n",
    "    valid = list()\n",
    "    for filename in tqdm(valid_files):\n",
    "        df = pd.read_parquet(filename)\n",
    "        if CFG.debug:\n",
    "            df = df.sample(n = 10000, random_state = CFG.seed)\n",
    "            df = df.reset_index(drop = True)\n",
    "        df = downcast_floats(df)\n",
    "        valid.append(df)\n",
    "\n",
    "    valid = pd.concat(valid, axis=0, ignore_index=True)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase fraction of positive targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train_pos_index = train[train['label'] == 1].index\n",
    "# train_neg_index = train[train['label'] == 0].index\n",
    "# train_neg_index = np.random.choice(train_neg_index, size=int(len(train_pos_index)*((1-CFG.pos_frac)/CFG.pos_frac)))\n",
    "# train_pos_index = np.concatenate([train_pos_index, train_neg_index])\n",
    "# np.random.shuffle(train_pos_index)\n",
    "# train = train.loc[train_pos_index].reset_index(drop=True)\n",
    "# del train_pos_index, train_neg_index\n",
    "# gc.collect()\n",
    "\n",
    "# if not CFG.full and not CFG.folds:\n",
    "#     valid_pos_index = valid[valid['label'] == 1].index\n",
    "#     valid_neg_index = valid[valid['label'] == 0].index\n",
    "#     valid_neg_index = np.random.choice(valid_neg_index, size=int(len(valid_pos_index)*((1-CFG.pos_frac)/CFG.pos_frac)))\n",
    "#     valid_pos_index = np.concatenate([valid_pos_index, valid_neg_index])\n",
    "#     np.random.shuffle(valid_pos_index)\n",
    "#     valid = valid.loc[valid_pos_index].reset_index(drop=True)\n",
    "#     del valid_pos_index, valid_neg_index\n",
    "#     gc.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop bad features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if bad_features:\n",
    "    train = train.drop(bad_features, axis=1)\n",
    "    if not CFG.full and not CFG.folds:\n",
    "        valid = valid.drop(bad_features, axis=1)\n",
    "        \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "features = list(train.select_dtypes(include=numerics).columns)\n",
    "features.remove(CFG.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset by folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.folds > 0:\n",
    "    # kf = StratifiedGroupKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "    kf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
    "    for i, (trn_idx, val_idx) in tqdm(enumerate(kf.split(train, train[\"label\"], train[\"label\"]))):\n",
    "        train.loc[val_idx, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.where(y_hat < 0.5, 0, 1)  \n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "def lgb_f2_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.where(y_hat < 0.5, 0, 1)  \n",
    "    return 'f2', fbeta_score(y_true, y_hat, beta=2), True\n",
    "\n",
    "def fit_lgbm(X_train, y_train, X_val, y_val, init_model=None, \n",
    "             params=None, es_rounds=50, num_iter=0):\n",
    "    train_dataset = lgb.Dataset(X_train, y_train)\n",
    "    valid_dataset = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set = train_dataset, \n",
    "        valid_sets = [train_dataset, valid_dataset],\n",
    "        init_model = init_model,\n",
    "#         feval=lgb_f1_score,\n",
    "        callbacks = [lgb.log_evaluation(10), \n",
    "                     lgb.early_stopping(stopping_rounds=es_rounds),\n",
    "                    ]\n",
    "        )\n",
    "\n",
    "    file = f'{CFG.model_dir}/lgbm.pkl'\n",
    "    pickle.dump(model, open(file, 'wb'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_lgbm_folds(X, y, folds, init_model=None, params=None, es_rounds=50, num_iter=0):\n",
    "    models = []\n",
    "    \n",
    "    for i in tqdm(range(CFG.folds)):\n",
    "        print(f\"== fold {i} ==\")\n",
    "        trn_idx = folds != i\n",
    "        val_idx = folds == i\n",
    "    \n",
    "        train_dataset = lgb.Dataset(X.iloc[trn_idx], y.iloc[trn_idx])\n",
    "        valid_dataset = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_set = train_dataset, \n",
    "            valid_sets = [train_dataset, valid_dataset],\n",
    "            init_model = init_model,\n",
    "#             feval=lgb_f1_score,\n",
    "            callbacks = [lgb.log_evaluation(10), \n",
    "                         lgb.early_stopping(stopping_rounds=es_rounds),\n",
    "                        ]\n",
    "            )\n",
    "\n",
    "        models.append(model)\n",
    "    \n",
    "        file = f'{CFG.model_dir}/lgbm_fold_{i}.pkl'\n",
    "        pickle.dump(model, open(file, 'wb'))\n",
    "\n",
    "    return models\n",
    "\n",
    "def predict_(model, X_val, y_val, threshold):\n",
    "    pred = model.predict(X_val)\n",
    "    return pred\n",
    "\n",
    "def predict_folds(models, X, y, folds, threshold):\n",
    "    oof = np.zeros((len(y)), dtype=np.float64)\n",
    "    \n",
    "    for i in tqdm(range(CFG.folds)):\n",
    "        trn_idx = folds != i\n",
    "        val_idx = folds == i\n",
    "        \n",
    "        pred = models[i].predict(X.iloc[val_idx])\n",
    "        oof[val_idx] = pred\n",
    "    \n",
    "    return oof\n",
    "\n",
    "def show_metrics(pred, threshold, y):\n",
    "    y_hat = np.where(pred < threshold, 0, 1)  \n",
    "    acc = (y_hat == y).mean()\n",
    "    f1 = f1_score(y, y_hat)\n",
    "    f2 = fbeta_score(y, y_hat, beta=2)\n",
    "#     print(f\"Accuracy: {acc}, F1: {f1}, F2: {f2}\")\n",
    "    return acc, f1, f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "\n",
    "params = {\n",
    "    'seed': CFG.seed,\n",
    "#     'device': 'gpu',\n",
    "#     'gpu_platform_id': 0,\n",
    "#     'gpu_device_id': 0,\n",
    "    'objective': 'binary',\n",
    "#     'first_metric_only': True,\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.2,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "#     'max_bin': 200,\n",
    "    'max_depth': 7,   \n",
    "    'num_leaves': 35, \n",
    "#     'min_data_in_leaf': 40,\n",
    "    'n_estimators': 1500, \n",
    "    'colsample_bytree': 0.9,\n",
    "    'verbose': -1\n",
    "}\n",
    "            \n",
    "\n",
    "if CFG.select_features:\n",
    "    import lofo\n",
    "    from lofo import LOFOImportance, Dataset, plot_importance\n",
    "    # extract a sample of the data\n",
    "    train = train.sample(frac=0.15, random_state=CFG.seed)\n",
    "    valid = valid.sample(frac=0.15, random_state=CFG.seed)\n",
    "    train = pd.concat([train, valid], ignore_index=True)\n",
    "    del valid\n",
    "    gc.collect()\n",
    "    # define the validation scheme\n",
    "    cv = KFold(n_splits=2)\n",
    "    # define the binary target and the features\n",
    "    dataset = Dataset(df=train, target=CFG.target, features=new_features)\n",
    "    # define the validation scheme and scorer\n",
    "    lofo_imp = LOFOImportance(dataset, scoring=\"roc_auc\", cv=cv, model=lgb.LGBMClassifier(**params))\n",
    "    # get the mean and standard deviation of the importances in pandas format\n",
    "    importance_df = lofo_imp.get_importance()\n",
    "    importance_df.to_csv('importance_df.csv')\n",
    "    # plot the means and standard deviations of the importances\n",
    "    plot_importance(importance_df, figsize=(12, 20))\n",
    "elif CFG.folds and CFG.train:\n",
    "    models = fit_lgbm_folds(train[features], train[CFG.target], folds=train['fold'].values,\n",
    "                            params=params, es_rounds=CFG.es_rounds)\n",
    "elif CFG.full and CFG.train:\n",
    "    model = fit_lgbm(train[features], train[CFG.target], \n",
    "                     train[features], train[CFG.target], \n",
    "                     params=params, es_rounds=CFG.es_rounds)\n",
    "elif CFG.train:\n",
    "    model = fit_lgbm(train[features], train[CFG.target], \n",
    "                     valid[features], valid[CFG.target], \n",
    "                     params=params, es_rounds=CFG.es_rounds)\n",
    "elif CFG.folds:\n",
    "    model_files = glob(os.path.join(CFG.model_dir, \"lgbm*.pkl\"))\n",
    "    models = list()\n",
    "    for model_file in model_files:\n",
    "        with open(model_file, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            models.append(model)\n",
    "else:\n",
    "    model_file = f'{CFG.model_dir}/lgbm.pkl'\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_thr = 0.5\n",
    "best_cv = 0\n",
    "\n",
    "if CFG.folds:\n",
    "    X = train[features]\n",
    "    y = train[CFG.target]\n",
    "else:\n",
    "    X = valid[features]\n",
    "    y = valid[CFG.target]\n",
    "\n",
    "if CFG.folds:\n",
    "    pred = predict_folds(models, X, y, train['fold'].values, best_thr)\n",
    "else:\n",
    "    pred = predict_(model, X, y, best_thr)\n",
    "\n",
    "# for thr in tqdm(np.arange(0.3, 0.45, 0.005)):\n",
    "#     y_hat = np.where(pred < thr, 0, 1)  \n",
    "#     cv = f1_score(y, y_hat) # F1\n",
    "#     print(f'Threshold is {thr}, score is {cv}')\n",
    "#     if cv > best_cv:\n",
    "#         best_cv = cv\n",
    "#         best_thr = thr\n",
    "            \n",
    "# acc, f1, f2 = show_metrics(pred, best_thr, y)\n",
    "# print(f'Best threshold is {best_thr}, Accuracy is {acc}, F1 score is {f1}, F2 score is {f2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuctions for postprocessing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T23:07:12.983819Z",
     "iopub.status.busy": "2022-05-20T23:07:12.983027Z",
     "iopub.status.idle": "2022-05-20T23:07:12.994514Z",
     "shell.execute_reply": "2022-05-20T23:07:12.993726Z",
     "shell.execute_reply.started": "2022-05-20T23:07:12.983695Z"
    },
    "id": "yHFNkcnglAkW",
    "outputId": "b886a788-4f08-468b-8050-17e92da005ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(input_df)\n",
    "    poi2ids = get_poi2ids(input_df)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in df[\"matches\"].values:\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def get_matches(df, preds):\n",
    "    match_id = df[\"match_id\"].values\n",
    "    matches = []\n",
    "\n",
    "    for df_id, pred, match_idx in tqdm(zip(df[\"id\"], preds, match_id), total=df.shape[0]):\n",
    "        idx = np.round(pred)\n",
    "        if pred == 1:\n",
    "            matches.append(df_id + \" \" + match_idx)\n",
    "        else:\n",
    "            matches.append(df_id)\n",
    "    \n",
    "    df['matches'] = matches\n",
    "    df = postprocess(df)\n",
    "    \n",
    "    return df[['id', 'matches', 'point_of_interest']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add POI column to validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.full:\n",
    "    data_root = 'foursquare_location_matching'\n",
    "    data = pd.read_csv(os.path.join(data_root, 'train.csv'))[['id', 'point_of_interest']]\n",
    "\n",
    "    if CFG.folds:\n",
    "        valid = train.merge(data, how='left', on='id')\n",
    "    else:\n",
    "        valid = valid.merge(data, how='left', on='id')\n",
    "\n",
    "    del data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predict matches and postprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_hat = np.where(pred < best_thr, 0, 1) \n",
    "    \n",
    "if not CFG.full:\n",
    "    res = get_matches(valid, y_hat)\n",
    "    res = res.drop_duplicates()\n",
    "    print(f\"IOU: {get_score(res):.6f}\")\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_importances(model):\n",
    "    importance_df = pd.DataFrame(model.feature_importance(), \n",
    "                                 index=features, \n",
    "                                 columns=['importance'])\\\n",
    "                        .sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    plt.subplots(figsize=(len(features) // 4, 5))\n",
    "    plt.bar(importance_df.index, importance_df.importance)\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_importances(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-folds, auc, 1000 iter, thresh 0.5\n",
    "# ACC: 0.9830761855878474\n",
    "# F1: 0.9144342837012177\n",
    "# F2 0.9083372734752316\n",
    "# IOU: 0.9\n",
    "# LB: ?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
